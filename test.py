import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoConfig, AutoModel, AutoTokenizer
import os
import json

# å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"

class SigLIP2TextEncoder(nn.Module):
    """SigLIP2æ–‡æœ¬ç¼–ç å™¨ - å®Œå…¨ä¿®å¤ç‰ˆæœ¬"""
    
    def __init__(self, text_model_name="google/siglip2-large-patch16-256", max_length=32, device="cuda", torch_dtype=torch.float16):
        super().__init__()
        self.max_length = max_length
        self.device = device
        self.torch_dtype = torch_dtype
        
        print(f"ğŸ”§ åˆå§‹åŒ–SigLIP2TextEncoder (å®Œå…¨ä¿®å¤ç‰ˆ)")
        print(f"   æ¨¡å‹è·¯å¾„: {text_model_name}")
        print(f"   æœ€å¤§é•¿åº¦: {max_length}")
        
        # ğŸ¯ æ–¹æ³•1ï¼šå°è¯•ç›´æ¥åŠ è½½å®Œæ•´SigLIPæ¨¡å‹
        if self._try_load_siglip_model(text_model_name):
            return
            
        # ğŸ¯ æ–¹æ³•2ï¼šå°è¯•ä¿®å¤åˆ†è¯å™¨é…ç½®ååŠ è½½
        if self._try_load_with_fixed_tokenizer(text_model_name):
            return
            
        # ğŸ¯ æ–¹æ³•3ï¼šä½¿ç”¨transformersè‡ªåŠ¨åŠ è½½
        if self._try_auto_load(text_model_name):
            return
            
        # ğŸ¯ æ–¹æ³•4ï¼šåˆ›å»ºå¤‡ç”¨ç¼–ç å™¨
        print(f"   ğŸ”„ æ‰€æœ‰åŠ è½½æ–¹æ³•å¤±è´¥ï¼Œåˆ›å»ºå¤‡ç”¨ç¼–ç å™¨...")
        self._create_fallback_encoder()
    
    def _try_load_siglip_model(self, model_path):
        """æ–¹æ³•1ï¼šå°è¯•åŠ è½½å®Œæ•´SigLIPæ¨¡å‹"""
        try:
            print(f"   ğŸ”„ æ–¹æ³•1ï¼šå°è¯•åŠ è½½å®Œæ•´SigLIP2æ¨¡å‹...")
            
            from transformers import SiglipModel, SiglipProcessor
            
            # åŠ è½½å®Œæ•´æ¨¡å‹
            self.full_model = SiglipModel.from_pretrained(
                model_path,
                local_files_only=True,
                torch_dtype=self.torch_dtype
            )
            
            # æå–æ–‡æœ¬ç¼–ç å™¨
            self.text_model = self.full_model.text_model
            
            # åŠ è½½processor
            self.processor = SiglipProcessor.from_pretrained(
                model_path,
                local_files_only=True
            )
            self.tokenizer = self.processor.tokenizer
            
            self.hidden_size = self.text_model.config.hidden_size
            self.text_model.eval()
            
            print(f"   âœ… æ–¹æ³•1æˆåŠŸï¼SigLIP2å®Œæ•´æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"      éšè—å±‚å¤§å°: {self.hidden_size}")
            return True
            
        except Exception as e:
            print(f"   âŒ æ–¹æ³•1å¤±è´¥: {e}")
            return False
    
    def _try_load_with_fixed_tokenizer(self, model_path):
        """æ–¹æ³•2ï¼šä¿®å¤åˆ†è¯å™¨é…ç½®ååŠ è½½"""
        try:
            print(f"   ğŸ”„ æ–¹æ³•2ï¼šå°è¯•ä¿®å¤åˆ†è¯å™¨é…ç½®...")
            
            # æ£€æŸ¥tokenizeré…ç½®æ–‡ä»¶
            tokenizer_config_path = os.path.join(model_path, "tokenizer_config.json")
            if os.path.exists(tokenizer_config_path):
                with open(tokenizer_config_path, 'r') as f:
                    config = json.load(f)
                    print(f"      åˆ†è¯å™¨ç±»å‹: {config.get('tokenizer_class', 'Unknown')}")
            
            # å°è¯•ä½¿ç”¨æ­£ç¡®çš„åˆ†è¯å™¨ç±»å‹
            from transformers import GemmaTokenizer, SiglipModel
            
            # ç›´æ¥ä½¿ç”¨GemmaTokenizer
            self.tokenizer = GemmaTokenizer.from_pretrained(
                model_path,
                local_files_only=True
            )
            
            # åŠ è½½SigLIPæ¨¡å‹
            self.full_model = SiglipModel.from_pretrained(
                model_path,
                local_files_only=True,
                torch_dtype=self.torch_dtype
            )
            self.text_model = self.full_model.text_model
            
            self.hidden_size = self.text_model.config.hidden_size
            self.text_model.eval()
            
            print(f"   âœ… æ–¹æ³•2æˆåŠŸï¼ä½¿ç”¨GemmaTokenizer + SigLIPæ¨¡å‹")
            print(f"      éšè—å±‚å¤§å°: {self.hidden_size}")
            return True
            
        except Exception as e:
            print(f"   âŒ æ–¹æ³•2å¤±è´¥: {e}")
            return False
    
    def _try_auto_load(self, model_path):
        """æ–¹æ³•3ï¼šä½¿ç”¨AutoModelè‡ªåŠ¨åŠ è½½"""
        try:
            print(f"   ğŸ”„ æ–¹æ³•3ï¼šå°è¯•AutoModelè‡ªåŠ¨åŠ è½½...")
            
            # ä½¿ç”¨AutoTokenizerè‡ªåŠ¨æ£€æµ‹åˆ†è¯å™¨ç±»å‹
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                local_files_only=True,
                trust_remote_code=True  # å…è®¸è‡ªå®šä¹‰ä»£ç 
            )
            
            # ä½¿ç”¨AutoModelåŠ è½½æ¨¡å‹
            self.full_model = AutoModel.from_pretrained(
                model_path,
                local_files_only=True,
                torch_dtype=self.torch_dtype,
                trust_remote_code=True
            )
            
            # å°è¯•è·å–æ–‡æœ¬ç¼–ç å™¨
            if hasattr(self.full_model, 'text_model'):
                self.text_model = self.full_model.text_model
            elif hasattr(self.full_model, 'get_text_features'):
                self.text_model = self.full_model  # æ•´ä¸ªæ¨¡å‹å°±æ˜¯æ–‡æœ¬ç¼–ç å™¨
            else:
                raise ValueError("æ— æ³•æ‰¾åˆ°æ–‡æœ¬ç¼–ç å™¨ç»„ä»¶")
            
            # è·å–éšè—å±‚å¤§å°
            if hasattr(self.text_model, 'config'):
                self.hidden_size = self.text_model.config.hidden_size
            else:
                self.hidden_size = 1024  # é»˜è®¤å€¼
                
            self.text_model.eval()
            
            print(f"   âœ… æ–¹æ³•3æˆåŠŸï¼AutoModelåŠ è½½æˆåŠŸ")
            print(f"      æ¨¡å‹ç±»å‹: {type(self.full_model).__name__}")
            print(f"      åˆ†è¯å™¨ç±»å‹: {type(self.tokenizer).__name__}")
            print(f"      éšè—å±‚å¤§å°: {self.hidden_size}")
            return True
            
        except Exception as e:
            print(f"   âŒ æ–¹æ³•3å¤±è´¥: {e}")
            return False
    
    def _create_fallback_encoder(self):
        """æ–¹æ³•4ï¼šåˆ›å»ºå¤‡ç”¨ç¼–ç å™¨"""
        self.hidden_size = 1024  # ä¸è§†è§‰ç¼–ç å™¨åŒ¹é…
        self.text_model = None
        self.full_model = None
        
        # ç®€å•ä½†åŠŸèƒ½å®Œæ•´çš„tokenizer
        class ImprovedTokenizer:
            def __init__(self, max_length):
                self.max_length = max_length
                self.pad_token_id = 0
                self.vocab_size = 5000  # æ›´å¤§çš„è¯æ±‡è¡¨
                
                # åˆ›å»ºç®€å•çš„è¯æ±‡è¡¨
                self.vocab = {f"token_{i}": i for i in range(self.vocab_size)}
                self.vocab.update({
                    "<pad>": 0, "<unk>": 1, "<bos>": 2, "<eos>": 3,
                    " ": 4, ".": 5, ",": 6, "!": 7, "?": 8
                })
                
            def __call__(self, texts, max_length=None, padding="max_length", 
                        truncation=True, return_tensors="pt", **kwargs):
                if isinstance(texts, str):
                    texts = [texts]
                
                max_len = max_length or self.max_length
                input_ids = []
                attention_masks = []
                
                for text in texts:
                    # æ”¹è¿›çš„tokenization
                    words = text.lower().split()
                    tokens = [2]  # <bos>
                    
                    for word in words[:max_len-3]:  # ç•™å‡ºç©ºé—´ç»™ç‰¹æ®Štoken
                        if word in self.vocab:
                            tokens.append(self.vocab[word])
                        else:
                            # å­—ç¬¦çº§åˆ«ç¼–ç ä½œä¸ºåå¤‡
                            for char in word[:5]:  # é™åˆ¶å•è¯é•¿åº¦
                                tokens.append(min(ord(char), self.vocab_size-1))
                    
                    tokens.append(3)  # <eos>
                    
                    # Padding
                    attention_mask = [1] * len(tokens)
                    while len(tokens) < max_len:
                        tokens.append(0)  # <pad>
                        attention_mask.append(0)
                    
                    input_ids.append(tokens[:max_len])
                    attention_masks.append(attention_mask[:max_len])
                
                return {
                    'input_ids': torch.tensor(input_ids, dtype=torch.long),
                    'attention_mask': torch.tensor(attention_masks, dtype=torch.long)
                }
        
        self.tokenizer = ImprovedTokenizer(self.max_length)
        
        # åˆ›å»ºæ›´å¤æ‚çš„embeddingå±‚
        self.token_embedding = nn.Embedding(self.tokenizer.vocab_size, self.hidden_size)
        self.position_embedding = nn.Embedding(self.max_length, self.hidden_size)
        
        # æ·»åŠ ç®€å•çš„transformerå±‚
        self.transformer_layer = nn.TransformerEncoderLayer(
            d_model=self.hidden_size,
            nhead=8,
            dim_feedforward=self.hidden_size * 4,
            dropout=0.1,
            batch_first=True
        )
        
        self.layer_norm = nn.LayerNorm(self.hidden_size)
        
        print(f"   âœ… å¤‡ç”¨ç¼–ç å™¨åˆ›å»ºå®Œæˆï¼Œhidden_size: {self.hidden_size}")
        print(f"      è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")

    def forward(self, text_inputs):
        """
        ç¼–ç æ–‡æœ¬æŒ‡ä»¤
        Args:
            text_inputs: æ–‡æœ¬æŒ‡ä»¤åˆ—è¡¨æˆ–å·²ç¼–ç çš„token ids
        Returns:
            text_embeddings: (B, L, D) æ–‡æœ¬åµŒå…¥
            text_mask: (B, L) æ–‡æœ¬æ©ç 
        """
        device = next(self.parameters()).device if list(self.parameters()) else self.device
        
        # 1. Tokenization
        if isinstance(text_inputs, list):
            tokens = self.tokenizer(
                text_inputs,
                max_length=self.max_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )
            input_ids = tokens.input_ids.to(device)
            attention_mask = tokens.attention_mask.to(device)
        else:
            input_ids = text_inputs.to(device)
            attention_mask = (input_ids != getattr(self.tokenizer, 'pad_token_id', 0)).to(device)

        # 2. Forward pass
        if self.text_model is not None:
            # ä½¿ç”¨çœŸå®çš„SigLIP2/å…¶ä»–æ–‡æœ¬ç¼–ç å™¨
            try:
                with torch.no_grad():
                    if hasattr(self.text_model, '__call__'):
                        text_outputs = self.text_model(
                            input_ids=input_ids,
                            attention_mask=attention_mask
                        )
                        if hasattr(text_outputs, 'last_hidden_state'):
                            text_embeddings = text_outputs.last_hidden_state
                        else:
                            text_embeddings = text_outputs
                    else:
                        raise ValueError("æ–‡æœ¬æ¨¡å‹ä¸å¯è°ƒç”¨")
            except Exception as e:
                print(f"âš ï¸ æ–‡æœ¬æ¨¡å‹å‰å‘ä¼ æ’­å¤±è´¥: {e}")
                # å›é€€åˆ°å¤‡ç”¨æ–¹æ³•
                return self._fallback_forward(input_ids, attention_mask)
        else:
            # ä½¿ç”¨å¤‡ç”¨ç¼–ç å™¨
            return self._fallback_forward(input_ids, attention_mask)

        return text_embeddings, attention_mask.bool()
    
    def _fallback_forward(self, input_ids, attention_mask):
        """å¤‡ç”¨å‰å‘ä¼ æ’­"""
        batch_size, seq_length = input_ids.shape
        device = input_ids.device
        
        # Token embeddings
        token_embeds = self.token_embedding(input_ids)
        
        # Position embeddings
        positions = torch.arange(seq_length, device=device).unsqueeze(0).expand(batch_size, -1)
        pos_embeds = self.position_embedding(positions)
        
        # ç»„åˆ
        embeddings = token_embeds + pos_embeds
        
        # å¦‚æœæœ‰transformerå±‚ï¼Œåº”ç”¨å®ƒ
        if hasattr(self, 'transformer_layer'):
            # åˆ›å»ºpadding mask (Trueè¡¨ç¤ºéœ€è¦å¿½ç•¥çš„ä½ç½®)
            src_key_padding_mask = ~attention_mask.bool()
            embeddings = self.transformer_layer(
                embeddings, 
                src_key_padding_mask=src_key_padding_mask
            )
        
        # å½’ä¸€åŒ–
        text_embeddings = self.layer_norm(embeddings)
        
        return text_embeddings, attention_mask.bool()


# ğŸ§ª æµ‹è¯•å‡½æ•°
def test_text_encoder_loading():
    """æµ‹è¯•æ–‡æœ¬ç¼–ç å™¨åŠ è½½"""
    print("ğŸ§ª æµ‹è¯•SigLIP2æ–‡æœ¬ç¼–ç å™¨åŠ è½½...")
    
    # æµ‹è¯•ä¸åŒçš„æ¨¡å‹è·¯å¾„
    test_paths = [
        "/home/deng_xiang/qian_daichao/RoboTwin/policy/RDT_flare/siglip2-large-patch16-256",
        "google/siglip2-large-patch16-256",
        "./models/siglip2-large-patch16-256"
    ]
    
    for model_path in test_paths:
        print(f"\nğŸ“ æµ‹è¯•è·¯å¾„: {model_path}")
        
        try:
            encoder = SigLIP2TextEncoder(
                text_model_name=model_path,
                max_length=32
            )
            
            # æµ‹è¯•ç¼–ç 
            test_texts = ["pick up the red cube", "move to the left"]
            embeddings, mask = encoder(test_texts)
            
            print(f"âœ… æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {embeddings.shape}")
            print(f"   æ©ç å½¢çŠ¶: {mask.shape}")
            print(f"   æœ‰æ•ˆtokenæ•°: {mask.sum(dim=1)}")
            
            return encoder  # è¿”å›æˆåŠŸçš„ç¼–ç å™¨
            
        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}")
            continue
    
    print("\nâš ï¸ æ‰€æœ‰è·¯å¾„éƒ½å¤±è´¥äº†")
    return None


# ğŸ› ï¸ è¯Šæ–­å·¥å…·
def diagnose_model_files(model_path):
    """è¯Šæ–­æ¨¡å‹æ–‡ä»¶"""
    print(f"ğŸ” è¯Šæ–­æ¨¡å‹æ–‡ä»¶: {model_path}")
    
    if not os.path.exists(model_path):
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        "config.json",
        "tokenizer_config.json", 
        "tokenizer.json",
        "pytorch_model.bin",
        "model.safetensors"
    ]
    
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file}")
            
            # å¦‚æœæ˜¯é…ç½®æ–‡ä»¶ï¼Œæ˜¾ç¤ºå†…å®¹
            if file.endswith('.json'):
                try:
                    with open(file_path, 'r') as f:
                        config = json.load(f)
                        if 'tokenizer_class' in config:
                            print(f"   â””â”€â”€ tokenizer_class: {config['tokenizer_class']}")
                        if 'model_type' in config:
                            print(f"   â””â”€â”€ model_type: {config['model_type']}")
                except:
                    pass
        else:
            print(f"âŒ {file}")


if __name__ == "__main__":
    # å…ˆè¯Šæ–­æ–‡ä»¶
    model_path = "/home/deng_xiang/qian_daichao/RoboTwin/policy/RDT_flare/siglip2-large-patch16-256"
    diagnose_model_files(model_path)
    
    # ç„¶åæµ‹è¯•åŠ è½½
    test_text_encoder_loading()