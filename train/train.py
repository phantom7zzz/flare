#!/usr/bin/env python
# coding=utf-8
# FLAREå¢å¼ºçš„è®­ç»ƒè„šæœ¬ - A800 BF16ä¼˜åŒ–ç‰ˆ

import copy
import logging
import math
import os
from pathlib import Path

import diffusers
import torch
import torch.utils.checkpoint
import transformers
import yaml
from accelerate import Accelerator
from accelerate.utils import DeepSpeedPlugin, ProjectConfiguration, set_seed
from diffusers.optimization import get_scheduler
from diffusers.utils import is_wandb_available
from huggingface_hub import create_repo, upload_folder
from tqdm.auto import tqdm
from safetensors.torch import load_model

from models.ema_model import EMAModel
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
from models.multimodal_encoder.t5_encoder import T5Embedder
from models.rdt_runner import RDTRunnerWithFLARE
from train.dataset import DataCollatorForVLAConsumerDatasetWithFLARE, VLAConsumerDatasetWithFLARE
from train.sample import log_sample_res

if is_wandb_available():
    import wandb


def save_model_card(repo_id: str, base_model=str, repo_folder=None):
    yaml_content = f"""
---
license: mit
base_model: {base_model}
language:
- en
pipeline_tag: robotics
library_name: transformers
tags:
- robotics
- pytorch
- multimodal
- pretraining
- vla
- diffusion
- rdt
- flare
- bf16
- a800
---
    """
    model_card = f"""
# RDT-FLARE - {repo_id}

This is a FLARE-enhanced RDT model derived from {base_model}. The weights were trained using [RDT](https://rdt-robotics.github.io/rdt-robotics/) with FLARE (Future-conditioned Language-guided Action REpresentation) enhancement.

## FLARE Features
- Future observation alignment
- Vision-Language token fusion
- Q-Former target generation
- DiT activation alignment
- BF16 mixed precision training on A800

The model includes future observation prediction capabilities for improved action planning.
Optimized for A800 GPU with BF16 mixed precision training.
"""
    with open(os.path.join(repo_folder, "README.md"), "w") as f:
        f.write(yaml_content + model_card)


def configure_a800_optimizations():
    """é…ç½®A800ä¸“ç”¨ä¼˜åŒ–"""
    # ğŸ¯ A800 Tensor Coreä¼˜åŒ–
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    
    # ğŸ¯ BF16ä¼˜åŒ–
    torch.backends.cuda.enable_flash_sdp(True)  # Flash Attention
    
    # ğŸ¯ æ˜¾å­˜ä¼˜åŒ–
    torch.cuda.empty_cache()
    
    print("âœ… A800 GPUä¼˜åŒ–å·²å¯ç”¨:")
    print(f"   - Tensor Core TF32: {torch.backends.cuda.matmul.allow_tf32}")
    print(f"   - cuDNN TF32: {torch.backends.cudnn.allow_tf32}")
    print(f"   - Benchmarkæ¨¡å¼: {torch.backends.cudnn.benchmark}")
    if hasattr(torch.backends.cuda, "is_flash_attention_available"):
        print(f"   - Flash Attention: {torch.backends.cuda.is_flash_attention_available()}")
    else:
        print("   - Flash Attention: (å½“å‰PyTorchç‰ˆæœ¬ä¸æ”¯æŒè¯¥æ£€æµ‹)")


def check_gpu_capabilities(logger):
    """æ£€æŸ¥GPUèƒ½åŠ›å¹¶ä¼˜åŒ–é…ç½®"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {gpu_name}")
        logger.info(f"ğŸ¯ GPUæ˜¾å­˜: {gpu_memory:.1f}GB")
        
        # A800æ£€æµ‹å’Œä¼˜åŒ–
        if "A800" in gpu_name or gpu_memory > 70:
            logger.info("ğŸš€ A800 GPUæ£€æµ‹åˆ°ï¼Œå¯ç”¨ä¸“ç”¨ä¼˜åŒ–")
            
            # A800ä¸“ç”¨æ˜¾å­˜ç®¡ç†
            torch.cuda.set_per_process_memory_fraction(0.95)
            
            return True, gpu_memory
    
    return False, 0


def train(args, logger):
    # ğŸ¯ A800ä¼˜åŒ–é…ç½®
    configure_a800_optimizations()
    
    # Read the config
    with open(args.config_path, "r") as fp:
        config = yaml.safe_load(fp)

    with open(args.model_config_path, "r") as f:
        model_config = yaml.safe_load(f)
    
    args.output_dir = model_config["checkpoint_path"]
    logging_dir = Path(args.output_dir, args.logging_dir)

    # FLAREå‚æ•°ä»æ¨¡å‹é…ç½®æˆ–å‘½ä»¤è¡Œå‚æ•°ä¸­è¯»å–
    enable_flare = getattr(args, 'enable_flare', model_config.get('enable_flare', True))
    num_future_tokens = getattr(args, 'num_future_tokens', model_config.get('num_future_tokens', 32))
    activation_layer = getattr(args, 'activation_layer', model_config.get('activation_layer', 6))
    alignment_loss_weight = getattr(args, 'alignment_loss_weight', model_config.get('alignment_loss_weight', 0.1))

    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)
    accelerator = Accelerator(
        deepspeed_plugin=(DeepSpeedPlugin(hf_ds_config=args.deepspeed) if args.deepspeed is not None else None),
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_dir=logging_dir,
        project_config=accelerator_project_config,
    )
    is_a800, gpu_memory = check_gpu_capabilities(logger)
    if args.report_to == "wandb":
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

    # Make one log on every process with the configuration for debugging.
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger.info(accelerator.state, main_process_only=False)
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # If passed along, set the training seed now.
    if args.seed is not None:
        set_seed(args.seed)

    # Handle the repository creation
    if accelerator.is_main_process:
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        if args.push_to_hub:
            repo_id = create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
                token=args.hub_token,
            ).repo_id

    # ğŸ¯ ä¼˜åŒ–çš„æ•°æ®ç±»å‹é…ç½®
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
        logger.info("ğŸ¯ ä½¿ç”¨FP16æ··åˆç²¾åº¦")
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16
        logger.info("ğŸ¯ ä½¿ç”¨BF16æ··åˆç²¾åº¦ (æ¨èç”¨äºA800)")
    else:
        logger.info("ğŸ¯ ä½¿ç”¨FP32ç²¾åº¦")
    
    # A800å»ºè®®é…ç½®æ£€æŸ¥
    if is_a800 and accelerator.mixed_precision != "bf16":
        logger.warning("ğŸ’¡ å»ºè®®åœ¨A800ä¸Šä½¿ç”¨BF16æ··åˆç²¾åº¦ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    
    # åˆå§‹åŒ–ç¼–ç å™¨
    if args.precomp_lang_embed:
        tokenizer, text_encoder = None, None
    else:
        text_embedder = T5Embedder(
            from_pretrained=args.pretrained_text_encoder_name_or_path,
            model_max_length=config["dataset"]["tokenizer_max_length"],
            device=accelerator.device,
            torch_dtype=weight_dtype,  # ğŸ¯ æŒ‡å®šæ•°æ®ç±»å‹
        )
        tokenizer, text_encoder = text_embedder.tokenizer, text_embedder.model

    # ğŸ¯ åˆ›å»ºè§†è§‰ç¼–ç å™¨æ—¶æŒ‡å®šæœ¬åœ°æ–‡ä»¶
    vision_encoder = SiglipVisionTower(
        vision_tower=args.pretrained_vision_encoder_name_or_path, 
        args=None
    )
    image_processor = vision_encoder.image_processor

    # ğŸ¯ æ„å»ºFLAREå¢å¼ºçš„RDTæ¨¡å‹ - ä¿®å¤æ¨¡å‹åˆ›å»ºé€»è¾‘
    pretrained_path = args.pretrained_model_name_or_path
    
    if (pretrained_path is not None and 
        (os.path.isfile(pretrained_path) or os.path.isdir(pretrained_path))):
        
        logger.info(f"ä»é¢„è®­ç»ƒè·¯å¾„æ„å»ºFLAREæ¨¡å‹: {pretrained_path}")
        
        # é¦–å…ˆåˆ›å»ºFLAREæ¨¡å‹æ¶æ„
        img_cond_len = (config["common"]["img_history_size"] * config["common"]["num_cameras"] *
                        vision_encoder.num_patches)
        
        rdt = RDTRunnerWithFLARE(
            action_dim=config["common"]["state_dim"],
            pred_horizon=config["common"]["action_chunk_size"],
            config=config["model"],
            lang_token_dim=config["model"]["lang_token_dim"],
            img_token_dim=config["model"]["img_token_dim"],
            state_token_dim=config["model"]["state_token_dim"],
            max_lang_cond_len=config["dataset"]["tokenizer_max_length"],
            img_cond_len=img_cond_len,
            img_pos_embed_config=[
                ("image", (
                    config["common"]["img_history_size"],
                    config["common"]["num_cameras"],
                    -vision_encoder.num_patches,
                )),
            ],
            lang_pos_embed_config=[
                ("lang", -config["dataset"]["tokenizer_max_length"]),
            ],
            dtype=weight_dtype,
            # FLAREç‰¹å®šå‚æ•°
            num_future_tokens=num_future_tokens,
            activation_layer=activation_layer,
            alignment_loss_weight=alignment_loss_weight,
            enable_flare=enable_flare,
        )
        
        # ğŸ¯ å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæ˜¯æ–‡ä»¶ï¼‰
        if os.path.isfile(pretrained_path):
            try:
                logger.info(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
                checkpoint = torch.load(pretrained_path, map_location='cpu')
                rdt.load_state_dict(checkpoint["module"], strict=False)
                logger.info("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸï¼ˆéƒ¨åˆ†å‚æ•°ï¼ŒFLAREç»„ä»¶éšæœºåˆå§‹åŒ–ï¼‰")
            except Exception as e:
                logger.warning(f"âš ï¸  é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–: {e}")
        else:
            logger.info("ä½¿ç”¨ç›®å½•è·¯å¾„ï¼Œè·³è¿‡æƒé‡åŠ è½½")
            
    else:
        logger.info("ä»é…ç½®æ–‡ä»¶æ„å»ºFLAREæ¨¡å‹ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰")
        rdt = create_flare_model_from_standard_rdt(args, config, vision_encoder, weight_dtype)

    # ğŸ¯ ç¡®ä¿æ¨¡å‹æ•°æ®ç±»å‹æ­£ç¡®
    rdt = rdt.to(dtype=weight_dtype)
    
    # æ•°æ®ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥
    logger.info("ğŸ” æ£€æŸ¥æ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´æ€§...")
    dtype_issues = []
    for name, param in rdt.named_parameters():
        if param.dtype != weight_dtype:
            dtype_issues.append(f"{name}: {param.dtype}")
    
    if dtype_issues:
        logger.warning(f"âš ï¸  å‘ç°æ•°æ®ç±»å‹ä¸ä¸€è‡´: {len(dtype_issues)} ä¸ªå‚æ•°")
        # å¼ºåˆ¶è½¬æ¢
        rdt = rdt.to(weight_dtype)
        logger.info("âœ… å·²å¼ºåˆ¶è½¬æ¢æ‰€æœ‰å‚æ•°åˆ°ç»Ÿä¸€æ•°æ®ç±»å‹")
    else:
        logger.info("âœ… æ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")

    # EMAæ¨¡å‹
    ema_rdt = copy.deepcopy(rdt)
    ema_model = EMAModel(
        ema_rdt,
        update_after_step=config["model"]["ema"]["update_after_step"],
        inv_gamma=config["model"]["ema"]["inv_gamma"],
        power=config["model"]["ema"]["power"],
        min_value=config["model"]["ema"]["min_value"],
        max_value=config["model"]["ema"]["max_value"],
    )

    # create custom saving & loading hooks
    def save_model_hook(models, weights, output_dir):
        if accelerator.is_main_process:
            for model in models:
                model_to_save = model.module if hasattr(model, "module") else model
                if isinstance(model_to_save, type(accelerator.unwrap_model(rdt))):
                    model_to_save.save_pretrained(output_dir)

    accelerator.register_save_state_pre_hook(save_model_hook)

    if args.gradient_checkpointing:
        logger.warning("æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½æš‚æœªåœ¨FLAREä¸­å®ç°")

    # Enable TF32 for faster training on Ampere GPUs
    if args.allow_tf32:
        torch.backends.cuda.matmul.allow_tf32 = True

    if args.scale_lr:
        original_lr = args.learning_rate
        args.learning_rate = (args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size *
                              accelerator.num_processes)
        logger.info(f"ğŸ¯ å­¦ä¹ ç‡ç¼©æ”¾: {original_lr} -> {args.learning_rate}")

    # ğŸ¯ A800ä¼˜åŒ–çš„batch sizeå»ºè®®
    if is_a800 and args.train_batch_size < 24:
        logger.info(f"ğŸ’¡ A800å»ºè®®ä½¿ç”¨æ›´å¤§çš„batch_size (å½“å‰: {args.train_batch_size}, å»ºè®®: 24-32)")

    # Optimizer creation
    if args.use_8bit_adam:
        try:
            import bitsandbytes as bnb
        except ImportError:
            raise ImportError("To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.")
        optimizer_class = bnb.optim.AdamW8bit
        logger.info("ğŸ¯ ä½¿ç”¨8bit Adamä¼˜åŒ–å™¨")
    else:
        optimizer_class = torch.optim.AdamW
        logger.info("ğŸ¯ ä½¿ç”¨æ ‡å‡†AdamWä¼˜åŒ–å™¨")

    params_to_optimize = rdt.parameters()
    optimizer = optimizer_class(
        params_to_optimize,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )

    # åˆ›å»ºFLAREå¢å¼ºçš„æ•°æ®é›†
    train_dataset = VLAConsumerDatasetWithFLARE(
        model_config_path=args.model_config_path,
        config=config["dataset"],
        tokenizer=tokenizer,
        image_processor=image_processor,
        num_cameras=config["common"]["num_cameras"],
        img_history_size=config["common"]["img_history_size"],
        dataset_type=args.dataset_type,
        image_aug=args.image_aug,
        cond_mask_prob=args.cond_mask_prob,
        cam_ext_mask_prob=args.cam_ext_mask_prob,
        state_noise_snr=args.state_noise_snr,
        use_hdf5=args.load_from_hdf5,
        use_precomp_lang_embed=args.precomp_lang_embed,
        # FLAREå‚æ•°
        enable_future_obs=enable_flare,
        future_obs_prob=model_config.get('future_obs_prob', 0.8),
        action_chunk_size=config["common"]["action_chunk_size"],
    )
    
    sample_dataset = VLAConsumerDatasetWithFLARE(
        model_config_path=args.model_config_path,
        config=config["dataset"],
        tokenizer=tokenizer,
        image_processor=image_processor,
        num_cameras=config["common"]["num_cameras"],
        img_history_size=config["common"]["img_history_size"],
        dataset_type=args.dataset_type,
        image_aug=False,
        cond_mask_prob=0,
        cam_ext_mask_prob=-1,
        state_noise_snr=None,
        use_hdf5=args.load_from_hdf5,
        use_precomp_lang_embed=args.precomp_lang_embed,
        # FLAREå‚æ•°
        enable_future_obs=False,  # é‡‡æ ·æ—¶ä¸ä½¿ç”¨æœªæ¥è§‚æµ‹
        future_obs_prob=0.0,
        action_chunk_size=config["common"]["action_chunk_size"],
    )

    data_collator = DataCollatorForVLAConsumerDatasetWithFLARE(tokenizer)

    # ğŸ¯ A800ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨é…ç½®
    num_workers = args.dataloader_num_workers
    if is_a800 and num_workers < 12:
        num_workers = min(16, num_workers * 2)
        logger.info(f"ğŸ¯ A800ä¼˜åŒ–: æ•°æ®åŠ è½½å™¨workerså¢åŠ åˆ° {num_workers}")

    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.train_batch_size,
        shuffle=True,
        collate_fn=data_collator,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
    )
    sample_dataloader = torch.utils.data.DataLoader(
        sample_dataset,
        batch_size=args.sample_batch_size,
        shuffle=True,
        collate_fn=data_collator,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
    )

    # Scheduler and math around the number of training steps
    overrode_max_train_steps = False
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
        num_cycles=args.lr_num_cycles,
        power=args.lr_power,
    )

    # Prepare everything with accelerator
    rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler = (accelerator.prepare(
        rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler))

    # ğŸ¯ å°†æ¨¡å‹ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹
    rdt.to(accelerator.device, dtype=weight_dtype)
    ema_rdt.to(accelerator.device, dtype=weight_dtype)

    if text_encoder is not None:
        text_encoder.to(accelerator.device, dtype=weight_dtype)

    if vision_encoder is not None:
        vision_encoder.vision_tower.to(accelerator.device, dtype=weight_dtype)

    # Recalculate training steps
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # Initialize trackers
    if accelerator.is_main_process:
        tracker_config = vars(args)
        tracker_config.update({
            "enable_flare": enable_flare,
            "num_future_tokens": num_future_tokens,
            "activation_layer": activation_layer,
            "alignment_loss_weight": alignment_loss_weight,
            "weight_dtype": str(weight_dtype),
            "is_a800": is_a800,
            "gpu_memory_gb": gpu_memory,
        })
        
        accelerator.init_trackers(
            "VLA_FLARE",
            config=tracker_config,
            init_kwargs={"wandb": {
                "name": f"RDT_FLARE_{args.CONFIG_NAME}_{accelerator.mixed_precision}",
                "tags": ["rdt", "flare", "multimodal", "robotics", "a800", accelerator.mixed_precision],
            }},
        )

    # Training info
    total_batch_size = (args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps)

    logger.info("***** Running FLARE training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Num batches each epoch = {len(train_dataloader)}")
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {args.max_train_steps}")
    logger.info(f"  Mixed precision = {accelerator.mixed_precision}")
    logger.info(f"  Weight dtype = {weight_dtype}")
    logger.info(f"  FLARE enabled = {enable_flare}")
    logger.info(f"  Future tokens = {num_future_tokens}")
    logger.info(f"  Activation layer = {activation_layer}")
    logger.info(f"  Alignment loss weight = {alignment_loss_weight}")
    
    global_step = 0
    first_epoch = 0

    # Load from pretrained checkpoint - ç§»åˆ°æ¨¡å‹åˆ›å»ºå
    # (æƒé‡åŠ è½½å·²ç»åœ¨æ¨¡å‹åˆ›å»ºæ—¶å¤„ç†)

    # Resume from checkpoint
    if args.resume_from_checkpoint:
        if args.resume_from_checkpoint != "latest":
            path = os.path.basename(args.resume_from_checkpoint)
        else:
            # Get the most recent checkpoint
            dirs = os.listdir(args.output_dir)
            dirs = [d for d in dirs if d.startswith("checkpoint")]
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
            path = dirs[-1] if len(dirs) > 0 else None

        if path is None:
            accelerator.print(
                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
            args.resume_from_checkpoint = None
        else:
            accelerator.print(f"Resuming from checkpoint {path}")
            try:
                accelerator.load_state(os.path.join(args.output_dir, path))
            except:
                logger.info("Resuming training state failed. Attempting to only load from model checkpoint.")
                checkpoint = torch.load(
                    os.path.join(
                        args.output_dir,
                        path,
                        "pytorch_model",
                        "mp_rank_00_model_states.pt",
                    ))
                unwrapped_rdt = accelerator.unwrap_model(rdt)
                unwrapped_rdt.load_state_dict(checkpoint["module"], strict=False)

            load_model(ema_rdt, os.path.join(args.output_dir, path, "ema", "model.safetensors"))
            global_step = int(path.split("-")[1])

            resume_global_step = global_step * args.gradient_accumulation_steps
            first_epoch = global_step // num_update_steps_per_epoch
            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)

    # Progress bar
    progress_bar = tqdm(
        range(global_step, args.max_train_steps),
        disable=not accelerator.is_local_main_process,
    )
    progress_bar.set_description(f"FLARE Training ({accelerator.mixed_precision})")

    # Training metrics
    loss_for_log = {}
    alignment_metrics_log = {}
    
    for epoch in range(first_epoch, args.num_train_epochs):
        rdt.train()

        # Set progress bar to correct position
        if args.resume_from_checkpoint and epoch == first_epoch:
            progress_bar.update(resume_step // args.gradient_accumulation_steps)

        # ğŸ¯ FLAREè®­ç»ƒå¾ªç¯ - æ”¯æŒBF16æ··åˆç²¾åº¦
        for batch in train_dataloader:
            with accelerator.accumulate(rdt):
                # ğŸ¯ ä½¿ç”¨autocaståŒ…è£…å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
                with torch.autocast(device_type="cuda", dtype=weight_dtype, enabled=(accelerator.mixed_precision != "no")):
                    # åŸºç¡€æ•°æ®å‡†å¤‡
                    images = batch["images"]
                    states = batch["states"]
                    states = states[:, -1:, :]  # åªä½¿ç”¨æœ€åä¸€ä¸ªçŠ¶æ€
                    actions = batch["actions"]
                    state_elem_mask = batch["state_elem_mask"]
                    ctrl_freqs = batch["ctrl_freqs"]

                    # FLAREç‰¹å®šæ•°æ®
                    future_obs_images = batch.get("future_obs_images")
                    text_instructions = batch.get("text_instructions", [""] * images.shape[0])
                    has_future_obs = batch.get("has_future_obs")

                    with torch.no_grad():
                        # ç¼–ç å›¾åƒ
                        batch_size, _, C, H, W = images.shape
                        image_embeds = vision_encoder(images.reshape(-1, C, H, W)).detach()
                        image_embeds = image_embeds.reshape((batch_size, -1, vision_encoder.hidden_size))

                        # ç¼–ç è¯­è¨€
                        lang_attn_mask = batch["lang_attn_mask"]
                        if args.precomp_lang_embed:
                            text_embeds = batch["lang_embeds"]
                        else:
                            text_embeds = text_encoder(
                                input_ids=batch["input_ids"], 
                                attention_mask=lang_attn_mask
                            )["last_hidden_state"].detach()

                        # ç¼–ç æœªæ¥è§‚æµ‹å›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        future_vision_embeds = None
                        if future_obs_images is not None and enable_flare:
                            future_vision_embeds = vision_encoder(future_obs_images).detach()

                    state_elem_mask = state_elem_mask.unsqueeze(1)
                    
                    # è®¡ç®—FLAREå¢å¼ºçš„æŸå¤±
                    unwrapped_rdt = accelerator.unwrap_model(rdt)
                    total_loss, loss_dict = unwrapped_rdt.compute_loss_with_flare(
                        lang_tokens=text_embeds,
                        lang_attn_mask=lang_attn_mask,
                        img_tokens=image_embeds,
                        state_tokens=states,
                        action_gt=actions,
                        action_mask=state_elem_mask,
                        ctrl_freqs=ctrl_freqs,
                        future_vision_tokens=future_vision_embeds,
                        text_instructions=text_instructions,
                        has_future_obs=has_future_obs,
                        future_obs_images=batch.get("future_obs_images")
                    )

                # åå‘ä¼ æ’­ï¼ˆacceleratorä¼šè‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦ï¼‰
                accelerator.backward(total_loss)
                
                if accelerator.sync_gradients:
                    params_to_clip = rdt.parameters()
                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad(set_to_none=args.set_grads_to_none)

            ema_model.step(accelerator.unwrap_model(rdt))

            # Check if optimization step occurred
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

                # Checkpointing
                if global_step % args.checkpointing_period == 0:
                    save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                    accelerator.save_state(save_path)
                    ema_save_path = os.path.join(save_path, f"ema")
                    accelerator.save_model(ema_rdt, ema_save_path)
                    logger.info(f"Saved state to {save_path}")

                # Sampling and evaluation
                if args.sample_period > 0 and global_step % args.sample_period == 0:
                    sample_loss_for_log = log_sample_res(
                        text_encoder,
                        vision_encoder,
                        rdt,
                        args,
                        accelerator,
                        weight_dtype,
                        sample_dataset.get_dataset_id2name(),
                        sample_dataloader,
                        logger,
                    )
                    logger.info(sample_loss_for_log)
                    accelerator.log(sample_loss_for_log, step=global_step)

                # è·å–å¯¹é½æŒ‡æ ‡
                if enable_flare and global_step % 100 == 0:
                    try:
                        unwrapped_rdt = accelerator.unwrap_model(rdt)
                        alignment_metrics = unwrapped_rdt.get_alignment_metrics()
                        if alignment_metrics:
                            alignment_metrics_log.update({f"alignment_{k}": v for k, v in alignment_metrics.items()})
                    except Exception as e:
                        logger.debug(f"Failed to get alignment metrics: {e}")

                # ğŸ¯ A800æ€§èƒ½ç›‘æ§
                if is_a800 and global_step % 500 == 0:
                    try:
                        gpu_utilization = torch.cuda.utilization()
                        memory_allocated = torch.cuda.memory_allocated() / 1024**3
                        memory_reserved = torch.cuda.memory_reserved() / 1024**3
                        
                        performance_metrics = {
                            "gpu_utilization": gpu_utilization,
                            "memory_allocated_gb": memory_allocated,
                            "memory_reserved_gb": memory_reserved,
                            "memory_usage_ratio": memory_allocated / gpu_memory if gpu_memory > 0 else 0,
                        }
                        logger.info(f"ğŸ¯ A800æ€§èƒ½: GPUåˆ©ç”¨ç‡={gpu_utilization}%, æ˜¾å­˜={memory_allocated:.1f}GB/{gpu_memory:.1f}GB")
                        accelerator.log(performance_metrics, step=global_step)
                    except Exception as e:
                        logger.debug(f"Failed to get performance metrics: {e}")

            # è®°å½•æŸå¤±
            logs = {
                "loss": total_loss.detach().item(), 
                "lr": lr_scheduler.get_last_lr()[0],
                "diffusion_loss": loss_dict.get('diffusion_loss', 0.0),
                "alignment_loss": loss_dict.get('alignment_loss', 0.0),
                "used_flare": float(loss_dict.get('used_flare', False)),
                "epoch": epoch,
            }
            
            # ğŸ¯ BF16ç‰¹å®šæŒ‡æ ‡
            if accelerator.mixed_precision == "bf16":
                logs["bf16_training"] = True
                # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°ï¼ˆBF16è®­ç»ƒä¸­å¾ˆé‡è¦ï¼‰
                try:
                    total_norm = 0.0
                    for p in rdt.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm = total_norm ** (1. / 2)
                    logs["grad_norm"] = total_norm
                except:
                    pass
            
            progress_bar.set_postfix(**{k: f"{v:.4f}" if isinstance(v, float) else str(v) for k, v in logs.items()})
            logs.update(loss_for_log)
            logs.update(alignment_metrics_log)
            accelerator.log(logs, step=global_step)

            if global_step >= args.max_train_steps:
                break

    # Save final model
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        accelerator.unwrap_model(rdt).save_pretrained(args.output_dir)
        ema_save_path = os.path.join(args.output_dir, f"ema")
        accelerator.save_model(ema_rdt, ema_save_path)

        logger.info(f"âœ… FLAREæ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")
        
        # ğŸ¯ ä¿å­˜è®­ç»ƒé…ç½®å’Œæ€§èƒ½ä¿¡æ¯
        training_info = {
            "mixed_precision": accelerator.mixed_precision,
            "weight_dtype": str(weight_dtype),
            "is_a800": is_a800,
            "gpu_memory_gb": gpu_memory,
            "final_loss": logs.get("loss", 0.0),
            "total_steps": global_step,
            "enable_flare": enable_flare,
            "num_future_tokens": num_future_tokens,
            "alignment_loss_weight": alignment_loss_weight,
        }
        
        import json
        with open(os.path.join(args.output_dir, "training_info.json"), "w") as f:
            json.dump(training_info, f, indent=2)

        if args.push_to_hub:
            save_model_card(
                repo_id,
                base_model=args.pretrained_model_name_or_path,
                repo_folder=args.output_dir,
            )
            upload_folder(
                repo_id=repo_id,
                folder_path=args.output_dir,
                commit_message="End of FLARE training with BF16 mixed precision",
                token=args.hub_token,
                allow_patterns=["pytorch_model.bin", "*.json", "*.md"],
            )

    accelerator.end_training()


def create_flare_model_from_standard_rdt(args, config, vision_encoder, weight_dtype):
    """
    åˆ›å»ºFLAREæ¨¡å‹ï¼ˆæ”¯æŒä»é¢„è®­ç»ƒRDTåˆå§‹åŒ–æˆ–å®Œå…¨éšæœºåˆå§‹åŒ–ï¼‰
    """
    from models.rdt_runner import RDTRunnerWithFLARE
    import logging
    
    logger = logging.getLogger(__name__)
    
    # è®¡ç®—å›¾åƒæ¡ä»¶é•¿åº¦
    img_cond_len = (config["common"]["img_history_size"] * config["common"]["num_cameras"] *
                    vision_encoder.num_patches)
    
    logger.info("åˆ›å»ºFLAREå¢å¼ºçš„RDTæ¨¡å‹...")
    
    # åˆ›å»ºFLAREæ¨¡å‹
    flare_rdt = RDTRunnerWithFLARE(
        action_dim=config["common"]["state_dim"],
        pred_horizon=config["common"]["action_chunk_size"],
        config=config["model"],
        lang_token_dim=config["model"]["lang_token_dim"],
        img_token_dim=config["model"]["img_token_dim"],
        state_token_dim=config["model"]["state_token_dim"],
        max_lang_cond_len=config["dataset"]["tokenizer_max_length"],
        img_cond_len=img_cond_len,
        img_pos_embed_config=[
            ("image", (
                config["common"]["img_history_size"],
                config["common"]["num_cameras"],
                -vision_encoder.num_patches,
            )),
        ],
        lang_pos_embed_config=[
            ("lang", -config["dataset"]["tokenizer_max_length"]),
        ],
        dtype=weight_dtype,
        # FLAREå‚æ•°
        num_future_tokens=getattr(args, 'num_future_tokens', 32),
        activation_layer=getattr(args, 'activation_layer', 6),
        alignment_loss_weight=getattr(args, 'alignment_loss_weight', 0.1),
        enable_flare=getattr(args, 'enable_flare', True),
    )
    
    # ç¡®ä¿æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹æ­£ç¡®
    flare_rdt = flare_rdt.to(dtype=weight_dtype)
    
    logger.info(f"âœ… FLAREæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    logger.info(f"   å‚æ•°æ€»æ•°: {sum(p.numel() for p in flare_rdt.parameters()):,}")
    logger.info(f"   æ¨¡å‹æ•°æ®ç±»å‹: {weight_dtype}")
    logger.info(f"   FLAREç»„ä»¶: æ–°å¢ VL Tokenç”Ÿæˆå™¨ã€Q-Formerã€æ¿€æ´»å¯¹é½å™¨")
    
    return flare_rdt


def monitor_training_health(loss_dict, global_step, logger):
    """ç›‘æ§è®­ç»ƒå¥åº·çŠ¶å†µ"""
    diffusion_loss = loss_dict.get('diffusion_loss', 0.0)
    alignment_loss = loss_dict.get('alignment_loss', 0.0)
    
    # æ£€æŸ¥NaNæˆ–å¼‚å¸¸å€¼
    if math.isnan(diffusion_loss) or math.isnan(alignment_loss):
        logger.error(f"âš ï¸  Step {global_step}: æ£€æµ‹åˆ°NaNæŸå¤±å€¼!")
        return False
        
    # æ£€æŸ¥æŸå¤±çˆ†ç‚¸
    if diffusion_loss > 100 or alignment_loss > 100:
        logger.warning(f"âš ï¸  Step {global_step}: æŸå¤±å€¼å¼‚å¸¸å¤§ (diffusion: {diffusion_loss:.3f}, alignment: {alignment_loss:.3f})")
        
    # è®°å½•å¥åº·çŠ¶æ€
    if global_step % 1000 == 0:
        logger.info(f"ğŸ’Š è®­ç»ƒå¥åº·æ£€æŸ¥ Step {global_step}:")
        logger.info(f"   æ‰©æ•£æŸå¤±: {diffusion_loss:.4f}")
        logger.info(f"   å¯¹é½æŸå¤±: {alignment_loss:.4f}")
        logger.info(f"   ä½¿ç”¨FLARE: {loss_dict.get('used_flare', False)}")
    
    return True