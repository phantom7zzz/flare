#!/usr/bin/env python
# coding=utf-8
# FLAREå¢å¼ºçš„è®­ç»ƒè„šæœ¬ - åŒç¼–ç å™¨ç‰ˆæœ¬ A800 BF16ä¼˜åŒ–

import copy
import logging
import math
import os
from pathlib import Path

import diffusers
import torch
import torch.utils.checkpoint
import transformers
import yaml
from accelerate import Accelerator
from accelerate.utils import DeepSpeedPlugin, ProjectConfiguration, set_seed
from diffusers.optimization import get_scheduler
from diffusers.utils import is_wandb_available
from huggingface_hub import create_repo, upload_folder
from tqdm.auto import tqdm
from safetensors.torch import load_model

from models.ema_model import EMAModel
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
from models.multimodal_encoder.t5_encoder import T5Embedder
from models.rdt_runner import RDTRunnerWithFLARE
from train.dataset import DataCollatorForVLAConsumerDatasetWithFLARE, VLAConsumerDatasetWithFLARE
from train.sample import log_sample_res

if is_wandb_available():
    import wandb
torch.autograd.set_detect_anomaly(True)

def save_model_card(repo_id: str, base_model=str, repo_folder=None):
    yaml_content = f"""
---
license: mit
base_model: {base_model}
language:
- en
pipeline_tag: robotics
library_name: transformers
tags:
- robotics
- pytorch
- multimodal
- pretraining
- vla
- diffusion
- rdt
- flare
- dual-encoder
- bf16
- a800
---
    """
    model_card = f"""
# RDT-FLARE Dual Encoder - {repo_id}

This is a FLARE-enhanced RDT model with dual vision encoders derived from {base_model}. 

## Dual Encoder Architecture
- **Current Image Encoder**: SigLIP-384 for processing current observations â†’ DiT layers
- **Future Observation Encoder**: SigLIP2-256 for processing future observations â†’ FLARE components

## FLARE Features
- Future observation alignment with dual encoder architecture
- Vision-Language token fusion (SigLIP2-256)
- Q-Former target generation
- DiT activation alignment
- BF16 mixed precision training on A800

The model includes future observation prediction capabilities with specialized encoders for improved action planning.
Optimized for A800 GPU with BF16 mixed precision training.
"""
    with open(os.path.join(repo_folder, "README.md"), "w") as f:
        f.write(yaml_content + model_card)


def configure_a800_optimizations():
    """é…ç½®A800ä¸“ç”¨ä¼˜åŒ–"""
    # ğŸ¯ A800 Tensor Coreä¼˜åŒ–
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    
    # ğŸ¯ BF16ä¼˜åŒ–
    torch.backends.cuda.enable_flash_sdp(True)  # Flash Attention
    
    # ğŸ¯ æ˜¾å­˜ä¼˜åŒ–
    torch.cuda.empty_cache()
    
    print("âœ… A800 GPUä¼˜åŒ–å·²å¯ç”¨:")
    print(f"   - Tensor Core TF32: {torch.backends.cuda.matmul.allow_tf32}")
    print(f"   - cuDNN TF32: {torch.backends.cudnn.allow_tf32}")
    print(f"   - Benchmarkæ¨¡å¼: {torch.backends.cudnn.benchmark}")
    if hasattr(torch.backends.cuda, "is_flash_attention_available"):
        print(f"   - Flash Attention: {torch.backends.cuda.is_flash_attention_available()}")
    else:
        print("   - Flash Attention: (å½“å‰PyTorchç‰ˆæœ¬ä¸æ”¯æŒè¯¥æ£€æµ‹)")


def check_gpu_capabilities(logger):
    """æ£€æŸ¥GPUèƒ½åŠ›å¹¶ä¼˜åŒ–é…ç½®"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {gpu_name}")
        logger.info(f"ğŸ¯ GPUæ˜¾å­˜: {gpu_memory:.1f}GB")
        
        # A800æ£€æµ‹å’Œä¼˜åŒ–
        if "A800" in gpu_name or gpu_memory > 70:
            logger.info("ğŸš€ A800 GPUæ£€æµ‹åˆ°ï¼Œå¯ç”¨ä¸“ç”¨ä¼˜åŒ–")
            
            # A800ä¸“ç”¨æ˜¾å­˜ç®¡ç†
            torch.cuda.set_per_process_memory_fraction(0.95)
            
            return True, gpu_memory
    
    return False, 0


def train(args, logger):
    # ğŸ¯ A800ä¼˜åŒ–é…ç½®
    configure_a800_optimizations()
    
    # Read the config
    with open(args.config_path, "r") as fp:
        config = yaml.safe_load(fp)

    with open(args.model_config_path, "r") as f:
        model_config = yaml.safe_load(f)
    
    args.output_dir = model_config["checkpoint_path"]
    logging_dir = Path(args.output_dir, args.logging_dir)

    # ğŸ”§ FLAREå‚æ•°ä»æ¨¡å‹é…ç½®æˆ–å‘½ä»¤è¡Œå‚æ•°ä¸­è¯»å–
    enable_flare = getattr(args, 'enable_flare', model_config.get('enable_flare', True))
    num_future_tokens = getattr(args, 'num_future_tokens', model_config.get('num_future_tokens', 32))
    activation_layer = getattr(args, 'activation_layer', model_config.get('activation_layer', 21))
    alignment_loss_weight = getattr(args, 'alignment_loss_weight', model_config.get('alignment_loss_weight', 0.2))

    # ğŸ”§ åŒç¼–ç å™¨è·¯å¾„é…ç½®
    current_vision_path = args.pretrained_vision_encoder_name_or_path
    future_vision_path = getattr(args, 'future_vision_encoder_path', './models/siglip2-large-patch16-256')
    future_text_path = getattr(args, 'future_text_encoder_path', None) or future_vision_path
    max_text_length = getattr(args, 'max_text_length', 32)
    current_vision_image_size = getattr(args, 'current_vision_image_size', 384)
    future_vision_image_size = getattr(args, 'future_vision_image_size', 256)

    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)
    accelerator = Accelerator(
        deepspeed_plugin=(DeepSpeedPlugin(hf_ds_config=args.deepspeed) if args.deepspeed is not None else None),
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_dir=logging_dir,
        project_config=accelerator_project_config,
    )
    is_a800, gpu_memory = check_gpu_capabilities(logger)
    
    if args.report_to == "wandb":
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

    # Make one log on every process with the configuration for debugging.
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger.info(accelerator.state, main_process_only=False)
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # If passed along, set the training seed now.
    if args.seed is not None:
        set_seed(args.seed)

    # Handle the repository creation
    if accelerator.is_main_process:
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        if args.push_to_hub:
            repo_id = create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
                token=args.hub_token,
            ).repo_id

    # ğŸ¯ ä¼˜åŒ–çš„æ•°æ®ç±»å‹é…ç½®
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
        logger.info("ğŸ¯ ä½¿ç”¨FP16æ··åˆç²¾åº¦")
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16
        logger.info("ğŸ¯ ä½¿ç”¨BF16æ··åˆç²¾åº¦ (æ¨èç”¨äºA800)")
    else:
        logger.info("ğŸ¯ ä½¿ç”¨FP32ç²¾åº¦")
    
    # A800å»ºè®®é…ç½®æ£€æŸ¥
    if is_a800 and accelerator.mixed_precision != "bf16":
        logger.warning("ğŸ’¡ å»ºè®®åœ¨A800ä¸Šä½¿ç”¨BF16æ··åˆç²¾åº¦ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    
    # ğŸ”§ åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨
    if args.precomp_lang_embed:
        tokenizer, text_encoder = None, None
    else:
        text_embedder = T5Embedder(
            from_pretrained=args.pretrained_text_encoder_name_or_path,
            model_max_length=config["dataset"]["tokenizer_max_length"],
            device=accelerator.device,
            torch_dtype=weight_dtype,
        )
        tokenizer, text_encoder = text_embedder.tokenizer, text_embedder.model

    # ğŸ”§ é…ç½®å¹¶æ˜¾ç¤ºåŒè§†è§‰ç¼–ç å™¨ç³»ç»Ÿ
    logger.info("=" * 80)
    logger.info("ğŸ”§ é…ç½®åŒè§†è§‰ç¼–ç å™¨ç³»ç»Ÿ:")
    logger.info("=" * 80)
    logger.info(f"ğŸ“· å½“å‰å›¾åƒç¼–ç å™¨(SigLIP-384):")
    logger.info(f"   è·¯å¾„: {current_vision_path}")
    logger.info(f"   å›¾åƒå°ºå¯¸: {current_vision_image_size}x{current_vision_image_size}")
    logger.info(f"   åŠŸèƒ½: å¤„ç†å½“å‰è§‚æµ‹å›¾åƒ â†’ DiT layers â†’ åŠ¨ä½œé¢„æµ‹")
    logger.info("")
    logger.info(f"ğŸ”® æœªæ¥è§‚æµ‹ç¼–ç å™¨(SigLIP2-256):")
    logger.info(f"   è§†è§‰è·¯å¾„: {future_vision_path}")
    logger.info(f"   æ–‡æœ¬è·¯å¾„: {future_text_path}")
    logger.info(f"   å›¾åƒå°ºå¯¸: {future_vision_image_size}x{future_vision_image_size}")
    logger.info(f"   æ–‡æœ¬é•¿åº¦: {max_text_length} tokens")
    logger.info(f"   åŠŸèƒ½: å¤„ç†æœªæ¥è§‚æµ‹å›¾åƒ+æŒ‡ä»¤ â†’ FLARE â†’ ç›®æ ‡ç”Ÿæˆ")
    logger.info("=" * 80)
    
    # ğŸ”§ åˆ›å»ºå½“å‰å›¾åƒçš„è§†è§‰ç¼–ç å™¨ï¼ˆSigLIP-384ï¼‰- ç”¨äºDiTå¤„ç†
    logger.info("ğŸ“· åˆå§‹åŒ–å½“å‰å›¾åƒç¼–ç å™¨ï¼ˆSigLIP-384ï¼‰...")
    current_vision_encoder = SiglipVisionTower(
        vision_tower=current_vision_path,
        args=None
    )
    image_processor = current_vision_encoder.image_processor
    
    logger.info(f"âœ… å½“å‰å›¾åƒç¼–ç å™¨åŠ è½½å®Œæˆ:")
    logger.info(f"   æ¨¡å‹: {current_vision_encoder.vision_tower_name}")
    logger.info(f"   Hidden size: {current_vision_encoder.hidden_size}")
    logger.info(f"   Num patches: {current_vision_encoder.num_patches}")
    logger.info(f"   Image size: {current_vision_encoder.config.image_size}")
    
    # ğŸ”§ éªŒè¯æœªæ¥è§‚æµ‹ç¼–ç å™¨è·¯å¾„ï¼ˆFLAREå†…éƒ¨ä¼šåˆ›å»ºSigLIP2ç¼–ç å™¨ï¼‰
    logger.info("ğŸ”® éªŒè¯æœªæ¥è§‚æµ‹ç¼–ç å™¨è·¯å¾„...")
    if os.path.exists(future_vision_path):
        logger.info(f"âœ… æœªæ¥è§‚æµ‹ç¼–ç å™¨è·¯å¾„æœ‰æ•ˆ: {future_vision_path}")
    else:
        logger.warning(f"âš ï¸  æœªæ¥è§‚æµ‹ç¼–ç å™¨è·¯å¾„ä¸å­˜åœ¨: {future_vision_path}")
        logger.warning("   FLAREç»„ä»¶å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")

    # ğŸ¯ æ„å»ºFLAREå¢å¼ºçš„RDTæ¨¡å‹
    pretrained_path = args.pretrained_model_name_or_path
    
    # è®¡ç®—å½“å‰å›¾åƒçš„æ¡ä»¶é•¿åº¦ï¼ˆåŸºäºå½“å‰ç¼–ç å™¨ï¼‰
    img_cond_len = (config["common"]["img_history_size"] * config["common"]["num_cameras"] *
                    current_vision_encoder.num_patches)
    
    logger.info(f"ğŸ”§ æ„å»ºFLAREå¢å¼ºçš„RDTæ¨¡å‹...")
    logger.info(f"   å›¾åƒæ¡ä»¶é•¿åº¦: {img_cond_len} (åŸºäºå½“å‰ç¼–ç å™¨patchæ•°: {current_vision_encoder.num_patches})")
    
    if (pretrained_path is not None and 
        (os.path.isfile(pretrained_path) or os.path.isdir(pretrained_path))):
        
        logger.info(f"ä»é¢„è®­ç»ƒè·¯å¾„æ„å»ºFLAREæ¨¡å‹: {pretrained_path}")
        
        # ğŸ”§ åˆ›å»ºå¸¦åŒç¼–ç å™¨é…ç½®çš„FLAREæ¨¡å‹
        rdt = RDTRunnerWithFLARE(
            action_dim=config["common"]["state_dim"],
            pred_horizon=config["common"]["action_chunk_size"],
            config=config["model"],
            lang_token_dim=config["model"]["lang_token_dim"],
            img_token_dim=config["model"]["img_token_dim"],
            state_token_dim=config["model"]["state_token_dim"],
            max_lang_cond_len=max_text_length,  # ğŸ”§ ä½¿ç”¨æ–°çš„æ–‡æœ¬é•¿åº¦
            img_cond_len=img_cond_len,
            img_pos_embed_config=[
                ("image", (
                    config["common"]["img_history_size"],
                    config["common"]["num_cameras"],
                    -current_vision_encoder.num_patches,  # ğŸ”§ åŸºäºå½“å‰ç¼–ç å™¨
                )),
            ],
            lang_pos_embed_config=[
                ("lang", -max_text_length),  # ğŸ”§ ä½¿ç”¨æ–°çš„æ–‡æœ¬é•¿åº¦
            ],
            dtype=weight_dtype,
            # FLAREç‰¹å®šå‚æ•°
            num_future_tokens=num_future_tokens,
            activation_layer=activation_layer,
            alignment_loss_weight=alignment_loss_weight,
            enable_flare=enable_flare,
            # ğŸ”§ å…³é”®ï¼šåŒç¼–ç å™¨è·¯å¾„é…ç½®
            future_vision_model_name=future_vision_path,
            future_text_model_name=future_text_path,
            current_vision_image_size=current_vision_image_size,
            future_vision_image_size=future_vision_image_size,
        )
        
        # ğŸ¯ å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæ˜¯æ–‡ä»¶ï¼‰
        # if os.path.isfile(pretrained_path):
        #     try:
        #         logger.info(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
        #         checkpoint = torch.load(pretrained_path, map_location='cpu')
        #         rdt.load_state_dict(checkpoint["module"], strict=False)
        #         logger.info("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸï¼ˆéƒ¨åˆ†å‚æ•°ï¼ŒFLAREç»„ä»¶éšæœºåˆå§‹åŒ–ï¼‰")
        #     except Exception as e:
        #         logger.warning(f"âš ï¸  é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–: {e}")
        # else:
        #     logger.info("ä½¿ç”¨ç›®å½•è·¯å¾„ï¼Œè·³è¿‡æƒé‡åŠ è½½")
        if os.path.isfile(pretrained_path):
            try:
                logger.info(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
                ckpt = torch.load(pretrained_path, map_location="cpu")
                # æ”¯æŒå¤šç§ checkpoint æ ¼å¼
                if isinstance(ckpt, dict) and "module" in ckpt:
                    state_dict = ckpt["module"]
                elif isinstance(ckpt, dict) and "state_dict" in ckpt:
                    state_dict = ckpt["state_dict"]
                else:
                    state_dict = ckpt

                # è¿‡æ»¤æ‰å½¢çŠ¶ä¸åŒ¹é…çš„å‚æ•°
                model_sd = rdt.state_dict()
                filtered_sd = {}
                for k, v in state_dict.items():
                    if k in model_sd and v.shape == model_sd[k].shape:
                        filtered_sd[k] = v
                    else:
                        logger.warning(
                            f"è·³è¿‡ {k}: checkpoint {tuple(v.shape)} vs model {tuple(model_sd.get(k, v).shape)}"
                        )

                # å¢é‡åŠ è½½åŒ¹é…çš„å‚æ•°ï¼Œå…¶ä½™ä¿æŒéšæœºåˆå§‹åŒ–
                rdt.load_state_dict(filtered_sd, strict=False)
                logger.info("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸï¼ˆå·²åŠ è½½æ‰€æœ‰ shape åŒ¹é…çš„å‚æ•°ï¼Œå…¶ä½™éšæœºåˆå§‹åŒ–ï¼‰")
            except Exception as e:
                logger.warning(f"âš ï¸  é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–: {e}")
        else:
            logger.info("ä½¿ç”¨ç›®å½•è·¯å¾„ï¼Œè·³è¿‡æƒé‡åŠ è½½")
            
    else:
        logger.info("ä»é…ç½®æ–‡ä»¶æ„å»ºFLAREæ¨¡å‹ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰")
        rdt = create_flare_model_from_standard_rdt(
            args, config, current_vision_encoder, weight_dtype,
            future_vision_path, future_text_path, max_text_length
        )

    # ğŸ¯ ç¡®ä¿æ¨¡å‹æ•°æ®ç±»å‹æ­£ç¡®
    rdt = rdt.to(dtype=weight_dtype)
    
    # æ•°æ®ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥
    logger.info("ğŸ” æ£€æŸ¥æ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´æ€§...")
    dtype_issues = []
    for name, param in rdt.named_parameters():
        if param.dtype != weight_dtype:
            dtype_issues.append(f"{name}: {param.dtype}")
    
    if dtype_issues:
        logger.warning(f"âš ï¸  å‘ç°æ•°æ®ç±»å‹ä¸ä¸€è‡´: {len(dtype_issues)} ä¸ªå‚æ•°")
        rdt = rdt.to(weight_dtype)
        logger.info("âœ… å·²å¼ºåˆ¶è½¬æ¢æ‰€æœ‰å‚æ•°åˆ°ç»Ÿä¸€æ•°æ®ç±»å‹")
    else:
        logger.info("âœ… æ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")

    # EMAæ¨¡å‹
    ema_rdt = copy.deepcopy(rdt)
    ema_model = EMAModel(
        ema_rdt,
        update_after_step=config["model"]["ema"]["update_after_step"],
        inv_gamma=config["model"]["ema"]["inv_gamma"],
        power=config["model"]["ema"]["power"],
        min_value=config["model"]["ema"]["min_value"],
        max_value=config["model"]["ema"]["max_value"],
    )

    # create custom saving & loading hooks
    def save_model_hook(models, weights, output_dir):
        if accelerator.is_main_process:
            for model in models:
                model_to_save = model.module if hasattr(model, "module") else model
                if isinstance(model_to_save, type(accelerator.unwrap_model(rdt))):
                    model_to_save.save_pretrained(output_dir)

    accelerator.register_save_state_pre_hook(save_model_hook)

    if args.gradient_checkpointing:
        logger.warning("æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½æš‚æœªåœ¨FLAREä¸­å®ç°")

    # Enable TF32 for faster training on Ampere GPUs
    if args.allow_tf32:
        torch.backends.cuda.matmul.allow_tf32 = True

    if args.scale_lr:
        original_lr = args.learning_rate
        args.learning_rate = (args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size *
                              accelerator.num_processes)
        logger.info(f"ğŸ¯ å­¦ä¹ ç‡ç¼©æ”¾: {original_lr} -> {args.learning_rate}")

    # ğŸ¯ A800ä¼˜åŒ–çš„batch sizeå»ºè®®
    if is_a800 and args.train_batch_size < 24:
        logger.info(f"ğŸ’¡ A800å»ºè®®ä½¿ç”¨æ›´å¤§çš„batch_size (å½“å‰: {args.train_batch_size}, å»ºè®®: 24-32)")

    # Optimizer creation
    if args.use_8bit_adam:
        try:
            import bitsandbytes as bnb
        except ImportError:
            raise ImportError("To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.")
        optimizer_class = bnb.optim.AdamW8bit
        logger.info("ğŸ¯ ä½¿ç”¨8bit Adamä¼˜åŒ–å™¨")
    else:
        optimizer_class = torch.optim.AdamW
        logger.info("ğŸ¯ ä½¿ç”¨æ ‡å‡†AdamWä¼˜åŒ–å™¨")

    params_to_optimize = rdt.parameters()
    optimizer = optimizer_class(
        params_to_optimize,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )

    # ğŸ”§ åˆ›å»ºFLAREå¢å¼ºçš„æ•°æ®é›†ï¼ˆä½¿ç”¨å½“å‰ç¼–ç å™¨çš„image_processorï¼‰
    train_dataset = VLAConsumerDatasetWithFLARE(
        model_config_path=args.model_config_path,
        config=config["dataset"],
        tokenizer=tokenizer,
        image_processor=image_processor,  # ğŸ”§ ä½¿ç”¨å½“å‰ç¼–ç å™¨çš„processor
        num_cameras=config["common"]["num_cameras"],
        img_history_size=config["common"]["img_history_size"],
        dataset_type=args.dataset_type,
        image_aug=args.image_aug,
        cond_mask_prob=args.cond_mask_prob,
        cam_ext_mask_prob=args.cam_ext_mask_prob,
        state_noise_snr=args.state_noise_snr,
        use_hdf5=args.load_from_hdf5,
        use_precomp_lang_embed=args.precomp_lang_embed,
        # FLAREå‚æ•°
        enable_future_obs=enable_flare,
        future_obs_prob=model_config.get('future_obs_prob', 0.8),
        action_chunk_size=config["common"]["action_chunk_size"],
    )
    
    sample_dataset = VLAConsumerDatasetWithFLARE(
        model_config_path=args.model_config_path,
        config=config["dataset"],
        tokenizer=tokenizer,
        image_processor=image_processor,  # ğŸ”§ ä½¿ç”¨å½“å‰ç¼–ç å™¨çš„processor
        num_cameras=config["common"]["num_cameras"],
        img_history_size=config["common"]["img_history_size"],
        dataset_type=args.dataset_type,
        image_aug=False,
        cond_mask_prob=0,
        cam_ext_mask_prob=-1,
        state_noise_snr=None,
        use_hdf5=args.load_from_hdf5,
        use_precomp_lang_embed=args.precomp_lang_embed,
        # FLAREå‚æ•°
        enable_future_obs=False,  # é‡‡æ ·æ—¶ä¸ä½¿ç”¨æœªæ¥è§‚æµ‹
        future_obs_prob=0.0,
        action_chunk_size=config["common"]["action_chunk_size"],
    )

    data_collator = DataCollatorForVLAConsumerDatasetWithFLARE(tokenizer)

    # ğŸ¯ A800ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨é…ç½®
    num_workers = args.dataloader_num_workers
    if is_a800 and num_workers < 12:
        num_workers = min(16, num_workers * 2)
        logger.info(f"ğŸ¯ A800ä¼˜åŒ–: æ•°æ®åŠ è½½å™¨workerså¢åŠ åˆ° {num_workers}")

    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.train_batch_size,
        shuffle=True,
        collate_fn=data_collator,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
    )
    sample_dataloader = torch.utils.data.DataLoader(
        sample_dataset,
        batch_size=args.sample_batch_size,
        shuffle=True,
        collate_fn=data_collator,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
    )

    # Scheduler and math around the number of training steps
    overrode_max_train_steps = False
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
        num_cycles=args.lr_num_cycles,
        power=args.lr_power,
    )

    # Prepare everything with accelerator
    rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler = (accelerator.prepare(
        rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler))

    # ğŸ¯ å°†æ¨¡å‹ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹
    rdt.to(accelerator.device, dtype=weight_dtype)
    ema_rdt.to(accelerator.device, dtype=weight_dtype)

    if text_encoder is not None:
        text_encoder.to(accelerator.device, dtype=weight_dtype)

    # ğŸ”§ åªç§»åŠ¨å½“å‰å›¾åƒç¼–ç å™¨åˆ°è®¾å¤‡ï¼ˆæœªæ¥ç¼–ç å™¨åœ¨FLAREå†…éƒ¨ç®¡ç†ï¼‰
    if current_vision_encoder is not None:
        current_vision_encoder.vision_tower.to(accelerator.device, dtype=weight_dtype)
        logger.info("âœ… å½“å‰å›¾åƒç¼–ç å™¨å·²ç§»åŠ¨åˆ°è®¾å¤‡")

    # Recalculate training steps
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # Initialize trackers
    if accelerator.is_main_process:
        tracker_config = vars(args)
        tracker_config.update({
            "enable_flare": enable_flare,
            "num_future_tokens": num_future_tokens,
            "activation_layer": activation_layer,
            "alignment_loss_weight": alignment_loss_weight,
            "weight_dtype": str(weight_dtype),
            "is_a800": is_a800,
            "gpu_memory_gb": gpu_memory,
            # ğŸ”§ åŒç¼–ç å™¨é…ç½®
            "dual_encoder": True,
            "current_vision_path": current_vision_path,
            "future_vision_path": future_vision_path,
            "current_image_size": current_vision_image_size,
            "future_image_size": future_vision_image_size,
            "max_text_length": max_text_length,
        })
        
        accelerator.init_trackers(
            "VLA_FLARE_DualEncoder",
            config=tracker_config,
            init_kwargs={"wandb": {
                "name": f"RDT_FLARE_Dual_{args.CONFIG_NAME}_{accelerator.mixed_precision}",
                "tags": ["rdt", "flare", "dual-encoder", "multimodal", "robotics", "a800", accelerator.mixed_precision],
            }},
        )

    # Training info
    total_batch_size = (args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps)

    logger.info("***** Running FLARE Dual Encoder training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Num batches each epoch = {len(train_dataloader)}")
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {args.max_train_steps}")
    logger.info(f"  Mixed precision = {accelerator.mixed_precision}")
    logger.info(f"  Weight dtype = {weight_dtype}")
    logger.info(f"  FLARE enabled = {enable_flare}")
    logger.info(f"  Future tokens = {num_future_tokens}")
    logger.info(f"  Activation layer = {activation_layer}")
    logger.info(f"  Alignment loss weight = {alignment_loss_weight}")
    logger.info(f"  Current encoder: SigLIP-{current_vision_image_size}")
    logger.info(f"  Future encoder: SigLIP2-{future_vision_image_size}")
    
    global_step = 0
    first_epoch = 0

    # Resume from checkpoint
    if args.resume_from_checkpoint:
        if args.resume_from_checkpoint != "latest":
            path = os.path.basename(args.resume_from_checkpoint)
        else:
            # Get the most recent checkpoint
            dirs = os.listdir(args.output_dir)
            dirs = [d for d in dirs if d.startswith("checkpoint")]
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
            path = dirs[-1] if len(dirs) > 0 else None

        if path is None:
            accelerator.print(
                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
            args.resume_from_checkpoint = None
        else:
            accelerator.print(f"Resuming from checkpoint {path}")
            try:
                accelerator.load_state(os.path.join(args.output_dir, path))
            except:
                logger.info("Resuming training state failed. Attempting to only load from model checkpoint.")
                checkpoint = torch.load(
                    os.path.join(
                        args.output_dir,
                        path,
                        "pytorch_model",
                        "mp_rank_00_model_states.pt",
                    ))
                unwrapped_rdt = accelerator.unwrap_model(rdt)
                unwrapped_rdt.load_state_dict(checkpoint["module"], strict=False)

            load_model(ema_rdt, os.path.join(args.output_dir, path, "ema", "model.safetensors"))
            global_step = int(path.split("-")[1])

            resume_global_step = global_step * args.gradient_accumulation_steps
            first_epoch = global_step // num_update_steps_per_epoch
            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)

    # Progress bar
    progress_bar = tqdm(
        range(global_step, args.max_train_steps),
        disable=not accelerator.is_local_main_process,
    )
    progress_bar.set_description(f"FLARE Dual Encoder Training ({accelerator.mixed_precision})")

    # Training metrics
    loss_for_log = {}
    alignment_metrics_log = {}
    
    # for epoch in range(first_epoch, args.num_train_epochs):
    #     rdt.train()

    #     # Set progress bar to correct position
    #     if args.resume_from_checkpoint and epoch == first_epoch:
    #         progress_bar.update(resume_step // args.gradient_accumulation_steps)

    #     # ğŸ¯ FLAREåŒç¼–ç å™¨è®­ç»ƒå¾ªç¯ - æ”¯æŒBF16æ··åˆç²¾åº¦
    #     for batch in train_dataloader:
    #         with accelerator.accumulate(rdt):
    #             # ğŸ¯ ä½¿ç”¨autocaståŒ…è£…å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
    #             with torch.autocast(device_type="cuda", dtype=weight_dtype, enabled=(accelerator.mixed_precision != "no")):
    #                 # åŸºç¡€æ•°æ®å‡†å¤‡
    #                 images = batch["images"]
    #                 states = batch["states"]
    #                 states = states[:, -1:, :]  # åªä½¿ç”¨æœ€åä¸€ä¸ªçŠ¶æ€
    #                 actions = batch["actions"]
    #                 state_elem_mask = batch["state_elem_mask"]
    #                 ctrl_freqs = batch["ctrl_freqs"]

    #                 # FLAREç‰¹å®šæ•°æ®
    #                 future_obs_images = batch.get("future_obs_images")
    #                 text_instructions = batch.get("text_instructions", [""] * len(images))
    #                 has_future_obs = batch.get("has_future_obs")

    #                 with torch.no_grad():
    #                     # ğŸ”§ ä½¿ç”¨å½“å‰å›¾åƒç¼–ç å™¨å¤„ç†å½“å‰è§‚æµ‹ï¼ˆç”¨äºDiTï¼‰
    #                     images_tensor = torch.stack(images, dim=0)  # [B, num_imgs, C, H, W]
    #                     batch_size, _, C, H, W = images_tensor.shape
    #                     images = images_tensor  # æ›´æ–°imageså˜é‡
                        
    #                     # ğŸ”§ å½“å‰å›¾åƒç¼–ç ï¼šSigLIP-384 â†’ DiT layers
    #                     image_embeds = current_vision_encoder(images.reshape(-1, C, H, W)).detach()
    #                     image_embeds = image_embeds.reshape((batch_size, -1, current_vision_encoder.hidden_size))

    #                     # ç¼–ç è¯­è¨€ï¼ˆç”¨äºDiTï¼‰
    #                     lang_attn_mask = batch["lang_attn_mask"]
    #                     if args.precomp_lang_embed:
    #                         text_embeds = batch["lang_embeds"]
    #                     else:
    #                         text_embeds = text_encoder(
    #                             input_ids=batch["input_ids"], 
    #                             attention_mask=lang_attn_mask
    #                         )["last_hidden_state"].detach()

    #                     # ğŸ”§ æ³¨æ„ï¼šæœªæ¥è§‚æµ‹ç¼–ç ç”±FLAREå†…éƒ¨çš„SigLIP2-256å¤„ç†
    #                     # è¿™é‡Œä¸éœ€è¦é¢„å…ˆç¼–ç æœªæ¥è§‚æµ‹ï¼Œç›´æ¥ä¼ é€’åŸå§‹å›¾åƒç»™FLARE
    #                     future_vision_embeds = None  # FLAREå†…éƒ¨å¤„ç†

    #                 state_elem_mask = state_elem_mask.unsqueeze(1)
                    
    #                 # ğŸ”§ è®¡ç®—FLAREå¢å¼ºçš„æŸå¤±ï¼ˆåŒç¼–ç å™¨ç‰ˆæœ¬ï¼‰
    #                 unwrapped_rdt = accelerator.unwrap_model(rdt)
    #                 total_loss, loss_dict = unwrapped_rdt.compute_loss_with_flare(
    #                     lang_tokens=text_embeds,
    #                     lang_attn_mask=lang_attn_mask,
    #                     img_tokens=image_embeds,  # å½“å‰å›¾åƒçš„ç¼–ç ï¼ˆSigLIP-384ï¼‰
    #                     state_tokens=states,
    #                     action_gt=actions,
    #                     action_mask=state_elem_mask,
    #                     ctrl_freqs=ctrl_freqs,
    #                     future_vision_tokens=future_vision_embeds,  # Noneï¼ŒFLAREå†…éƒ¨å¤„ç†
    #                     text_instructions=text_instructions,
    #                     has_future_obs=has_future_obs,
    #                     future_obs_images=future_obs_images  # ğŸ”§ ä¼ é€’åŸå§‹å›¾åƒç»™FLAREï¼ˆSigLIP2-256å¤„ç†ï¼‰
    #                 )
    for epoch in range(first_epoch, args.num_train_epochs):
        rdt.train()

        # Set progress bar to correct position
        if args.resume_from_checkpoint and epoch == first_epoch:
            progress_bar.update(resume_step // args.gradient_accumulation_steps)

        # ğŸ¯ FLAREç»Ÿä¸€T5æ¶æ„è®­ç»ƒå¾ªç¯ - æ”¯æŒBF16æ··åˆç²¾åº¦
        for batch in train_dataloader:
            with accelerator.accumulate(rdt):
                # ğŸ¯ ä½¿ç”¨autocaståŒ…è£…å‰å‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
                with torch.autocast(device_type="cuda", dtype=weight_dtype, enabled=(accelerator.mixed_precision != "no")):
                    # åŸºç¡€æ•°æ®å‡†å¤‡
                    images = batch["images"]
                    states = batch["states"]
                    states = states[:, -1:, :]  # åªä½¿ç”¨æœ€åä¸€ä¸ªçŠ¶æ€
                    actions = batch["actions"]
                    state_elem_mask = batch["state_elem_mask"]
                    ctrl_freqs = batch["ctrl_freqs"]

                    # ğŸ¯ FLAREç‰¹å®šæ•°æ® - ç»Ÿä¸€T5æ¶æ„ä¿®å¤
                    future_obs_images = batch.get("future_obs_images")
                    has_future_obs = batch.get("has_future_obs")
                    
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ¹æ®æ˜¯å¦ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥é€‰æ‹©æ–‡æœ¬æ•°æ®æº
                    if args.precomp_lang_embed:
                        # ä½¿ç”¨T5åµŒå…¥è·¯å¾„ç»™FLAREï¼ˆç»Ÿä¸€T5æ¶æ„ï¼‰
                        text_instructions = batch.get("flare_text_embed_paths", [])
                        
                        # è°ƒè¯•ä¿¡æ¯
                        if global_step % 100 == 0:  # æ¯100æ­¥æ‰“å°ä¸€æ¬¡
                            print(f"ğŸ¯ ç»Ÿä¸€T5æ¶æ„ - Step {global_step}:")
                            print(f"   FLAREä½¿ç”¨T5åµŒå…¥è·¯å¾„: {len(text_instructions)} ä¸ªæ–‡ä»¶")
                            if text_instructions:
                                print(f"   ç¤ºä¾‹è·¯å¾„: {text_instructions[0]}")
                            else:
                                print("   âš ï¸ æœªè·å–åˆ°T5åµŒå…¥è·¯å¾„")
                    else:
                        # ä½¿ç”¨åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
                        text_instructions = batch.get("text_instructions", [""] * len(images))
                        if global_step % 100 == 0:
                            print(f"ğŸ¯ ä½¿ç”¨åŸå§‹æ–‡æœ¬: {text_instructions[0] if text_instructions else 'None'}")

                    with torch.no_grad():
                        # ğŸ”§ ä½¿ç”¨å½“å‰å›¾åƒç¼–ç å™¨å¤„ç†å½“å‰è§‚æµ‹ï¼ˆç”¨äºDiTï¼‰
                        images_tensor = torch.stack(images, dim=0)  # [B, num_imgs, C, H, W]
                        batch_size, _, C, H, W = images_tensor.shape
                        images = images_tensor  # æ›´æ–°imageså˜é‡
                        
                        # ğŸ”§ å½“å‰å›¾åƒç¼–ç ï¼šSigLIP-384 â†’ DiT layers
                        image_embeds = current_vision_encoder(images.reshape(-1, C, H, W)).detach()
                        image_embeds = image_embeds.reshape((batch_size, -1, current_vision_encoder.hidden_size))

                        # ğŸ¯ ç¼–ç è¯­è¨€ï¼ˆç”¨äºDiTï¼‰- T5åµŒå…¥
                        lang_attn_mask = batch["lang_attn_mask"]
                        if args.precomp_lang_embed:
                            text_embeds = batch["lang_embeds"]  # T5é¢„è®¡ç®—åµŒå…¥
                        else:
                            text_embeds = text_encoder(
                                input_ids=batch["input_ids"], 
                                attention_mask=lang_attn_mask
                            )["last_hidden_state"].detach()

                        # ğŸ”§ æ³¨æ„ï¼šæœªæ¥è§‚æµ‹ç¼–ç ç”±FLAREå†…éƒ¨çš„ç»Ÿä¸€T5æ¶æ„å¤„ç†
                        # VLTokenGeneratorä¼šä½¿ç”¨ç›¸åŒçš„T5åµŒå…¥è·¯å¾„ï¼Œç¡®ä¿æ¶æ„ç»Ÿä¸€
                        future_vision_embeds = None  # FLAREå†…éƒ¨å¤„ç†

                    state_elem_mask = state_elem_mask.unsqueeze(1)
                    
                    # ğŸ¯ è®¡ç®—FLAREå¢å¼ºçš„æŸå¤±ï¼ˆç»Ÿä¸€T5æ¶æ„ç‰ˆæœ¬ï¼‰
                    unwrapped_rdt = accelerator.unwrap_model(rdt)
                    
                    
                    total_loss, loss_dict = unwrapped_rdt.compute_loss_with_flare(
                        lang_tokens=text_embeds,                # T5åµŒå…¥ â†’ DiTå¤„ç†å½“å‰çŠ¶æ€
                        lang_attn_mask=lang_attn_mask,
                        img_tokens=image_embeds,                # å½“å‰å›¾åƒï¼ˆSigLIP-384ï¼‰â†’ DiT
                        state_tokens=states,
                        action_gt=actions,
                        action_mask=state_elem_mask,
                        ctrl_freqs=ctrl_freqs,
                        future_vision_tokens=future_vision_embeds,  # Noneï¼ŒFLAREå†…éƒ¨å¤„ç†
                        text_instructions=text_instructions,    # ğŸ¯ T5è·¯å¾„ â†’ FLAREç»Ÿä¸€å¤„ç†
                        has_future_obs=has_future_obs,
                        future_obs_images=future_obs_images,    # æœªæ¥å›¾åƒ â†’ FLAREï¼ˆå†…éƒ¨ç»Ÿä¸€T5å¤„ç†ï¼‰
                    )
                

                # åå‘ä¼ æ’­ï¼ˆacceleratorä¼šè‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦ï¼‰
                accelerator.backward(total_loss)
                
                if accelerator.sync_gradients:
                    params_to_clip = rdt.parameters()
                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad(set_to_none=args.set_grads_to_none)

            ema_model.step(accelerator.unwrap_model(rdt))

            # Check if optimization step occurred
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

                # Checkpointing
                if global_step % args.checkpointing_period == 0:
                    save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                    accelerator.save_state(save_path)
                    ema_save_path = os.path.join(save_path, f"ema")
                    accelerator.save_model(ema_rdt, ema_save_path)
                    logger.info(f"Saved state to {save_path}")

                # Sampling and evaluation
                if args.sample_period > 0 and global_step % args.sample_period == 0:
                    sample_loss_for_log = log_sample_res(
                        text_encoder,
                        current_vision_encoder,  # ğŸ”§ ä½¿ç”¨å½“å‰ç¼–ç å™¨è¿›è¡Œé‡‡æ ·
                        rdt,
                        args,
                        accelerator,
                        weight_dtype,
                        sample_dataset.get_dataset_id2name(),
                        sample_dataloader,
                        logger,
                    )
                    logger.info(sample_loss_for_log)
                    accelerator.log(sample_loss_for_log, step=global_step)

                # è·å–å¯¹é½æŒ‡æ ‡
                if enable_flare and global_step % 100 == 0:
                    try:
                        unwrapped_rdt = accelerator.unwrap_model(rdt)
                        alignment_metrics = unwrapped_rdt.get_alignment_metrics()
                        if alignment_metrics:
                            alignment_metrics_log.update({f"alignment_{k}": v for k, v in alignment_metrics.items()})
                    except Exception as e:
                        logger.debug(f"Failed to get alignment metrics: {e}")

                # ğŸ¯ A800æ€§èƒ½ç›‘æ§
                if is_a800 and global_step % 500 == 0:
                    try:
                        gpu_utilization = torch.cuda.utilization()
                        memory_allocated = torch.cuda.memory_allocated() / 1024**3
                        memory_reserved = torch.cuda.memory_reserved() / 1024**3
                        
                        performance_metrics = {
                            "gpu_utilization": gpu_utilization,
                            "memory_allocated_gb": memory_allocated,
                            "memory_reserved_gb": memory_reserved,
                            "memory_usage_ratio": memory_allocated / gpu_memory if gpu_memory > 0 else 0,
                        }
                        logger.info(f"ğŸ¯ A800æ€§èƒ½: GPUåˆ©ç”¨ç‡={gpu_utilization}%, æ˜¾å­˜={memory_allocated:.1f}GB/{gpu_memory:.1f}GB")
                        accelerator.log(performance_metrics, step=global_step)
                    except Exception as e:
                        logger.debug(f"Failed to get performance metrics: {e}")

                # ğŸ”§ åŒç¼–ç å™¨ç‰¹å®šçš„ç›‘æ§
                if global_step % 200 == 0:
                    logger.info(f"ğŸ”§ åŒç¼–ç å™¨çŠ¶æ€ç›‘æ§ Step {global_step}:")
                    logger.info(f"   å½“å‰ç¼–ç å™¨: å¤„ç†äº† {batch_size} ä¸ªå½“å‰è§‚æµ‹")
                    logger.info(f"   æœªæ¥ç¼–ç å™¨: å¤„ç†äº† {has_future_obs.sum().item() if has_future_obs is not None else 0} ä¸ªæœªæ¥è§‚æµ‹")
                    logger.info(f"   FLAREä½¿ç”¨ç‡: {loss_dict.get('used_flare', False)}")

            # è®°å½•æŸå¤±
            logs = {
                "loss": total_loss.detach().item(), 
                "lr": lr_scheduler.get_last_lr()[0],
                "diffusion_loss": loss_dict.get('diffusion_loss', 0.0),
                "alignment_loss": loss_dict.get('alignment_loss', 0.0),
                "used_flare": float(loss_dict.get('used_flare', False)),
                "epoch": epoch,
                # ğŸ”§ åŒç¼–ç å™¨ç‰¹å®šæŒ‡æ ‡
                "current_encoder_active": True,
                "future_encoder_active": bool(enable_flare and future_obs_images is not None),
                "future_obs_ratio": batch.get("future_obs_ratio", 0.0),
            }
            
            # ğŸ¯ BF16ç‰¹å®šæŒ‡æ ‡
            if accelerator.mixed_precision == "bf16":
                logs["bf16_training"] = True
                # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°ï¼ˆBF16è®­ç»ƒä¸­å¾ˆé‡è¦ï¼‰
                try:
                    total_norm = 0.0
                    for p in rdt.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm = total_norm ** (1. / 2)
                    logs["grad_norm"] = total_norm
                except:
                    pass
            
            progress_bar.set_postfix(**{k: f"{v:.4f}" if isinstance(v, float) else str(v) for k, v in logs.items()})
            logs.update(loss_for_log)
            logs.update(alignment_metrics_log)
            accelerator.log(logs, step=global_step)

            if global_step >= args.max_train_steps:
                break

    # Save final model
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        accelerator.unwrap_model(rdt).save_pretrained(args.output_dir)
        ema_save_path = os.path.join(args.output_dir, f"ema")
        accelerator.save_model(ema_rdt, ema_save_path)

        logger.info(f"âœ… FLAREåŒç¼–ç å™¨æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")
        
        # ğŸ¯ ä¿å­˜è®­ç»ƒé…ç½®å’Œæ€§èƒ½ä¿¡æ¯
        training_info = {
            "mixed_precision": accelerator.mixed_precision,
            "weight_dtype": str(weight_dtype),
            "is_a800": is_a800,
            "gpu_memory_gb": gpu_memory,
            "final_loss": logs.get("loss", 0.0),
            "total_steps": global_step,
            "enable_flare": enable_flare,
            "num_future_tokens": num_future_tokens,
            "alignment_loss_weight": alignment_loss_weight,
            # ğŸ”§ åŒç¼–ç å™¨ä¿¡æ¯
            "dual_encoder": True,
            "current_vision_encoder": current_vision_path,
            "future_vision_encoder": future_vision_path,
            "current_image_size": current_vision_image_size,
            "future_image_size": future_vision_image_size,
            "max_text_length": max_text_length,
        }
        
        import json
        with open(os.path.join(args.output_dir, "training_info.json"), "w") as f:
            json.dump(training_info, f, indent=2)

        if args.push_to_hub:
            save_model_card(
                repo_id,
                base_model=args.pretrained_model_name_or_path,
                repo_folder=args.output_dir,
            )
            upload_folder(
                repo_id=repo_id,
                folder_path=args.output_dir,
                commit_message="End of FLARE dual encoder training with BF16 mixed precision",
                token=args.hub_token,
                allow_patterns=["pytorch_model.bin", "*.json", "*.md"],
            )

    accelerator.end_training()


def create_flare_model_from_standard_rdt(args, config, current_vision_encoder, weight_dtype,
                                        future_vision_path, future_text_path, max_text_length):
    """
    åˆ›å»ºFLAREæ¨¡å‹ï¼ˆæ”¯æŒåŒç¼–ç å™¨é…ç½®ï¼‰
    """
    from models.rdt_runner import RDTRunnerWithFLARE
    import logging
    
    logger = logging.getLogger(__name__)
    
    # ğŸ”§ è®¡ç®—å›¾åƒæ¡ä»¶é•¿åº¦ï¼ˆåŸºäºå½“å‰ç¼–ç å™¨ï¼‰
    img_cond_len = (config["common"]["img_history_size"] * config["common"]["num_cameras"] *
                    current_vision_encoder.num_patches)
    
    logger.info("åˆ›å»ºFLAREå¢å¼ºçš„åŒç¼–ç å™¨RDTæ¨¡å‹...")
    
    # ğŸ”§ åˆ›å»ºå¸¦åŒç¼–ç å™¨é…ç½®çš„FLAREæ¨¡å‹
    flare_rdt = RDTRunnerWithFLARE(
        action_dim=config["common"]["state_dim"],
        pred_horizon=config["common"]["action_chunk_size"],
        config=config["model"],
        lang_token_dim=config["model"]["lang_token_dim"],
        img_token_dim=config["model"]["img_token_dim"],
        state_token_dim=config["model"]["state_token_dim"],
        max_lang_cond_len=max_text_length,  # ğŸ”§ ä½¿ç”¨æ–°çš„æ–‡æœ¬é•¿åº¦
        img_cond_len=img_cond_len,
        img_pos_embed_config=[
            ("image", (
                config["common"]["img_history_size"],
                config["common"]["num_cameras"],
                -current_vision_encoder.num_patches,  # ğŸ”§ åŸºäºå½“å‰ç¼–ç å™¨
            )),
        ],
        lang_pos_embed_config=[
            ("lang", -max_text_length),  # ğŸ”§ ä½¿ç”¨æ–°çš„æ–‡æœ¬é•¿åº¦
        ],
        dtype=weight_dtype,
        # FLAREå‚æ•°
        num_future_tokens=getattr(args, 'num_future_tokens', 32),
        activation_layer=getattr(args, 'activation_layer', 21),
        alignment_loss_weight=getattr(args, 'alignment_loss_weight', 0.2),
        enable_flare=getattr(args, 'enable_flare', True),
        # ğŸ”§ åŒç¼–ç å™¨è·¯å¾„
        future_vision_model_name=future_vision_path,
        future_text_model_name=future_text_path,
        current_vision_image_size=getattr(args, 'current_vision_image_size', 384),
        future_vision_image_size=getattr(args, 'future_vision_image_size', 256),
    )
    
    # ç¡®ä¿æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹æ­£ç¡®
    flare_rdt = flare_rdt.to(dtype=weight_dtype)
    
    logger.info(f"âœ… FLAREåŒç¼–ç å™¨æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    logger.info(f"   å‚æ•°æ€»æ•°: {sum(p.numel() for p in flare_rdt.parameters()):,}")
    logger.info(f"   æ¨¡å‹æ•°æ®ç±»å‹: {weight_dtype}")
    logger.info(f"   å½“å‰ç¼–ç å™¨: SigLIP-384 (DiTå¤„ç†)")
    logger.info(f"   æœªæ¥ç¼–ç å™¨: SigLIP2-256 (FLAREå¤„ç†)")
    logger.info(f"   FLAREç»„ä»¶: VL Tokenç”Ÿæˆå™¨ã€Q-Formerã€æ¿€æ´»å¯¹é½å™¨")
    
    return flare_rdt


def monitor_training_health(loss_dict, global_step, logger):
    """ç›‘æ§è®­ç»ƒå¥åº·çŠ¶å†µ"""
    diffusion_loss = loss_dict.get('diffusion_loss', 0.0)
    alignment_loss = loss_dict.get('alignment_loss', 0.0)
    
    # æ£€æŸ¥NaNæˆ–å¼‚å¸¸å€¼
    if math.isnan(diffusion_loss) or math.isnan(alignment_loss):
        logger.error(f"âš ï¸  Step {global_step}: æ£€æµ‹åˆ°NaNæŸå¤±å€¼!")
        return False
        
    # æ£€æŸ¥æŸå¤±çˆ†ç‚¸
    if diffusion_loss > 100 or alignment_loss > 100:
        logger.warning(f"âš ï¸  Step {global_step}: æŸå¤±å€¼å¼‚å¸¸å¤§ (diffusion: {diffusion_loss:.3f}, alignment: {alignment_loss:.3f})")
        
    # è®°å½•å¥åº·çŠ¶æ€
    if global_step % 1000 == 0:
        logger.info(f"ğŸ’Š è®­ç»ƒå¥åº·æ£€æŸ¥ Step {global_step}:")
        logger.info(f"   æ‰©æ•£æŸå¤±: {diffusion_loss:.4f}")
        logger.info(f"   å¯¹é½æŸå¤±: {alignment_loss:.4f}")
        logger.info(f"   ä½¿ç”¨FLARE: {loss_dict.get('used_flare', False)}")
    
    return True