# train/dataset.py - ç®€åŒ–ç‰ˆæœ¬

import traceback
import time
import os
import json
import math
import random
from typing import Dict, Sequence

import numpy as np
import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import transformers

from data.filelock import FileLock
from data.hdf5_vla_dataset import HDF5VLADataset
from train.image_corrupt import image_corrupt


class VLAConsumerDatasetWithFLARE(Dataset):
    """
    æ”¯æŒFLAREåŠŸèƒ½çš„VLAæ¶ˆè´¹è€…æ•°æ®é›†
    æ ¸å¿ƒç†å¿µï¼šåŠ¨æ€ä½¿ç”¨è½¨è¿¹æœ€åä¸€å¸§ä½œä¸ºæœªæ¥è§‚æµ‹ï¼Œæ— éœ€é¢„å¤„ç†
    """

    def __init__(
        self,
        model_config_path,
        config,
        tokenizer,
        image_processor,
        num_cameras,
        img_history_size,
        image_size=None,
        auto_adjust_image_brightness=False,
        image_aug=False,
        dataset_type="pretrain",
        cond_mask_prob=0.1,
        cam_ext_mask_prob=-1.0,
        state_noise_snr=None,
        use_hdf5=False,
        use_precomp_lang_embed=False,
        # FLAREç‰¹å®šå‚æ•°
        enable_future_obs=True,
        future_obs_prob=1,  # ä½¿ç”¨æœªæ¥è§‚æµ‹çš„æ¦‚ç‡
        action_chunk_size=32,  # åŠ¨ä½œå—å¤§å°
        future_obs_consistency_check=True
    ):
        super(VLAConsumerDatasetWithFLARE, self).__init__()

        # ... ä¿æŒåŸæœ‰çš„åˆå§‹åŒ–ä»£ç  ...
        # åŠ è½½æ§åˆ¶é¢‘ç‡
        with open("configs/dataset_control_freq.json", "r") as fp:
            self.control_freq = json.load(fp)
        
        # åŠ è½½æ•°æ®é›†åç§°
        dataset_names_cfg = ("configs/pretrain_datasets.json"
                             if dataset_type == "pretrain" else "configs/finetune_datasets.json")
        with open(dataset_names_cfg, "r") as file:
            DATASET_NAMES = json.load(file)
        
        # åˆ›å»ºæ•°æ®é›†åç§°å’ŒIDä¹‹é—´çš„æ˜ å°„
        self.dataset_name2id = {name: i for i, name in enumerate(DATASET_NAMES)}
        self.dataset_id2name = {i: name for i, name in enumerate(DATASET_NAMES)}

        # æ·»åŠ ä¸€è‡´æ€§æ£€æŸ¥å‚æ•°
        self.future_obs_consistency_check = future_obs_consistency_check
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.future_obs_stats = {
            'total_samples': 0,
            'valid_future_obs': 0,
            'invalid_future_obs': 0,
            'synthetic_future_obs': 0
        }
        
        
        self.image_processor = image_processor
        self.model_config_path = model_config_path
        self.buffer_dir = config["buf_path"]
        self.num_chunks = config["buf_num_chunks"]
        self.chunk_size = config["buf_chunk_size"]
        self.tokenizer_max_length = config["tokenizer_max_length"]
        self.image_aspect_ratio = config["image_aspect_ratio"]
        self.state_noise_snr = state_noise_snr
        self.num_cameras = num_cameras
        self.img_history_size = img_history_size
        self.cond_mask_prob = cond_mask_prob
        self.cam_ext_mask_prob = cam_ext_mask_prob
        self.use_hdf5 = use_hdf5
        self.hdf5_dataset = None
        
        # FLAREç‰¹å®šå±æ€§
        self.enable_future_obs = enable_future_obs
        self.future_obs_prob = future_obs_prob
        self.action_chunk_size = action_chunk_size
        
        if use_hdf5:
            self.hdf5_dataset = HDF5VLADataset(self.model_config_path)
        self.use_precomp_lang_embed = use_precomp_lang_embed
        if use_precomp_lang_embed:
            self.empty_lang_embed = torch.load("data/empty_lang_embed.pt")

        # åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        with open("configs/dataset_stat.json", "r") as f:
            dataset_stat = json.load(f)
        self.dataset_stat = dataset_stat

        self.tokenizer = tokenizer
        self.image_size = image_size
        self.auto_adjust_image_brightness = auto_adjust_image_brightness
        self.image_aug = image_aug

        self.last_content = None
        self.last_meta = None

    def get_dataset_name2id(self):
        return self.dataset_name2id

    def get_dataset_id2name(self):
        return self.dataset_id2name

    @staticmethod
    def pairwise(iterable):
        a = iter(iterable)
        return zip(a, a)

    def _load_data_from_chunk(self, chunk_dir, chunk_item_idx):
        """ä»chunkåŠ è½½æ•°æ®"""
        time_stmp = time.time()
        while time.time() - time_stmp < 10.0:
            try:
                locks = []
                file_path = os.path.join(chunk_dir, f"json_content_{chunk_item_idx}.json")
                lock = FileLock(file_path)
                locks.append(lock)
                lock.acquire_read_lock()
                with open(file_path, "r") as file:
                    json_content = json.load(file)
                lock.release_lock()
                
                file_path = os.path.join(chunk_dir, f"sample_{chunk_item_idx}.npz")
                lock = FileLock(file_path)
                locks.append(lock)
                lock.acquire_read_lock()
                with open(file_path, "rb") as file:
                    sample_dict = np.load(file)
                    meta = tuple(sample_dict.values())
                lock.release_lock()
                return json_content, meta
            except KeyboardInterrupt:
                for lock in locks:
                    lock.release_lock()
                raise KeyboardInterrupt
            except BaseException:
                for lock in locks:
                    lock.release_lock()
                continue
        raise RuntimeError("Failed to load sample.")

    def _safe_load(self, index):
        """å®‰å…¨åŠ è½½æ•°æ®"""
        from data.producer import get_dirty_item, read_dirty_bit, save_dirty_bit
        
        read_chunk_item_indices = []
        read_chunk_idx = index // self.chunk_size
        
        while len(read_chunk_item_indices) == 0:
            read_chunk_dir = os.path.join(self.buffer_dir, f"chunk_{read_chunk_idx}")
            try:
                read_chunk_item_indices = get_dirty_item(read_chunk_dir)
            except BaseException as e:
                print("Error catched when searching a clean chunk:", e)
                traceback.print_exc()
                read_chunk_item_indices = []
            read_chunk_idx = (read_chunk_idx + 1) % self.num_chunks

        random_item_index = index % len(read_chunk_item_indices)
        read_chunk_item_index = read_chunk_item_indices[random_item_index]

        # ä¿®æ”¹dirty bit
        try:
            dirty_bit = read_dirty_bit(read_chunk_dir)
            dirty_bit[read_chunk_item_index] = 1
            save_dirty_bit(read_chunk_dir, dirty_bit)
        except BaseException as e:
            print("Error catched when modifying the dirty bit:", e)
            traceback.print_exc()

        # åŠ è½½æ ·æœ¬
        try:
            content, meta = self._load_data_from_chunk(read_chunk_dir, read_chunk_item_index)
            self.last_content, self.last_meta = content, meta
        except BaseException as e:
            print("Error catched when loading sample:", e)
            traceback.print_exc()
            content, meta = self.last_content, self.last_meta

        return (content, *meta)

    def _extract_future_obs_from_chunk_data(self, chunk_data):
        """
        ä»chunkæ•°æ®ä¸­æå–æœªæ¥è§‚æµ‹
        æ ¸å¿ƒï¼šç›´æ¥ä»ç°æœ‰æ•°æ®ä¸­åŠ¨æ€è®¡ç®—æœªæ¥è§‚æµ‹
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¡ç®—çš„æœªæ¥è§‚æµ‹æ•°æ®
            if len(chunk_data) >= 16:  # åŸæœ‰14ä¸ªå­—æ®µ + 2ä¸ªæœªæ¥è§‚æµ‹å­—æ®µ
                future_obs_frame = chunk_data[14] if len(chunk_data) > 14 else None
                future_obs_mask = chunk_data[15] if len(chunk_data) > 15 else False
                return future_obs_frame, future_obs_mask
            
            # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„æ•°æ®ï¼Œè¿”å›None
            return None, False
            
        except Exception as e:
            print(f"Error extracting future obs from chunk data: {e}")
            return None, False

    def _compute_future_obs_from_episode_data(self, content, image_metas, current_step_id):
        """
        ä»episodeæ•°æ®åŠ¨æ€è®¡ç®—æœªæ¥è§‚æµ‹
        æ ¸å¿ƒé€»è¾‘ï¼šæœªæ¥è§‚æµ‹å¯¹åº”action chunkçš„å®é™…ç»“æŸå¸§
        """
        try:
            # è·å–episodeæ€»æ­¥æ•°
            total_steps = content.get("#steps", len(image_metas[0]))
            
            # è®¡ç®—ç†æƒ³çš„action chunkç»“æŸæ­¥éª¤
            ideal_end_step = current_step_id + self.action_chunk_size - 1
            
            # è®¡ç®—å®é™…çš„action chunkç»“æŸæ­¥éª¤
            if ideal_end_step < total_steps:
                # å®Œæ•´çš„action chunk
                actual_end_step = ideal_end_step
                future_obs_mask = True
            else:
                # ä¸å®Œæ•´çš„action chunkï¼Œä½¿ç”¨episodeæœ€åä¸€æ­¥
                actual_end_step = total_steps - 1
                future_obs_mask = True  # ä»ç„¶æœ‰æ•ˆ
            
            # ä»ä¸»æ‘„åƒå¤´è·å–å¯¹åº”å¸§
            main_camera_images = image_metas[0]  # ä¸»æ‘„åƒå¤´çš„å›¾åƒåºåˆ—
            
            if actual_end_step < len(main_camera_images):
                future_obs_frame = main_camera_images[actual_end_step]
            else:
                # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨æœ€åä¸€å¸§
                future_obs_frame = main_camera_images[-1]
            
            return future_obs_frame, future_obs_mask
            
        except Exception as e:
            print(f"Error computing future obs from episode data: {e}")
            return None, False

    def _process_future_obs_image(self, future_obs_frame):
        """å¤„ç†æœªæ¥è§‚æµ‹å›¾åƒ"""
        try:
            if future_obs_frame is None:
                return None
            
            # æ£€æŸ¥å›¾åƒæ˜¯å¦ä¸ºç©ºï¼ˆå½¢çŠ¶ä¸º0æˆ–å…¨é›¶ï¼‰
            if hasattr(future_obs_frame, 'shape'):
                if any(dim == 0 for dim in future_obs_frame.shape):
                    print("âš ï¸  æœªæ¥è§‚æµ‹å›¾åƒå½¢çŠ¶åŒ…å«0ç»´åº¦")
                    return None
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºå…¨é›¶å›¾åƒ
                if hasattr(future_obs_frame, 'sum') and future_obs_frame.sum() == 0:
                    print("âš ï¸  æœªæ¥è§‚æµ‹å›¾åƒä¸ºå…¨é›¶")
                    return None
                
            # è½¬æ¢ä¸ºPILå›¾åƒ
            future_image = Image.fromarray(future_obs_frame)
            
            # åº”ç”¨ç›¸åŒçš„å›¾åƒå¤„ç†æµç¨‹
            if self.image_size is not None:
                future_image = transforms.Resize(self.image_size)(future_image)
            
            if self.auto_adjust_image_brightness:
                pixel_values = list(future_image.getdata())
                average_brightness = sum(sum(pixel) for pixel in pixel_values) / (len(pixel_values) * 255.0 * 3)
                if average_brightness <= 0.15:
                    future_image = transforms.ColorJitter(brightness=(1.75, 1.75))(future_image)
            
            # è½»å¾®çš„å›¾åƒå¢å¼ºï¼ˆé¿å…å½±å“ç›®æ ‡ç”Ÿæˆè´¨é‡ï¼‰
            if self.image_aug and random.random() > 0.9:  # åªæœ‰10%æ¦‚ç‡
                future_image = transforms.ColorJitter(brightness=0.05, contrast=0.05)(future_image)
            
            # å›¾åƒå¡«å……
            if self.image_aspect_ratio == "pad":
                def expand2square(pil_img, background_color):
                    width, height = pil_img.size
                    if width == height:
                        return pil_img
                    elif width > height:
                        result = Image.new(pil_img.mode, (width, width), background_color)
                        result.paste(pil_img, (0, (width - height) // 2))
                        return result
                    else:
                        result = Image.new(pil_img.mode, (height, height), background_color)
                        result.paste(pil_img, ((height - width) // 2, 0))
                        return result
                        
                future_image = expand2square(
                    future_image, 
                    tuple(int(x * 255) for x in self.image_processor.image_mean)
                )
            
            # æœ€ç»ˆé¢„å¤„ç†
            future_obs_tensor = self.image_processor.preprocess(
                future_image, 
                return_tensors="pt"
            )["pixel_values"][0]
            
            return future_obs_tensor
            
        except Exception as e:
            print(f"Error processing future observation image: {e}")
            return None

    def __len__(self) -> int:
        if self.use_hdf5:
            return len(self.hdf5_dataset)
        else:
            return self.num_chunks * self.chunk_size
    def _validate_action_future_obs_consistency(self, actions, future_obs_frame, step_id, content):
        """
        éªŒè¯åŠ¨ä½œåºåˆ—å’Œæœªæ¥è§‚æµ‹çš„ä¸€è‡´æ€§
        ç¡®ä¿å½“åŠ¨ä½œè¢«å¡«å……æ—¶ï¼Œæœªæ¥è§‚æµ‹å¯¹åº”æ­£ç¡®çš„å¸§
        """
        try:
            total_steps = content.get("#steps", 0)
            ideal_end_step = step_id + self.action_chunk_size - 1
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŠ¨ä½œå¡«å……çš„æƒ…å†µ
            if ideal_end_step >= total_steps:
                # åº”è¯¥ä½¿ç”¨æœ€åä¸€å¸§ä½œä¸ºæœªæ¥è§‚æµ‹
                expected_future_step = total_steps - 1
                return True
            else:
                # å®Œæ•´çš„action chunkï¼Œæœªæ¥è§‚æµ‹åº”è¯¥å¯¹åº”chunkç»“æŸå¸§
                return True
                
        except Exception as e:
            print(f"Error in action-future obs consistency validation: {e}")
            return False
    # train/dataset.py - å…³é”®ä¿®å¤éƒ¨åˆ†
    def _get_buffer_sample_data(self, index):
        """ä»bufferè·å–æ•°æ® - ç®€å•ä¿®å¤"""
        loaded_data = self._safe_load(index)
        
        # åŸºç¡€æ•°æ®è§£æ
        (
            content,
            step_id,
            states,
            _,  # state_chunk_time_mask
            actions,
            _,  # action_chunk_time_mask
            state_elem_mask,
            *image_metas,
            state_std,
            state_mean,
            state_norm,
        ) = loaded_data[:14]  # å–å‰14ä¸ªå­—æ®µ

        # æå–æœªæ¥è§‚æµ‹
        future_obs_frame, future_obs_mask = self._extract_future_obs_from_chunk_data(loaded_data)
        
        # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„æœªæ¥è§‚æµ‹ï¼ŒåŠ¨æ€è®¡ç®—
        if future_obs_frame is None and self.enable_future_obs:
            future_obs_frame, future_obs_mask = self._compute_future_obs_from_episode_data(
                content, image_metas, step_id
            )

        # éªŒè¯æœªæ¥è§‚æµ‹è´¨é‡
        valid_future_obs = self._validate_future_obs(
            future_obs_frame, future_obs_mask, step_id, content
        )
        
        if not valid_future_obs and self.enable_future_obs:
            # å°è¯•ä½¿ç”¨æœ€åä¸€å¸§
            try:
                main_camera_images = image_metas[0]
                if len(main_camera_images) > 0:
                    future_obs_frame = main_camera_images[-1]
                    valid_future_obs = self._validate_future_obs(future_obs_frame, True, -1, content)
                    if valid_future_obs:
                        self.future_obs_stats['synthetic_future_obs'] += 1
            except:
                valid_future_obs = False

        # æ›´æ–°ç»Ÿè®¡
        if valid_future_obs:
            self.future_obs_stats['valid_future_obs'] += 1
        else:
            self.future_obs_stats['invalid_future_obs'] += 1
        if self.enable_future_obs and valid_future_obs:
            consistency_check = self._validate_action_future_obs_consistency(
                actions, future_obs_frame, step_id, content
            )
            if not consistency_check:
                valid_future_obs = False
        # æ„å»ºæ•°æ®å­—å…¸
        data_dict = self._build_data_dict(
            content, states, actions, state_elem_mask, image_metas,
            state_std, state_mean, state_norm,
            future_obs_frame, valid_future_obs
        )
        
        return data_dict
    def _build_data_dict(self, content, states, actions, state_elem_mask, image_metas,
                     state_std, state_mean, state_norm, future_obs_frame, has_future_obs):
        """æ„å»ºæ•°æ®å­—å…¸ - ç®€å•å®ç°"""
    
        data_dict = {}
        data_dict["dataset_name"] = content["dataset_name"]
        data_dict["data_idx"] = self.dataset_name2id.get(data_dict["dataset_name"], 0)
        data_dict["ctrl_freq"] = self.control_freq.get(data_dict["dataset_name"], 25)

        # åŸºç¡€æ•°æ®
        data_dict["states"] = states
        data_dict["actions"] = actions
        data_dict["state_elem_mask"] = state_elem_mask
        data_dict["state_norm"] = state_norm

        # å¤„ç†å†å²å›¾åƒï¼ˆä¿æŒæ‚¨çš„åŸæœ‰é€»è¾‘ï¼‰
        background_color = np.array([int(x * 255) for x in self.image_processor.image_mean], dtype=np.uint8).reshape(1, 1, 3)
        background_image = np.ones((self.image_processor.size["height"], self.image_processor.size["width"], 3), dtype=np.uint8) * background_color

        image_metas = list(self.pairwise(image_metas))
        mask_probs = [self.cond_mask_prob] * self.num_cameras
        if self.cam_ext_mask_prob >= 0.0:
            mask_probs[0] = self.cam_ext_mask_prob

        rearranged_images = []
        for i in range(self.img_history_size):
            for j in range(self.num_cameras):
                if j < len(image_metas):
                    images, image_mask = image_metas[j]
                    if i < len(images) and i < len(image_mask):
                        image, valid = images[i], image_mask[i]
                        if valid and math.prod(image.shape) > 0 and random.random() > mask_probs[j]:
                            rearranged_images.append(image)
                        else:
                            rearranged_images.append(background_image.copy())
                    else:
                        rearranged_images.append(background_image.copy())
                else:
                    rearranged_images.append(background_image.copy())

        # é¢„å¤„ç†å›¾åƒ
        preprocessed_images = []
        for image in rearranged_images:
            processed_image = self._preprocess_single_image(image)
            preprocessed_images.append(processed_image)
        data_dict["images"] = preprocessed_images

        # å¤„ç†æœªæ¥è§‚æµ‹å›¾åƒ
        # future_obs_image = None
        # if has_future_obs and future_obs_frame is not None:
        #     future_obs_image = self._process_future_obs_image(future_obs_frame)
        #     if future_obs_image is None:
        #         has_future_obs = False

        # data_dict["future_obs_image"] = future_obs_image
        # data_dict["has_future_obs"] = has_future_obs
        future_obs_image = None
        if has_future_obs and future_obs_frame is not None:
            #print(f"ğŸ”§ å¼€å§‹å¤„ç†æœªæ¥è§‚æµ‹å›¾åƒï¼Œè¾“å…¥has_future_obs={has_future_obs}")
            future_obs_image = self._process_future_obs_image(future_obs_frame)
            #print(f"ğŸ”§ å›¾åƒå¤„ç†ç»“æœ: {type(future_obs_image)}")
            if future_obs_image is None:
                #print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥ï¼has_future_obsè®¾ä¸ºFalse")
                has_future_obs = False

        data_dict["future_obs_image"] = future_obs_image
        data_dict["has_future_obs"] = has_future_obs
        #print(f"ğŸ”§ æœ€ç»ˆè¾“å‡ºhas_future_obs={has_future_obs}")
        # å¤„ç†æ–‡æœ¬æŒ‡ä»¤
        text_instruction = content.get("instruction", "")
        if isinstance(text_instruction, bytes):
            text_instruction = text_instruction.decode('utf-8')
        data_dict["text_instruction"] = text_instruction

        # è¯­è¨€åµŒå…¥å¤„ç†
        if self.use_precomp_lang_embed:
            try:
                lang_embed = torch.load(content["instruction"]) if random.random() > self.cond_mask_prob else self.empty_lang_embed
                data_dict["lang_embed"] = lang_embed
            except:
                data_dict["lang_embed"] = self.empty_lang_embed
        else:
            instruction = text_instruction if random.random() > self.cond_mask_prob else ""
            tokenized = self.tokenizer(instruction, return_tensors="pt", padding="longest", truncation=True, max_length=self.tokenizer_max_length)
            data_dict["input_ids"] = tokenized.input_ids[0]

        # è½¬æ¢ä¸ºtensor
        for k, v in data_dict.items():
            if isinstance(v, np.ndarray):
                data_dict[k] = torch.from_numpy(v)

        return data_dict
    def _preprocess_single_image(self, image):
        """é¢„å¤„ç†å•ä¸ªå›¾åƒ - ç®€å•å®ç°"""
        try:
            image = Image.fromarray(image)
            
            if self.image_size is not None:
                image = transforms.Resize(self.image_size)(image)

            if self.image_aspect_ratio == "pad":
                # ç®€å•çš„æ­£æ–¹å½¢å¡«å……
                background_color = tuple(int(x * 255) for x in self.image_processor.image_mean)
                width, height = image.size
                if width != height:
                    size = max(width, height)
                    result = Image.new(image.mode, (size, size), background_color)
                    result.paste(image, ((size - width) // 2, (size - height) // 2))
                    image = result

            image = self.image_processor.preprocess(image, return_tensors="pt")["pixel_values"][0]
            return image
        except:
            # å¤±è´¥æ—¶è¿”å›é›¶å¼ é‡
            return torch.zeros(3, 224, 224)
    
    def __getitem__(self, index):
        """
        å¢å¼ºçš„æ•°æ®è·å–ï¼Œç¡®ä¿æœªæ¥è§‚æµ‹è´¨é‡å’Œä¸€è‡´æ€§
        """
        while True:
            try:
                data_dict = self._get_sample_data(index)
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if self._validate_sample_data(data_dict):
                    self.future_obs_stats['total_samples'] += 1
                    return data_dict
                else:
                    # æ•°æ®æ— æ•ˆï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ ·æœ¬
                    index = (index + 1) % len(self)
                    continue
                    
            except Exception as e:
                print(f"Error in __getitem__: {e}")
                index = (index + 1) % len(self)
                continue

    
    def _get_sample_data(self, index):
        """è·å–æ ·æœ¬æ•°æ®çš„æ ¸å¿ƒé€»è¾‘"""
        if self.use_hdf5:
            return self._get_hdf5_sample_data(index)
        else:
            return self._get_buffer_sample_data(index)
    
    def _get_hdf5_sample_data(self, index):
        """ä»HDF5è·å–æ•°æ® - ä¿®å¤ç‰ˆæœ¬"""
        res = self.hdf5_dataset.get_item(index)
        
        # åŸºç¡€æ•°æ®
        content = res["meta"]
        states = res["state"] 
        actions = res["actions"]
        state_elem_mask = res["state_indicator"]
        
        # å®Œæ•´çš„å›¾åƒæ•°æ®
        image_metas = [
            res["cam_high"], res["cam_high_mask"],
            res["cam_left_wrist"], res["cam_left_wrist_mask"],
            res["cam_right_wrist"], res["cam_right_wrist_mask"],
        ]
        
        state_std = res["state_std"]
        state_mean = res["state_mean"]
        state_norm = res["state_norm"]
        
        # æœªæ¥è§‚æµ‹å¤„ç†
        future_obs_frame = res.get("future_obs_frame")
        future_obs_mask = res.get("future_obs_mask", False)
        future_step_id = res.get("future_step_id", -1)
        
        # éªŒè¯æœªæ¥è§‚æµ‹
        valid_future_obs = self._validate_future_obs(future_obs_frame, future_obs_mask, future_step_id, content)
        
        if not valid_future_obs and self.enable_future_obs:
            # å°è¯•ä½¿ç”¨æœ€åä¸€å¸§
            try:
                main_camera_images = image_metas[0]
                if len(main_camera_images) > 0:
                    future_obs_frame = main_camera_images[-1]
                    valid_future_obs = self._validate_future_obs(future_obs_frame, True, -1, content)
                    if valid_future_obs:
                        self.future_obs_stats['synthetic_future_obs'] += 1
            except:
                valid_future_obs = False
        
        # æ›´æ–°ç»Ÿè®¡
        if valid_future_obs:
            self.future_obs_stats['valid_future_obs'] += 1
        else:
            self.future_obs_stats['invalid_future_obs'] += 1
        
        # æ„å»ºæ•°æ®å­—å…¸
        data_dict = self._build_data_dict(
            content, states, actions, state_elem_mask, image_metas,
            state_std, state_mean, state_norm,
            future_obs_frame, valid_future_obs
        )
        
        return data_dict
    
    def _validate_future_obs(self, future_obs_frame, future_obs_mask, future_step_id, content):
        """éªŒè¯æœªæ¥è§‚æµ‹çš„è´¨é‡"""
        if future_obs_frame is None:
            return False
        
        # æ£€æŸ¥å›¾åƒæœ‰æ•ˆæ€§
        if hasattr(future_obs_frame, 'shape'):
            # æ£€æŸ¥å½¢çŠ¶
            if any(dim <= 0 for dim in future_obs_frame.shape):
                return False
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå›¾åƒ
            if hasattr(future_obs_frame, 'sum') and future_obs_frame.sum() == 0:
                return False
            
            # æ£€æŸ¥å›¾åƒå†…å®¹æ˜¯å¦åˆç† (é¿å…å…¨ç™½æˆ–å…¨é»‘)
            if hasattr(future_obs_frame, 'std'):
                std_val = future_obs_frame.std()
                if std_val < 1.0:  # å›¾åƒå˜åŒ–å¤ªå°ï¼Œå¯èƒ½æ˜¯æ— æ•ˆå›¾åƒ
                    return False
        
        # å¦‚æœå¯ç”¨ä¸€è‡´æ€§æ£€æŸ¥
        if self.future_obs_consistency_check:
            # æ£€æŸ¥æœªæ¥æ­¥éª¤IDæ˜¯å¦åˆç†
            current_step = content.get("#steps", 0)
            if future_step_id >= current_step:
                return False
        
        return True
    
    def _generate_synthetic_future_obs(self, res, content):
        """ç”Ÿæˆåˆæˆçš„æœªæ¥è§‚æµ‹"""
        try:
            # ä½¿ç”¨æœ€åä¸€å¸§ä½œä¸ºæœªæ¥è§‚æµ‹
            image_metas = [
                res["cam_high"],
                res["cam_high_mask"],
                # ... å…¶ä»–æ‘„åƒå¤´
            ]
            
            main_camera_images = image_metas[0]
            if len(main_camera_images) > 0:
                # ä½¿ç”¨æœ€åä¸€å¸§
                synthetic_frame = main_camera_images[-1]
                
                # éªŒè¯åˆæˆå¸§
                if self._validate_future_obs(synthetic_frame, True, -1, content):
                    return synthetic_frame, True
            
            return None, False
            
        except Exception as e:
            print(f"Failed to generate synthetic future obs: {e}")
            return None, False
    
    def _validate_sample_data(self, data_dict):
        """éªŒè¯æ ·æœ¬æ•°æ®çš„å®Œæ•´æ€§"""
        required_keys = [
            "states", "actions", "images", "text_instruction"
        ]
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for key in required_keys:
            if key not in data_dict:
                return False
        
        # æ£€æŸ¥tensorå½¢çŠ¶
        try:
            states = data_dict["states"]
            actions = data_dict["actions"] 
            images = data_dict["images"]
            
            if states.shape[0] != 1:  # çŠ¶æ€åº”è¯¥æ˜¯1ä¸ªtoken
                return False
                
            if len(images) == 0:  # å¿…é¡»æœ‰å›¾åƒ
                return False
                
        except Exception:
            return False
        
        return True
    
    def get_stats(self):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        total = self.future_obs_stats['total_samples']
        if total > 0:
            stats = {
                'total_samples': total,
                'valid_future_obs_ratio': self.future_obs_stats['valid_future_obs'] / total,
                'invalid_future_obs_ratio': self.future_obs_stats['invalid_future_obs'] / total,
                'synthetic_future_obs_ratio': self.future_obs_stats['synthetic_future_obs'] / total,
            }
        else:
            stats = self.future_obs_stats.copy()
        
        return stats
    

class DataCollatorForVLAConsumerDatasetWithFLARE(object):
    """æ”¯æŒFLAREåŠŸèƒ½çš„æ•°æ®æ”¶é›†å™¨"""

    def __init__(self, tokenizer: transformers.PreTrainedTokenizer) -> None:
        self.tokenizer = tokenizer

    def __call__(self, instances):
        batch = {
            "states": [],
            "actions": [],
            "state_elem_mask": [],
            "state_norm": [],
            "images": [],
            "data_indices": [],
            "ctrl_freqs": [],
            "future_obs_images": [],  # æœªæ¥è§‚æµ‹å›¾åƒ
            "has_future_obs": [],     # æ˜¯å¦æœ‰æœ‰æ•ˆçš„æœªæ¥è§‚æµ‹
            "text_instructions": [],  # æ–‡æœ¬æŒ‡ä»¤
        }
        valid_future_obs_count = 0
        total_count = len(instances)
        
        input_ids = []
        lang_embeds = []
        lang_embed_lens = []

        for instance in instances:
            # è½¬æ¢numpyæ•°ç»„ä¸ºtensor
            keys_to_check = [
                "states",
                "actions", 
                "state_elem_mask",
                "state_norm",
            ]
            for key in keys_to_check:
                item = instance[key]
                if not isinstance(item, torch.Tensor):
                    item = torch.from_numpy(item)
                batch[key].append(item)

            # å¤„ç†è¯­è¨€æ•°æ®
            if "input_ids" in instance:
                input_ids.append(instance["input_ids"])
            else:
                lang_embeds.append(instance["lang_embed"])
                lang_embed_lens.append(instance["lang_embed"].shape[0])

            # å¤„ç†å›¾åƒæ•°æ®
            batch["images"].append(torch.stack(instance["images"], dim=0))
            batch["data_indices"].append(instance["data_idx"])
            batch["ctrl_freqs"].append(instance["ctrl_freq"])
            batch["text_instructions"].append(instance.get("text_instruction", ""))
            
            # å¤„ç†æœªæ¥è§‚æµ‹ (å…³é”®ä¿®å¤)
            future_obs_image = instance.get("future_obs_image")
            has_future_obs = instance.get("has_future_obs", False)
            
            #print(f"ğŸ“¦ æ ·æœ¬{len(batch['images'])-1}: è¾“å…¥has_future_obs={has_future_obs}")
            
            # éªŒè¯æœªæ¥è§‚æµ‹è´¨é‡
            if future_obs_image is not None and has_future_obs:
                #print(f"   ğŸ” è¿›è¡ŒtensoréªŒè¯...")
                # åŒé‡éªŒè¯
                if self._validate_future_obs_tensor(future_obs_image):
                    #print(f"   âœ… tensoréªŒè¯é€šè¿‡")
                    batch["future_obs_images"].append(future_obs_image)
                    batch["has_future_obs"].append(True)
                    valid_future_obs_count += 1
                else:
                    #print(f"   âŒ tensoréªŒè¯å¤±è´¥")
                    # ä½¿ç”¨é›¶å¡«å……
                    dummy_shape = instance["images"][0].shape
                    batch["future_obs_images"].append(torch.zeros(dummy_shape))
                    batch["has_future_obs"].append(False)
            else:
                #print(f"   âš ï¸ æœªæ¥è§‚æµ‹æ— æ•ˆ (image={type(future_obs_image)}, has_obs={has_future_obs})")
                # ä½¿ç”¨é›¶å¡«å……
                dummy_shape = instance["images"][0].shape
                batch["future_obs_images"].append(torch.zeros(dummy_shape))
                batch["has_future_obs"].append(False)

        # æ‰¹æ¬¡è´¨é‡æ£€æŸ¥å’Œtensorè½¬æ¢
        batch["future_obs_images"] = torch.stack(batch["future_obs_images"], dim=0)
        
        # è®°å½•æ‰¹æ¬¡ç»Ÿè®¡
        batch["future_obs_ratio"] = valid_future_obs_count / total_count
        
        # å¦‚æœæ‰¹æ¬¡ä¸­æœªæ¥è§‚æµ‹å¤ªå°‘ï¼Œå‘å‡ºè­¦å‘Š
        if valid_future_obs_count < total_count * 0.5:  # å°‘äº50%
            print(f"Warning: Low future obs ratio in batch: {valid_future_obs_count}/{total_count}")
        
        # å…¶ä½™å­—æ®µè½¬æ¢ä¸ºtensor
        for key in ["states", "actions", "state_elem_mask", "state_norm"]:
            batch[key] = torch.stack(batch[key], dim=0)
        batch["ctrl_freqs"] = torch.tensor(batch["ctrl_freqs"])
        batch["has_future_obs"] = torch.tensor(batch["has_future_obs"], dtype=torch.bool)

        # è¯­è¨€å¤„ç†
        if len(input_ids) > 0:
            input_ids = torch.nn.utils.rnn.pad_sequence(
                input_ids,
                batch_first=True,
                padding_value=self.tokenizer.pad_token_id
            )
            batch["input_ids"] = input_ids
            batch["lang_attn_mask"] = input_ids.ne(self.tokenizer.pad_token_id)
        else:
            lang_embeds = torch.nn.utils.rnn.pad_sequence(lang_embeds, batch_first=True, padding_value=0)
            input_lang_attn_mask = torch.zeros(lang_embeds.shape[0], lang_embeds.shape[1], dtype=torch.bool)
            for i, l in enumerate(lang_embed_lens):
                input_lang_attn_mask[i, :l] = True
            batch["lang_embeds"] = lang_embeds
            batch["lang_attn_mask"] = input_lang_attn_mask

        return batch
    def _validate_future_obs_tensor(self, tensor):
        """éªŒè¯æœªæ¥è§‚æµ‹tensorçš„è´¨é‡ - ç®€åŒ–ç‰ˆæœ¬"""
        if tensor is None:
            return False
        
        try:
            # åŸºæœ¬æ£€æŸ¥
            if not hasattr(tensor, 'shape') or tensor.ndim != 3:
                return False
            
            # åªæ£€æŸ¥æ˜¯å¦æœ‰æ„ä¹‰çš„å˜åŒ–
            if tensor.std().item() < 1e-6:
                return False
                
            # æ£€æŸ¥NaNå’ŒInf
            if torch.isnan(tensor).any() or torch.isinf(tensor).any():
                return False
                
            return True
            
        except Exception:
            return False
        
        
        