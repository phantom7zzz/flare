#!/usr/bin/env python
"""
FLAREé›†æˆæµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰FLAREç»„ä»¶æ˜¯å¦æ­£ç¡®é›†æˆ
"""

import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import yaml
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__)
project_root = current_file.parent.parent
sys.path.append(str(project_root))

def test_basic_imports():
    """æµ‹è¯•åŸºç¡€æ¨¡å—å¯¼å…¥"""
    print("ğŸ“¦ æµ‹è¯•åŸºç¡€æ¨¡å—å¯¼å…¥...")
    
    try:
        from models.multimodal_encoder.vl_token_generator import VLTokenGenerator
        print("  âœ… VL Tokenç”Ÿæˆå™¨å¯¼å…¥æˆåŠŸ")
        
        from models.multimodal_encoder.qformer_target_generator import QFormerTargetGenerator
        print("  âœ… Q-Formerç›®æ ‡ç”Ÿæˆå™¨å¯¼å…¥æˆåŠŸ")
        
        from models.rdt.dit_activation_extractor import FLAREActivationAligner
        print("  âœ… DiTæ¿€æ´»æå–å™¨å¯¼å…¥æˆåŠŸ")
        
        from models.rdt.model import RDTWithFLARE
        print("  âœ… FLARE RDTæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        
        from models.rdt_runner import RDTRunnerWithFLARE
        print("  âœ… FLARE RDT Runnerå¯¼å…¥æˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"  âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_config_generation():
    """æµ‹è¯•é…ç½®ç”Ÿæˆ"""
    print("\nâš™ï¸ æµ‹è¯•é…ç½®ç”Ÿæˆ...")
    
    try:
        # æ¨¡æ‹Ÿé…ç½®ç”Ÿæˆ
        test_config = {
            "model": "test_flare",
            "num_future_tokens": 32,
            "activation_layer": 6,
            "alignment_loss_weight": 0.1,
            "enable_flare": True,
        }
        
        # éªŒè¯é…ç½®
        assert test_config["num_future_tokens"] == 32
        assert test_config["activation_layer"] == 6
        assert test_config["enable_flare"] == True
        
        print("  âœ… é…ç½®ç”Ÿæˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"  âŒ é…ç½®ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_data_flow():
    """æµ‹è¯•æ•°æ®æµ"""
    print("\nğŸ’¾ æµ‹è¯•æ•°æ®æµ...")
    
    try:
        # æ¨¡æ‹Ÿæ•°æ®
        batch_size = 2
        action_chunk_size = 32
        
        # æ¨¡æ‹Ÿæœªæ¥è§‚æµ‹è®¡ç®—
        def compute_future_obs(current_step, episode_length):
            future_step = current_step + action_chunk_size - 1
            return min(future_step, episode_length - 1)
        
        # æµ‹è¯•ä¸åŒæƒ…å†µ
        test_cases = [
            (0, 100),   # æ­£å¸¸æƒ…å†µ
            (50, 100),  # ä¸­é—´æƒ…å†µ
            (90, 100),  # æ¥è¿‘è¾¹ç•Œ
            (99, 100),  # è¾¹ç•Œæƒ…å†µ
        ]
        
        for current_step, episode_length in test_cases:
            future_step = compute_future_obs(current_step, episode_length)
            assert 0 <= future_step < episode_length
            
        print("  âœ… æœªæ¥è§‚æµ‹è®¡ç®—é€»è¾‘æ­£ç¡®")
        print("  âœ… è¾¹ç•Œæƒ…å†µå¤„ç†æ­£ç¡®")
        return True
        
    except Exception as e:
        print(f"  âŒ æ•°æ®æµæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("\nğŸ¤– æµ‹è¯•æ¨¡å‹åˆ›å»º...")
    
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åŸºç¡€å‚æ•°
        hidden_size = 1152
        num_future_tokens = 32
        
        # åˆ›å»ºåŸºç¡€é…ç½®
        config = {
            'rdt': {
                'hidden_size': hidden_size,
                'depth': 8,
                'num_heads': 16,
            },
            'lang_adaptor': 'linear',
            'img_adaptor': 'linear',
            'state_adaptor': 'linear',
            'noise_scheduler': {
                'num_train_timesteps': 1000,
                'beta_schedule': 'linear',
                'prediction_type': 'epsilon',
                'clip_sample': True,
                'num_inference_timesteps': 100,
            }
        }
        
        # å°è¯•åˆ›å»ºFLARE Runner
        from models.rdt_runner import RDTRunnerWithFLARE
        
        runner = RDTRunnerWithFLARE(
            action_dim=128,
            pred_horizon=32,
            config=config,
            lang_token_dim=hidden_size,
            img_token_dim=hidden_size,
            state_token_dim=128,
            max_lang_cond_len=120,
            img_cond_len=1000,
            num_future_tokens=num_future_tokens,
            activation_layer=6,
            alignment_loss_weight=0.1,
            enable_flare=True,
        )
        
        print(f"  âœ… FLARE Runneråˆ›å»ºæˆåŠŸ")
        print(f"  âœ… å‚æ•°æ•°é‡: {sum(p.numel() for p in runner.parameters()):,}")
        return True
        
    except Exception as e:
        print(f"  âŒ æ¨¡å‹åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_loss_computation():
    """æµ‹è¯•æŸå¤±è®¡ç®—"""
    print("\nâš–ï¸ æµ‹è¯•æŸå¤±è®¡ç®—...")
    
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ¨¡æ‹ŸæŸå¤±è®¡ç®—
        diffusion_loss = torch.tensor(0.5)
        alignment_loss = torch.tensor(0.1)
        alignment_weight = 0.1
        
        total_loss = diffusion_loss + alignment_weight * alignment_loss
        
        expected_total = 0.5 + 0.1 * 0.1
        assert abs(total_loss.item() - expected_total) < 1e-6
        
        print(f"  âœ… æ‰©æ•£æŸå¤±: {diffusion_loss.item():.4f}")
        print(f"  âœ… å¯¹é½æŸå¤±: {alignment_loss.item():.4f}")
        print(f"  âœ… æ€»æŸå¤±: {total_loss.item():.4f}")
        return True
        
    except Exception as e:
        print(f"  âŒ æŸå¤±è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ”¥ FLAREé›†æˆæµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    tests = [
        test_basic_imports,
        test_config_generation,
        test_data_flow,
        test_model_creation,
        test_loss_computation,
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"æµ‹è¯• {test_func.__name__} å‡ºç°å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸŠ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼FLAREå®ç°å·²å°±ç»ªï¼")
        print("\nğŸš€ æ¥ä¸‹æ¥çš„æ­¥éª¤:")
        print("1. ç”Ÿæˆé…ç½®: python scripts/generate_flare_config.py your_task_name")
        print("2. å¼€å§‹è®­ç»ƒ: bash scripts/train_flare.sh your_task_name_flare")
        print("3. ç›‘æ§è®­ç»ƒ: æŸ¥çœ‹WandBæ—¥å¿—ä¸­çš„å¯¹é½æŸå¤±")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³ç»„ä»¶")
    
    print("=" * 60)
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
    