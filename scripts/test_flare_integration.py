import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import Image
import yaml
import os
import sys
from pathlib import Path
from contextlib import nullcontext

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__)
project_root = current_file.parent.parent
sys.path.append(str(project_root))

def setup_gpu_environment():
    """è®¾ç½®GPUç¯å¢ƒ"""
    print("ğŸ¯ è®¾ç½®GPUè®­ç»ƒç¯å¢ƒ...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒæµ‹è¯•")
        return None, None
    
    device = torch.device("cuda:0")
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    print(f"âœ… GPUè®¾å¤‡: {gpu_name}")
    print(f"âœ… GPUæ˜¾å­˜: {gpu_memory:.1f}GB")
    
    # è®¾ç½®ä¸ºBF16ä¼˜åŒ–
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    
    print("âœ… GPUä¼˜åŒ–è®¾ç½®å®Œæˆ")
    return device, gpu_memory

def test_gpu_tensor_operations():
    """æµ‹è¯•GPUä¸Šçš„åŸºç¡€å¼ é‡æ“ä½œ"""
    print("\nğŸ§® æµ‹è¯•GPUå¼ é‡æ“ä½œ...")
    
    device = torch.device("cuda:0")
    
    try:
        # æµ‹è¯•BF16å¼ é‡åˆ›å»ºå’Œæ“ä½œ
        x_fp32 = torch.randn(2, 32, 1152, device=device, dtype=torch.float32)
        x_bf16 = torch.randn(2, 32, 1152, device=device, dtype=torch.bfloat16)
        
        # æµ‹è¯•æ•°æ®ç±»å‹è½¬æ¢
        x_converted = x_fp32.to(torch.bfloat16)
        assert x_converted.device == device
        assert x_converted.dtype == torch.bfloat16
        
        # æµ‹è¯•åŸºç¡€è¿ç®—
        result = x_bf16 + x_converted
        assert result.dtype == torch.bfloat16
        
        # æµ‹è¯•autocast
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            linear = nn.Linear(1152, 1152).to(device=device, dtype=torch.bfloat16)
            output = linear(x_bf16)
            assert output.dtype == torch.bfloat16
        
        print("  âœ… BF16å¼ é‡æ“ä½œæ­£å¸¸")
        print("  âœ… æ•°æ®ç±»å‹è½¬æ¢æ­£å¸¸")
        print("  âœ… autocaståŠŸèƒ½æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"  âŒ GPUå¼ é‡æ“ä½œå¤±è´¥: {e}")
        return False

def test_rdt_components_on_gpu():
    """æµ‹è¯•RDTåŸºç¡€ç»„ä»¶åœ¨GPUä¸Šçš„è¿è¡Œ"""
    print("\nğŸ§© æµ‹è¯•GPUä¸Šçš„RDTç»„ä»¶...")
    
    device = torch.device("cuda:0")
    hidden_size = 1152
    
    try:
        from models.rdt.blocks import TimestepEmbedder, RDTBlock, FinalLayer
        
        # æ—¶é—´æ­¥åµŒå…¥å™¨
        t_embedder = TimestepEmbedder(hidden_size, dtype=torch.bfloat16).to(device)
        test_t = torch.randint(0, 1000, (2,), device=device)
        
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            t_embed = t_embedder(test_t)
            assert t_embed.device == device
            assert t_embed.shape == (2, hidden_size)
        
        print("  âœ… æ—¶é—´æ­¥åµŒå…¥å™¨GPUæµ‹è¯•é€šè¿‡")
        
        # RDT Block
        rdt_block = RDTBlock(hidden_size=hidden_size, num_heads=16).to(device, dtype=torch.bfloat16)
        test_x = torch.randn(2, 10, hidden_size, device=device, dtype=torch.bfloat16)
        test_c = torch.randn(2, 20, hidden_size, device=device, dtype=torch.bfloat16)
        
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            block_out = rdt_block(test_x, test_c)
            assert block_out.device == device
            assert block_out.shape == test_x.shape
            assert block_out.dtype == torch.bfloat16
        
        print("  âœ… RDT Block GPUæµ‹è¯•é€šè¿‡")
        
        # æœ€ç»ˆå±‚
        final_layer = FinalLayer(hidden_size=hidden_size, out_channels=128).to(device, dtype=torch.bfloat16)
        
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            final_out = final_layer(test_x)
            assert final_out.device == device
            assert final_out.shape == (2, 10, 128)
        
        print("  âœ… æœ€ç»ˆå±‚GPUæµ‹è¯•é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"  âŒ RDTç»„ä»¶GPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_flare_data_structures_gpu():
    """æµ‹è¯•FLAREæ•°æ®ç»“æ„åœ¨GPUä¸Šçš„å¤„ç†"""
    print("\nğŸ’¾ æµ‹è¯•GPUä¸Šçš„FLAREæ•°æ®ç»“æ„...")
    
    device = torch.device("cuda:0")
    batch_size = 2
    action_chunk_size = 32
    hidden_size = 1152
    
    try:
        # æ¨¡æ‹ŸFLAREè®­ç»ƒæ•°æ®ï¼ˆå…¨éƒ¨åœ¨GPUä¸Šï¼‰
        flare_batch = {
            "states": torch.randn(batch_size, 1, 128, device=device, dtype=torch.bfloat16),
            "actions": torch.randn(batch_size, action_chunk_size, 128, device=device, dtype=torch.bfloat16),
            "images": torch.randn(batch_size, 6, 3, 224, 224, device=device, dtype=torch.bfloat16),
            "future_obs_images": torch.randn(batch_size, 3, 224, 224, device=device, dtype=torch.bfloat16),
            "has_future_obs": torch.tensor([True, False], device=device),
            "lang_tokens": torch.randn(batch_size, 120, hidden_size, device=device, dtype=torch.bfloat16),
            "img_tokens": torch.randn(batch_size, 1000, hidden_size, device=device, dtype=torch.bfloat16),
        }
        
        # éªŒè¯æ•°æ®ç»“æ„
        for key, tensor in flare_batch.items():
            if isinstance(tensor, torch.Tensor):
                assert tensor.device == device, f"{key} ä¸åœ¨GPUä¸Š"
                if tensor.dtype.is_floating_point:
                    assert tensor.dtype == torch.bfloat16, f"{key} æ•°æ®ç±»å‹ä¸æ˜¯BF16"
        
        print("  âœ… FLAREæ•°æ®ç»“æ„GPUåˆ†é…æ­£ç¡®")
        
        # æµ‹è¯•ç»´åº¦å…¼å®¹æ€§
        img_seq_len = flare_batch["img_tokens"].shape[1]  # 1000
        
        # æ¨¡æ‹Ÿä½ç½®åµŒå…¥ç»´åº¦é—®é¢˜
        pos_embed_len = 4374  # ä½ é‡åˆ°çš„å®é™…é•¿åº¦
        if img_seq_len != pos_embed_len:
            print(f"  ğŸ” æ£€æµ‹åˆ°ç»´åº¦ä¸åŒ¹é…: {img_seq_len} vs {pos_embed_len}")
            
            # æµ‹è¯•åŠ¨æ€è°ƒæ•´ç­–ç•¥
            if img_seq_len < pos_embed_len:
                # æˆªæ–­ç­–ç•¥
                adjusted_embed = torch.randn(1, img_seq_len, hidden_size, device=device, dtype=torch.bfloat16)
                print(f"  ğŸ”§ æˆªæ–­ä½ç½®åµŒå…¥: {pos_embed_len} -> {img_seq_len}")
            else:
                # æ‰©å±•ç­–ç•¥
                base_embed = torch.randn(1, pos_embed_len, hidden_size, device=device, dtype=torch.bfloat16)
                extra_len = img_seq_len - pos_embed_len
                if extra_len <= pos_embed_len:
                    extra_embed = base_embed[:, -extra_len:, :]
                else:
                    repeat_times = (extra_len // pos_embed_len) + 1
                    repeated_embed = base_embed.repeat(1, repeat_times, 1)
                    extra_embed = repeated_embed[:, :extra_len, :]
                
                adjusted_embed = torch.cat([base_embed, extra_embed], dim=1)
                print(f"  ğŸ”§ æ‰©å±•ä½ç½®åµŒå…¥: {pos_embed_len} -> {img_seq_len}")
            
            assert adjusted_embed.shape[1] == img_seq_len
            assert adjusted_embed.device == device
            
        print("  âœ… ä½ç½®åµŒå…¥ç»´åº¦å¤„ç†ç­–ç•¥æµ‹è¯•é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"  âŒ FLAREæ•°æ®ç»“æ„GPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_loss_computation_gpu():
    """æµ‹è¯•GPUä¸Šçš„æŸå¤±è®¡ç®—"""
    print("\nâš–ï¸ æµ‹è¯•GPUæŸå¤±è®¡ç®—...")
    
    device = torch.device("cuda:0")
    
    try:
        # æ¨¡æ‹Ÿé¢„æµ‹å’Œç›®æ ‡
        pred = torch.randn(2, 32, 128, device=device, dtype=torch.bfloat16)
        target = torch.randn(2, 32, 128, device=device, dtype=torch.bfloat16)
        
        # åœ¨GPUä¸Šè®¡ç®—MSEæŸå¤±ï¼ˆBF16ï¼‰
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            diffusion_loss = F.mse_loss(pred, target)
            assert diffusion_loss.device == device
            assert not torch.isnan(diffusion_loss), "æŸå¤±è®¡ç®—å‡ºç°NaN"
        
        print("  âœ… GPU MSEæŸå¤±è®¡ç®—æ­£å¸¸")
        
        # æµ‹è¯•å¯¹é½æŸå¤±
        alignment_loss = torch.tensor(0.1, device=device, dtype=torch.bfloat16)
        alignment_weight = 0.1
        
        total_loss = diffusion_loss + alignment_weight * alignment_loss
        assert total_loss.device == device
        
        print("  âœ… å¯¹é½æŸå¤±è®¡ç®—æ­£å¸¸")
        
        # æµ‹è¯•æ¡ä»¶æŸå¤±ï¼ˆéƒ¨åˆ†æ ·æœ¬æœ‰æœªæ¥è§‚æµ‹ï¼‰
        has_future_obs = torch.tensor([True, False], device=device)
        valid_count = has_future_obs.sum().float()
        
        if valid_count > 0:
            scaled_alignment_loss = alignment_loss * (2 / valid_count)  # batch_size=2
            assert scaled_alignment_loss.device == device
        
        print("  âœ… æ¡ä»¶æŸå¤±è®¡ç®—æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"  âŒ GPUæŸå¤±è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_model_creation_gpu():
    """æµ‹è¯•åœ¨GPUä¸Šåˆ›å»ºFLAREæ¨¡å‹"""
    print("\nğŸ¤– æµ‹è¯•GPUä¸Šçš„FLAREæ¨¡å‹åˆ›å»º...")
    
    device = torch.device("cuda:0")
    
    try:
        # åŸºç¡€é…ç½®
        config = {
            'rdt': {
                'hidden_size': 1152,
                'depth': 6,  # å‡å°‘æ·±åº¦ä»¥åŠ å¿«æµ‹è¯•
                'num_heads': 16,
            },
            'lang_adaptor': 'linear',
            'img_adaptor': 'linear', 
            'state_adaptor': 'linear',
            'noise_scheduler': {
                'num_train_timesteps': 1000,
                'beta_schedule': 'linear',
                'prediction_type': 'epsilon',
                'clip_sample': True,
                'num_inference_timesteps': 100,
            }
        }
        
        # è·³è¿‡éœ€è¦å¤–éƒ¨æ¨¡å‹çš„å®Œæ•´FLAREåˆ›å»ºï¼Œåªæµ‹è¯•é€‚é…å™¨
        from models.rdt_runner import RDTRunnerWithFLARE
        
        print("  âš ï¸  è·³è¿‡å®Œæ•´æ¨¡å‹åˆ›å»ºï¼ˆé¿å…å¤–éƒ¨æ¨¡å‹ä¾èµ–ï¼‰")
        
        # æµ‹è¯•é€‚é…å™¨åˆ›å»º
        lang_token_dim = 1152
        img_token_dim = 1152
        hidden_size = 1152
        
        # åˆ›å»ºå•ç‹¬çš„é€‚é…å™¨è¿›è¡Œæµ‹è¯•
        lang_adaptor = nn.Linear(lang_token_dim, hidden_size).to(device, dtype=torch.bfloat16)
        img_adaptor = nn.Linear(img_token_dim, hidden_size).to(device, dtype=torch.bfloat16)
        
        # æµ‹è¯•é€‚é…å™¨å‰å‘ä¼ æ’­
        test_lang = torch.randn(2, 120, lang_token_dim, device=device, dtype=torch.bfloat16)
        test_img = torch.randn(2, 1000, img_token_dim, device=device, dtype=torch.bfloat16)
        
        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            adapted_lang = lang_adaptor(test_lang)
            adapted_img = img_adaptor(test_img)
            
            assert adapted_lang.device == device
            assert adapted_img.device == device
            assert adapted_lang.dtype == torch.bfloat16
            assert adapted_img.dtype == torch.bfloat16
        
        print("  âœ… é€‚é…å™¨åˆ›å»ºå’Œå‰å‘ä¼ æ’­æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"  âŒ GPUæ¨¡å‹åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_training_simulation_gpu():
    """æ¨¡æ‹ŸGPUè®­ç»ƒè¿‡ç¨‹"""
    print("\nğŸ‹ï¸ æ¨¡æ‹ŸGPUè®­ç»ƒè¿‡ç¨‹...")
    
    device = torch.device("cuda:0")
    
    try:
        # åˆ›å»ºç®€åŒ–çš„è®­ç»ƒç»„ä»¶
        hidden_size = 1152
        batch_size = 2
        
        # æ¨¡å‹ç»„ä»¶
        model = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 128)
        ).to(device, dtype=torch.bfloat16)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        inputs = torch.randn(batch_size, 32, hidden_size, device=device, dtype=torch.bfloat16)
        targets = torch.randn(batch_size, 32, 128, device=device, dtype=torch.bfloat16)
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
        for step in range(3):  # 3ä¸ªè®­ç»ƒæ­¥éª¤
            optimizer.zero_grad()
            
            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
                outputs = model(inputs)
                loss = F.mse_loss(outputs, targets)
            
            # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
            assert not torch.isnan(loss), f"ç¬¬{step}æ­¥å‡ºç°NaNæŸå¤±"
            assert loss.device == device
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦
            total_norm = 0.0
            for p in model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** (1. / 2)
            
            assert total_norm > 0, f"ç¬¬{step}æ­¥æ¢¯åº¦ä¸ºé›¶"
            assert not np.isnan(total_norm), f"ç¬¬{step}æ­¥æ¢¯åº¦ä¸ºNaN"
            
            optimizer.step()
            
            print(f"    æ­¥éª¤ {step+1}: æŸå¤±={loss.item():.4f}, æ¢¯åº¦èŒƒæ•°={total_norm:.4f}")
        
        print("  âœ… GPUè®­ç»ƒæ¨¡æ‹ŸæˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"  âŒ GPUè®­ç»ƒæ¨¡æ‹Ÿå¤±è´¥: {e}")
        return False

def test_memory_management():
    """æµ‹è¯•GPUæ˜¾å­˜ç®¡ç†"""
    print("\nğŸ’¾ æµ‹è¯•GPUæ˜¾å­˜ç®¡ç†...")
    
    try:
        device = torch.device("cuda:0")
        
        # è®°å½•åˆå§‹æ˜¾å­˜
        initial_memory = torch.cuda.memory_allocated() / 1024**2  # MB
        
        # åˆ›å»ºå¤§å‹å¼ é‡
        large_tensors = []
        for i in range(5):
            tensor = torch.randn(1024, 1024, device=device, dtype=torch.bfloat16)
            large_tensors.append(tensor)
        
        # è®°å½•ä½¿ç”¨åæ˜¾å­˜
        used_memory = torch.cuda.memory_allocated() / 1024**2
        memory_increase = used_memory - initial_memory
        
        print(f"  ğŸ“Š æ˜¾å­˜ä½¿ç”¨å¢åŠ : {memory_increase:.1f}MB")
        
        # æ¸…ç†æ˜¾å­˜
        del large_tensors
        torch.cuda.empty_cache()
        
        # è®°å½•æ¸…ç†åæ˜¾å­˜
        final_memory = torch.cuda.memory_allocated() / 1024**2
        memory_freed = used_memory - final_memory
        
        print(f"  ğŸ§¹ æ˜¾å­˜é‡Šæ”¾: {memory_freed:.1f}MB")
        print("  âœ… GPUæ˜¾å­˜ç®¡ç†æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"  âŒ GPUæ˜¾å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰GPUæµ‹è¯•"""
    print("ğŸš€ FLARE GPUè®­ç»ƒæµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # é¦–å…ˆè®¾ç½®GPUç¯å¢ƒ
    device, gpu_memory = setup_gpu_environment()
    if device is None:
        print("âŒ æ— æ³•è®¾ç½®GPUç¯å¢ƒï¼Œæµ‹è¯•ç»ˆæ­¢")
        return False
    
    tests = [
        ("GPUå¼ é‡æ“ä½œ", test_gpu_tensor_operations),
        ("RDTç»„ä»¶GPU", test_rdt_components_on_gpu),
        ("FLAREæ•°æ®ç»“æ„GPU", test_flare_data_structures_gpu),
        ("GPUæŸå¤±è®¡ç®—", test_loss_computation_gpu),
        ("GPUæ¨¡å‹åˆ›å»º", test_model_creation_gpu),
        ("GPUè®­ç»ƒæ¨¡æ‹Ÿ", test_training_simulation_gpu),
        ("GPUæ˜¾å­˜ç®¡ç†", test_memory_management),
    ]
    
    passed = 0
    total = len(tests)
    results = []
    
    for test_name, test_func in tests:
        print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        try:
            if test_func():
                passed += 1
                results.append(f"âœ… {test_name}")
            else:
                results.append(f"âŒ {test_name}")
        except Exception as e:
            print(f"  ğŸ’¥ æµ‹è¯• {test_name} å‡ºç°å¼‚å¸¸: {e}")
            results.append(f"ğŸ’¥ {test_name}: {str(e)[:50]}...")
    
    print("\n" + "=" * 60)
    print("ğŸ“Š GPUæµ‹è¯•ç»“æœæ±‡æ€»:")
    for result in results:
        print(f"  {result}")
    
    print(f"\nğŸ“ˆ æ€»ä½“ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("\nğŸŠ æ‰€æœ‰GPUæµ‹è¯•é€šè¿‡ï¼FLARE GPUè®­ç»ƒå·²å°±ç»ªï¼")
        print("\nğŸš€ GPUè®­ç»ƒå‡†å¤‡æ¸…å•:")
        print("âœ… GPUç¯å¢ƒé…ç½®æ­£ç¡®")
        print("âœ… BF16æ··åˆç²¾åº¦æ”¯æŒ")
        print("âœ… FLAREç»„ä»¶å…¼å®¹")
        print("âœ… æ˜¾å­˜ç®¡ç†æ­£å¸¸")
        print("\nğŸ”¥ å¯ä»¥å¼€å§‹GPUè®­ç»ƒï¼")
    elif passed >= total * 0.8:  # 80%ä»¥ä¸Šé€šè¿‡
        print("\nğŸ”¶ å¤§éƒ¨åˆ†GPUæµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥å°è¯•è®­ç»ƒ")
        print("âš ï¸  æ³¨æ„è§‚å¯Ÿå¤±è´¥çš„ç»„ä»¶")
    else:
        print("\nâŒ å¤šé¡¹GPUæµ‹è¯•å¤±è´¥ï¼Œè¯·å…ˆä¿®å¤å…³é”®é—®é¢˜")
    
    # æ˜¾ç¤ºGPUçŠ¶æ€
    print("\nğŸ“Š å½“å‰GPUçŠ¶æ€:")
    print(f"  æ˜¾å­˜å·²ç”¨: {torch.cuda.memory_allocated() / 1024**2:.1f}MB")
    print(f"  æ˜¾å­˜ä¿ç•™: {torch.cuda.memory_reserved() / 1024**2:.1f}MB")
    
    print("=" * 60)
    return passed >= total * 0.8

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)