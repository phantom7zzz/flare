#!/usr/bin/env python3
"""
FLAREè®­ç»ƒè¯Šæ–­è„šæœ¬
ç”¨äºæ£€æŸ¥alignment_lossä¸º0çš„åŸå› 
"""

import torch
import yaml
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

def debug_flare_data_loading(model_config_path="model_config/your_task_flare.yml"):
    """æ£€æŸ¥æ•°æ®åŠ è½½é˜¶æ®µçš„FLAREé…ç½®"""
    print("ğŸ” æ£€æŸ¥æ•°æ®åŠ è½½é…ç½®...")
    
    try:
        # åŠ è½½é…ç½®
        with open("configs/base.yaml", "r") as f:
            base_config = yaml.safe_load(f)
        
        with open(model_config_path, "r") as f:
            model_config = yaml.safe_load(f)
        
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"   - Base config loaded")
        print(f"   - Model config: {model_config_path}")
        
        # æ£€æŸ¥FLAREç›¸å…³é…ç½®
        flare_enabled = model_config.get("enable_flare", False)
        print(f"ğŸ“‹ FLAREé…ç½®:")
        print(f"   - enable_flare: {flare_enabled}")
        print(f"   - num_future_tokens: {model_config.get('num_future_tokens', 'Not set')}")
        print(f"   - activation_layer: {model_config.get('activation_layer', 'Not set')}")
        print(f"   - alignment_loss_weight: {model_config.get('alignment_loss_weight', 'Not set')}")
        
        return base_config, model_config
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return None, None

def debug_dataset_creation(base_config, model_config_path):
    """æ£€æŸ¥æ•°æ®é›†åˆ›å»ºå’Œæœªæ¥è§‚æµ‹ç”Ÿæˆ"""
    print("\nğŸ” æ£€æŸ¥æ•°æ®é›†åˆ›å»º...")
    
    try:
        from train.dataset import VLAConsumerDatasetWithFLARE
        from transformers import AutoTokenizer
        from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
        
        # åˆ›å»ºç¼–ç å™¨
        vision_encoder = SiglipVisionTower(
            vision_tower="google/siglip-so400m-patch14-384", 
            args=None
        )
        image_processor = vision_encoder.image_processor
        tokenizer = None  # ä½¿ç”¨é¢„è®¡ç®—çš„è¯­è¨€åµŒå…¥
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨FLAREå‚æ•°ï¼‰
        dataset = VLAConsumerDatasetWithFLARE(
            model_config_path=model_config_path,
            config=base_config["dataset"],
            tokenizer=tokenizer,
            image_processor=image_processor,
            num_cameras=base_config["common"]["num_cameras"],
            img_history_size=base_config["common"]["img_history_size"],
            dataset_type="finetune",
            enable_future_obs=True,          # å¼ºåˆ¶å¯ç”¨
            future_obs_prob=1.0,            # 100%æ¦‚ç‡ç”¨äºè°ƒè¯•
            action_chunk_size=base_config["common"]["action_chunk_size"],
            use_hdf5=True,
            use_precomp_lang_embed=True,
        )
        
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"   - enable_future_obs: {dataset.enable_future_obs}")
        print(f"   - future_obs_prob: {dataset.future_obs_prob}")
        print(f"   - action_chunk_size: {dataset.action_chunk_size}")
        
        return dataset
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def debug_single_sample(dataset):
    """æµ‹è¯•å•ä¸ªæ ·æœ¬çš„æœªæ¥è§‚æµ‹ç”Ÿæˆ"""
    print("\nğŸ” æµ‹è¯•å•ä¸ªæ ·æœ¬...")
    
    try:
        # è·å–ä¸€ä¸ªæ ·æœ¬
        sample = dataset[0]
        
        print(f"ğŸ“Š æ ·æœ¬æ•°æ®æ£€æŸ¥:")
        print(f"   - æ ·æœ¬keys: {list(sample.keys())}")
        
        # æ£€æŸ¥å…³é”®å­—æ®µ
        future_obs_image = sample.get('future_obs_image')
        has_future_obs = sample.get('has_future_obs', False)
        text_instruction = sample.get('text_instruction', '')
        
        print(f"   - has_future_obs: {has_future_obs}")
        print(f"   - future_obs_image: {future_obs_image is not None}")
        if future_obs_image is not None:
            print(f"   - future_obs_image shape: {future_obs_image.shape}")
            print(f"   - future_obs_image dtype: {future_obs_image.dtype}")
            print(f"   - future_obs_image range: [{future_obs_image.min():.3f}, {future_obs_image.max():.3f}]")
        
        print(f"   - text_instruction: '{text_instruction[:50]}...' (é•¿åº¦: {len(text_instruction)})")
        
        return sample
        
    except Exception as e:
        print(f"âŒ æ ·æœ¬æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def debug_dataloader(dataset):
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨æ‰¹æ¬¡å¤„ç†"""
    print("\nğŸ” æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
    
    try:
        from train.dataset import DataCollatorForVLAConsumerDatasetWithFLARE
        from torch.utils.data import DataLoader
        
        data_collator = DataCollatorForVLAConsumerDatasetWithFLARE(tokenizer=None)
        
        dataloader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            collate_fn=data_collator,
            num_workers=0,
        )
        
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡
        batch = next(iter(dataloader))
        
        print(f"ğŸ“Š æ‰¹æ¬¡æ•°æ®æ£€æŸ¥:")
        print(f"   - æ‰¹æ¬¡keys: {list(batch.keys())}")
        
        # æ£€æŸ¥æœªæ¥è§‚æµ‹ç›¸å…³å­—æ®µ
        if 'has_future_obs' in batch:
            has_future_obs = batch['has_future_obs']
            print(f"   - has_future_obs: {has_future_obs}")
            print(f"   - æœ‰æ•ˆæœªæ¥è§‚æµ‹æ•°é‡: {has_future_obs.sum()}/{len(has_future_obs)}")
        else:
            print(f"   - âŒ has_future_obså­—æ®µç¼ºå¤±!")
        
        if 'future_obs_images' in batch:
            future_obs_images = batch['future_obs_images']
            print(f"   - future_obs_images shape: {future_obs_images.shape}")
            print(f"   - future_obs_images dtype: {future_obs_images.dtype}")
        else:
            print(f"   - âŒ future_obs_imageså­—æ®µç¼ºå¤±!")
        
        if 'text_instructions' in batch:
            text_instructions = batch['text_instructions']
            print(f"   - text_instructionsæ•°é‡: {len(text_instructions)}")
            print(f"   - ç¤ºä¾‹æŒ‡ä»¤: '{text_instructions[0][:50]}...'")
        else:
            print(f"   - âŒ text_instructionså­—æ®µç¼ºå¤±!")
        
        return batch
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def debug_model_creation(base_config, model_config):
    """æ£€æŸ¥FLAREæ¨¡å‹åˆ›å»º"""
    print("\nğŸ” æ£€æŸ¥FLAREæ¨¡å‹åˆ›å»º...")
    
    try:
        from models.rdt_runner import RDTRunnerWithFLARE
        from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
        
        vision_encoder = SiglipVisionTower(
            vision_tower="google/siglip-so400m-patch14-384", 
            args=None
        )
        
        img_cond_len = (base_config["common"]["img_history_size"] * 
                       base_config["common"]["num_cameras"] * 
                       vision_encoder.num_patches)
        
        # åˆ›å»ºFLAREæ¨¡å‹
        rdt = RDTRunnerWithFLARE(
            action_dim=base_config["common"]["state_dim"],
            pred_horizon=base_config["common"]["action_chunk_size"],
            config=base_config["model"],
            lang_token_dim=base_config["model"]["lang_token_dim"],
            img_token_dim=base_config["model"]["img_token_dim"],
            state_token_dim=base_config["model"]["state_token_dim"],
            max_lang_cond_len=base_config["dataset"]["tokenizer_max_length"],
            img_cond_len=img_cond_len,
            # FLAREå‚æ•°
            num_future_tokens=model_config.get('num_future_tokens', 32),
            activation_layer=model_config.get('activation_layer', 6),
            alignment_loss_weight=model_config.get('alignment_loss_weight', 0.1),
            enable_flare=model_config.get('enable_flare', True),
            dtype=torch.bfloat16,
        )
        
        print(f"âœ… FLAREæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   - enable_flare: {rdt.enable_flare}")
        print(f"   - num_future_tokens: {rdt.num_future_tokens}")
        print(f"   - activation_layer: {rdt.activation_layer}")
        print(f"   - alignment_loss_weight: {rdt.alignment_loss_weight}")
        
        # æ£€æŸ¥FLAREç»„ä»¶
        print(f"   - VL token generator: {hasattr(rdt.model, 'vl_token_generator')}")
        print(f"   - Target generator: {hasattr(rdt.model, 'target_generator')}")
        print(f"   - Future obs tokens: {hasattr(rdt.model, 'future_obs_tokens')}")
        
        return rdt
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def debug_loss_computation(rdt, batch):
    """æµ‹è¯•æŸå¤±è®¡ç®—è¿‡ç¨‹"""
    print("\nğŸ” æµ‹è¯•æŸå¤±è®¡ç®—è¿‡ç¨‹...")
    
    try:
        # æ¨¡æ‹Ÿè®­ç»ƒå‚æ•°
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        weight_dtype = torch.bfloat16
        
        # å‡†å¤‡æ•°æ®
        lang_tokens = batch["lang_embeds"].to(dtype=weight_dtype)
        lang_attn_mask = batch["lang_attn_mask"]
        img_tokens = torch.randn(2, 1000, 1152).to(dtype=weight_dtype)  # æ¨¡æ‹Ÿå›¾åƒtoken
        state_tokens = batch["states"].to(dtype=weight_dtype)[:, -1:, :]
        action_gt = batch["actions"].to(dtype=weight_dtype)
        state_elem_mask = batch["state_elem_mask"].to(dtype=weight_dtype).unsqueeze(1)
        ctrl_freqs = batch["ctrl_freqs"]
        
        # FLAREæ•°æ®
        future_obs_images = batch.get("future_obs_images")
        text_instructions = batch.get("text_instructions", [""] * img_tokens.shape[0])
        has_future_obs = batch.get("has_future_obs")
        
        print(f"ğŸ“Š æŸå¤±è®¡ç®—è¾“å…¥æ£€æŸ¥:")
        print(f"   - future_obs_images: {future_obs_images is not None}")
        print(f"   - text_instructions: {len(text_instructions)} items")
        print(f"   - has_future_obs: {has_future_obs}")
        
        if future_obs_images is not None:
            print(f"   - future_obs_images shape: {future_obs_images.shape}")
            
            # æ¨¡æ‹Ÿè§†è§‰ç¼–ç 
            future_vision_embeds = torch.randn(2, 729, 1152).to(dtype=weight_dtype)
            print(f"   - future_vision_embeds shape: {future_vision_embeds.shape}")
        else:
            future_vision_embeds = None
        
        # å°è¯•è®¡ç®—æŸå¤±
        print(f"\nğŸ¯ è®¡ç®—FLAREæŸå¤±...")
        total_loss, loss_dict = rdt.compute_loss_with_flare(
            lang_tokens=lang_tokens,
            lang_attn_mask=lang_attn_mask,
            img_tokens=img_tokens,
            state_tokens=state_tokens,
            action_gt=action_gt,
            action_mask=state_elem_mask,
            ctrl_freqs=ctrl_freqs,
            future_vision_tokens=future_vision_embeds,
            text_instructions=text_instructions,
            has_future_obs=has_future_obs,
        )
        
        print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸ!")
        print(f"   - total_loss: {total_loss:.6f}")
        print(f"   - diffusion_loss: {loss_dict.get('diffusion_loss', 'N/A'):.6f}")
        print(f"   - alignment_loss: {loss_dict.get('alignment_loss', 'N/A'):.6f}")
        print(f"   - used_flare: {loss_dict.get('used_flare', 'N/A')}")
        print(f"   - alignment_loss_weight: {loss_dict.get('alignment_loss_weight', 'N/A')}")
        
        # è¯Šæ–­alignment_lossä¸º0çš„åŸå› 
        if loss_dict.get('alignment_loss', 0) == 0:
            print(f"\nâš ï¸  ALIGNMENT LOSSä¸º0çš„å¯èƒ½åŸå› :")
            if not loss_dict.get('used_flare', False):
                print(f"   - FLAREæœªè¢«ä½¿ç”¨ (used_flare=False)")
                print(f"   - æ£€æŸ¥: enable_flare, future_vision_tokens, text_instructions")
            else:
                print(f"   - FLAREè¢«ä½¿ç”¨ä½†alignment_loss=0")
                print(f"   - å¯èƒ½åŸå› : æ¿€æ´»æå–å¤±è´¥ã€ç›®æ ‡ç”Ÿæˆå¤±è´¥ã€å¯¹é½è®¡ç®—é”™è¯¯")
        
        return total_loss, loss_dict
        
    except Exception as e:
        print(f"âŒ æŸå¤±è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def main():
    """ä¸»è¯Šæ–­æµç¨‹"""
    print("ğŸš€ FLAREè®­ç»ƒè¯Šæ–­å¼€å§‹...")
    print("=" * 60)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶ï¼ˆä¿®æ”¹ä¸ºä½ çš„é…ç½®æ–‡ä»¶è·¯å¾„ï¼‰
    model_config_path = input("è¯·è¾“å…¥ä½ çš„æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ (ä¾‹: model_config/your_task_flare.yml): ").strip()
    if not model_config_path:
        model_config_path = "model_config/example_flare.yml"
    
    if not os.path.exists(model_config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {model_config_path}")
        return
    
    # Step 1: æ£€æŸ¥é…ç½®
    base_config, model_config = debug_flare_data_loading(model_config_path)
    if base_config is None or model_config is None:
        return
    
    # Step 2: æ£€æŸ¥æ•°æ®é›†
    dataset = debug_dataset_creation(base_config, model_config_path)
    if dataset is None:
        return
    
    # Step 3: æµ‹è¯•å•ä¸ªæ ·æœ¬
    sample = debug_single_sample(dataset)
    if sample is None:
        return
    
    # Step 4: æµ‹è¯•æ‰¹æ¬¡å¤„ç†
    batch = debug_dataloader(dataset)
    if batch is None:
        return
    
    # Step 5: æ£€æŸ¥æ¨¡å‹
    rdt = debug_model_creation(base_config, model_config)
    if rdt is None:
        return
    
    # Step 6: æµ‹è¯•æŸå¤±è®¡ç®—
    total_loss, loss_dict = debug_loss_computation(rdt, batch)
    
    print("\n" + "=" * 60)
    print("ğŸŠ è¯Šæ–­å®Œæˆ!")
    
    if loss_dict and loss_dict.get('alignment_loss', 0) > 0:
        print("âœ… FLAREè®­ç»ƒé…ç½®æ­£å¸¸ï¼Œalignment_loss > 0")
    else:
        print("âŒ å‘ç°é—®é¢˜ï¼Œalignment_loss = 0")
        print("\nğŸ”§ å»ºè®®æ£€æŸ¥:")
        print("   1. ç¡®ä¿enable_flare=True")
        print("   2. æ£€æŸ¥æ•°æ®é›†ä¸­future_obs_probè®¾ç½®")
        print("   3. éªŒè¯HDF5æ•°æ®é›†æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„æ—¶é—´æ­¥")
        print("   4. ç¡®è®¤æ¨¡å‹é…ç½®ä¸­FLAREå‚æ•°è®¾ç½®æ­£ç¡®")

if __name__ == "__main__":
    main()