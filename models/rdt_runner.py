import re
import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler

from models.hub_mixin import CompatiblePyTorchModelHubMixin
from models.rdt.model import RDTWithFLARE


class RDTRunnerWithFLARE(nn.Module, CompatiblePyTorchModelHubMixin):
    """
    å®Œæ•´é›†æˆçš„FLAREå¢å¼ºRDT Runner
    """

    def __init__(self,
                 *,
                 action_dim,
                 pred_horizon,
                 config,
                 lang_token_dim,
                 img_token_dim,
                 state_token_dim,
                 max_lang_cond_len=1024,
                 img_cond_len,
                 lang_pos_embed_config=None,
                 img_pos_embed_config=None,
                 dtype=torch.bfloat16,
                 # FLAREå‚æ•°
                 num_future_tokens=32,
                 activation_layer=21,
                 alignment_loss_weight=0.2,
                 num_vl_fusion_layers=4,
                 num_qformer_layers=2,
                 alignment_temperature=0.07,
                # ğŸ”§ åŒºåˆ†ä¸¤ä¸ªç¼–ç å™¨çš„å‚æ•°
                 future_vision_model_name=None,  # æœªæ¥è§‚æµ‹ç¼–ç å™¨ï¼ˆSigLIP2-256ï¼‰
                 future_text_model_name=None,    # æœªæ¥è§‚æµ‹æ–‡æœ¬ç¼–ç å™¨
                 current_vision_image_size=384,  # å½“å‰å›¾åƒå°ºå¯¸
                 future_vision_image_size=256,   # æœªæ¥å›¾åƒå°ºå¯¸
                 enable_flare=True):
        super().__init__()
        # ğŸ”§ è·¯å¾„å’Œé…ç½®å¤„ç†
        self.future_vision_path = future_vision_model_name or "/home/deng_xiang/qian_daichao/RoboTwin/policy/RDT_flare/siglip2-large-patch16-256"
        self.future_text_path = future_text_model_name or self.future_vision_path
        self.current_vision_image_size = current_vision_image_size
        self.future_vision_image_size = future_vision_image_size
        self.max_lang_cond_len = 1024
        print(f"ğŸ”§ RDTRunnerWithFLARE åŒç¼–ç å™¨é…ç½®:")
        print(f"   æœªæ¥è§‚æµ‹è§†è§‰æ¨¡å‹: {self.future_vision_path}")
        print(f"   æœªæ¥è§‚æµ‹æ–‡æœ¬æ¨¡å‹: {self.future_text_path}")
        print(f"   å½“å‰å›¾åƒå°ºå¯¸: {self.current_vision_image_size}")
        print(f"   æœªæ¥å›¾åƒå°ºå¯¸: {self.future_vision_image_size}")
        print(f"   æ–‡æœ¬æœ€å¤§é•¿åº¦: {max_lang_cond_len}")
        print(f"   FLARE enabled: {enable_flare}")
        self.alignment_loss_weight = alignment_loss_weight
        self.enable_flare = enable_flare
        self.num_future_tokens = num_future_tokens
        self.activation_layer = activation_layer
        
        # åˆ›å»ºFLAREå¢å¼ºçš„æ‰©æ•£æ¨¡å‹
        hidden_size = config['rdt']['hidden_size']
        self.model = RDTWithFLARE(
            output_dim=action_dim,
            horizon=pred_horizon,
            hidden_size=hidden_size,
            depth=config['rdt']['depth'],
            num_heads=config['rdt']['num_heads'],
            max_lang_cond_len=max_lang_cond_len,
            img_cond_len=img_cond_len,
            lang_pos_embed_config=lang_pos_embed_config,
            img_pos_embed_config=img_pos_embed_config,
            dtype=dtype,
            num_future_tokens=num_future_tokens,
            activation_layer=activation_layer,
            num_vl_fusion_layers=num_vl_fusion_layers,
            num_qformer_layers=num_qformer_layers,
            alignment_temperature=alignment_temperature,
            # ğŸ”§ åªä¼ é€’æœªæ¥è§‚æµ‹ç›¸å…³çš„è·¯å¾„
            future_vision_model_name=self.future_vision_path,
            future_text_model_name=self.future_text_path,
            future_vision_image_size=self.future_vision_image_size,
        )

        # åˆ›å»ºæ¡ä»¶é€‚é…å™¨
        self.lang_adaptor = self.build_condition_adapter(config['lang_adaptor'],
                                                         in_features=lang_token_dim,
                                                         out_features=hidden_size).to(dtype)
        self.img_adaptor = self.build_condition_adapter(config['img_adaptor'],
                                                        in_features=img_token_dim,
                                                        out_features=hidden_size).to(dtype)
        self.state_adaptor = self.build_condition_adapter(
            config['state_adaptor'],
            in_features=state_token_dim * 2,
            out_features=hidden_size).to(dtype)

        # FLARE: æœªæ¥è§‚æµ‹è§†è§‰tokené€‚é…å™¨
        self.future_vision_adaptor = self.build_condition_adapter(
            config.get('future_vision_adaptor', 'linear'),
            in_features=img_token_dim,
            out_features=hidden_size).to(dtype)
        self._ensure_bf16_consistency()

        # å™ªå£°è°ƒåº¦å™¨
        noise_scheduler_config = config['noise_scheduler']
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
            clip_sample=noise_scheduler_config['clip_sample'],
        )
        self.noise_scheduler_sample = DPMSolverMultistepScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
        )

        self.num_train_timesteps = noise_scheduler_config['num_train_timesteps']
        self.num_inference_timesteps = noise_scheduler_config['num_inference_timesteps']
        self.prediction_type = noise_scheduler_config['prediction_type']

        self.pred_horizon = pred_horizon
        self.action_dim = action_dim

        print("FLARE Diffusion params: %e" %
              sum([p.numel() for p in self.model.parameters()] + 
                  [p.numel() for p in self.lang_adaptor.parameters()] +
                  [p.numel() for p in self.img_adaptor.parameters()] + 
                  [p.numel() for p in self.state_adaptor.parameters()] +
                  [p.numel() for p in self.future_vision_adaptor.parameters()]))
    def _ensure_bf16_consistency(self):
        """ç¡®ä¿æ‰€æœ‰ç»„ä»¶ä½¿ç”¨BF16"""
        target_dtype = torch.bfloat16
        
        self.model = self.model.to(target_dtype)
        self.lang_adaptor = self.lang_adaptor.to(target_dtype)
        self.img_adaptor = self.img_adaptor.to(target_dtype)
        self.state_adaptor = self.state_adaptor.to(target_dtype)
        self.future_vision_adaptor = self.future_vision_adaptor.to(target_dtype)
        
        print(f"âœ… RDT Runnerç»Ÿä¸€ä½¿ç”¨: {target_dtype}")
        
    def build_condition_adapter(self, projector_type, in_features, out_features):
        """æ„å»ºæ¡ä»¶é€‚é…å™¨"""
        projector = None
        if projector_type == 'linear':
            projector = nn.Linear(in_features, out_features)
        else:
            mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu$', projector_type)
            if mlp_gelu_match:
                mlp_depth = int(mlp_gelu_match.group(1))
                modules = [nn.Linear(in_features, out_features)]
                for _ in range(1, mlp_depth):
                    modules.append(nn.GELU(approximate="tanh"))
                    modules.append(nn.Linear(out_features, out_features))
                projector = nn.Sequential(*modules)

        if projector is None:
            raise ValueError(f'Unknown projector type: {projector_type}')

        return projector

    
    def adapt_conditions(self, lang_tokens, img_tokens, state_action_traj, future_vision_tokens=None):
        # è·å–æ¨¡å‹æœŸæœ›çš„æ•°æ®ç±»å‹
        target_dtype = next(self.lang_adaptor.parameters()).dtype
        
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡ä¸æ¨¡å‹å‚æ•°ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
        if lang_tokens is not None:
            lang_tokens = lang_tokens.to(dtype=target_dtype)
        if img_tokens is not None:
            img_tokens = img_tokens.to(dtype=target_dtype)
        if state_action_traj is not None:
            state_action_traj = state_action_traj.to(dtype=target_dtype)
        if future_vision_tokens is not None:
            future_vision_tokens = future_vision_tokens.to(dtype=target_dtype)
        
        # åœ¨autocastèŒƒå›´å†…æ‰§è¡Œé€‚é…
        with torch.autocast(device_type='cuda', dtype=target_dtype, 
                        enabled=(target_dtype in [torch.float16, torch.bfloat16])):
            adapted_lang = self.lang_adaptor(lang_tokens) if lang_tokens is not None else None
            adapted_img = self.img_adaptor(img_tokens) if img_tokens is not None else None
            adapted_state = self.state_adaptor(state_action_traj) if state_action_traj is not None else None
            adapted_future_vision = self.future_vision_adaptor(future_vision_tokens) if future_vision_tokens is not None else None
        
        return adapted_lang, adapted_img, adapted_state, adapted_future_vision

    def conditional_sample(self, lang_cond, lang_attn_mask, img_cond, state_traj, action_mask, ctrl_freqs):
        """æ¡ä»¶é‡‡æ ·ï¼ˆæ¨ç†æ—¶ä¸éœ€è¦æœªæ¥è§‚æµ‹ï¼‰"""
        device = state_traj.device
        dtype = state_traj.dtype
        noisy_action = torch.randn(size=(state_traj.shape[0], self.pred_horizon, self.action_dim),
                                   dtype=dtype,
                                   device=device)
        action_mask = action_mask.expand(-1, self.pred_horizon, -1)

        # è®¾ç½®é‡‡æ ·æ­¥æ•°
        self.noise_scheduler_sample.set_timesteps(self.num_inference_timesteps)

        for t in self.noise_scheduler_sample.timesteps:
            # å‡†å¤‡çŠ¶æ€-åŠ¨ä½œè½¨è¿¹
            action_traj = torch.cat([noisy_action, action_mask], dim=2)
            action_traj = self.state_adaptor(action_traj)
            state_action_traj = torch.cat([state_traj, action_traj], dim=1)

            # æ¨¡å‹é¢„æµ‹ï¼ˆæ¨ç†æ—¶ä¸ä½¿ç”¨æœªæ¥è§‚æµ‹ï¼‰
            model_output = self.model(state_action_traj,
                                      ctrl_freqs,
                                      t.unsqueeze(-1).to(device),
                                      lang_cond,
                                      img_cond,
                                      lang_mask=lang_attn_mask,
                                      future_vision_tokens=None,
                                      text_instructions=None,
                                      return_alignment_loss=False)

            # è®¡ç®—å‰ä¸€æ­¥åŠ¨ä½œ: x_t -> x_t-1
            noisy_action = self.noise_scheduler_sample.step(model_output, t, noisy_action).prev_sample
            noisy_action = noisy_action.to(state_traj.dtype)

        # åº”ç”¨åŠ¨ä½œæ©ç 
        noisy_action = noisy_action * action_mask

        return noisy_action

    def compute_loss_with_flare(self, lang_tokens, lang_attn_mask, img_tokens, state_tokens, 
                            action_gt, action_mask, ctrl_freqs, future_vision_tokens=None, 
                            text_instructions=None, has_future_obs=None,
                            future_obs_images=None):
        """
        è®¡ç®—FLAREå¢å¼ºçš„æŸå¤± - ä¼˜åŒ–ç‰ˆæœ¬
        """
        # ğŸ”§ ç»Ÿä¸€è®¾å¤‡å’Œæ•°æ®ç±»å‹å¤„ç†
        device = next(self.model.parameters()).device  # è·å–æ¨¡å‹è®¾å¤‡
        target_dtype = torch.bfloat16  # æ˜ç¡®ä½¿ç”¨BF16
        batch_size = lang_tokens.shape[0]
        
        # ğŸ”§ ç»Ÿä¸€å°†æ‰€æœ‰è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡å’Œæ•°æ®ç±»å‹
        def to_device_dtype(tensor, device, dtype):
            """ç»Ÿä¸€çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹è½¬æ¢"""
            if tensor is not None:
                return tensor.to(dtype=dtype, device=device)
            return tensor
        
        # è½¬æ¢æ‰€æœ‰å¼ é‡è¾“å…¥
        lang_tokens = to_device_dtype(lang_tokens, device, target_dtype)
        img_tokens = to_device_dtype(img_tokens, device, target_dtype)
        state_tokens = to_device_dtype(state_tokens, device, target_dtype)
        action_gt = to_device_dtype(action_gt, device, target_dtype)
        action_mask = to_device_dtype(action_mask, device, target_dtype)
        future_vision_tokens = to_device_dtype(future_vision_tokens, device, target_dtype)
        future_obs_images = to_device_dtype(future_obs_images, device, target_dtype)
        
        # å¤„ç†text_instructionsï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å¼ é‡ï¼‰
        if text_instructions is not None:
            if isinstance(text_instructions, torch.Tensor):
                text_instructions = text_instructions.to(device)
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¿æŒä¸å˜
            
        
        # ğŸ”§ ç¡®ä¿ctrl_freqsä¹Ÿåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if isinstance(ctrl_freqs, torch.Tensor):
            ctrl_freqs = ctrl_freqs.to(dtype=target_dtype, device=device)
        
        # åœ¨autocastèŒƒå›´å†…è¿›è¡Œè®¡ç®—
        with torch.autocast(device_type='cuda', dtype=target_dtype):
            # é‡‡æ ·å™ªå£°å’Œæ—¶é—´æ­¥
            noise = torch.randn(action_gt.shape, dtype=target_dtype, device=device)
            timesteps = torch.randint(0, self.num_train_timesteps, (batch_size,), device=device).long()
            noisy_action = self.noise_scheduler.add_noise(action_gt, noise, timesteps)

            # æ‹¼æ¥çŠ¶æ€å’ŒåŠ¨ä½œtoken
            state_action_traj = torch.cat([state_tokens, noisy_action], dim=1)
            action_mask_expanded = action_mask.expand(-1, state_action_traj.shape[1], -1)
            state_action_traj = torch.cat([state_action_traj, action_mask_expanded], dim=2)
            
            # é€‚é…æ¡ä»¶
            adapted_results = self.adapt_conditions(lang_tokens, img_tokens, state_action_traj, future_vision_tokens)
            lang_cond, img_cond, state_action_traj = adapted_results[:3]
            adapted_future_vision = adapted_results[3] if len(adapted_results) > 3 else None
            
            # ğŸ”§ æ”¹è¿›çš„FLAREä½¿ç”¨åˆ¤æ–­é€»è¾‘
            use_flare = (
                self.enable_flare and 
                future_obs_images is not None and  # ä½¿ç”¨åŸå§‹å›¾åƒè€Œä¸æ˜¯vision tokens
                text_instructions is not None
            )
            
            # è¿›ä¸€æ­¥æ£€æŸ¥has_future_obs
            if use_flare and has_future_obs is not None:
                valid_indices = has_future_obs.bool()
                if valid_indices.sum() == 0:
                    use_flare = False
            
            # ğŸ”§ æ¨¡å‹å‰å‘ä¼ æ’­ - ç®€åŒ–é€»è¾‘
            try:
                if use_flare:
                    # ä½¿ç”¨FLAREå¢å¼ºçš„æ¨¡å‹
                    pred, alignment_loss = self.model(
                        state_action_traj, 
                        ctrl_freqs, 
                        timesteps, 
                        lang_cond, 
                        img_cond, 
                        lang_mask=lang_attn_mask, 
                        img_mask=None,
                        future_vision_tokens=adapted_future_vision,
                        text_instructions=text_instructions, 
                        future_obs_image=future_obs_images,
                        return_alignment_loss=True
                    )
                else:
                    # æ ‡å‡†æ‰©æ•£æ¨¡å‹
                    pred = self.model(
                        state_action_traj, 
                        ctrl_freqs, 
                        timesteps, 
                        lang_cond, 
                        img_cond,
                        lang_mask=lang_attn_mask,
                        img_mask=None,
                        future_vision_tokens=None,
                        text_instructions=None,
                        future_obs_image=None,  # ğŸ”§ æ ‡å‡†æ¨¡å¼ä¸ä½¿ç”¨æœªæ¥è§‚æµ‹
                        return_alignment_loss=False
                    )
                    alignment_loss = torch.tensor(0.0, device=device, dtype=target_dtype)
                    
            except Exception as e:
                print(f"âŒ æ¨¡å‹å‰å‘ä¼ æ’­å¤±è´¥: {e}")
                print(f"   use_flare: {use_flare}")
                print(f"   future_obs_images: {future_obs_images.shape if future_obs_images is not None else None}")
                print(f"   text_instructions: {type(text_instructions)}")
                raise e

            # ğŸ”§ å¯¹é½æŸå¤±çš„å¤„ç†
            if use_flare and alignment_loss is not None and has_future_obs is not None:
                valid_count = has_future_obs.sum().float()
                if valid_count > 0 and valid_count < batch_size:
                    # åªå¯¹æœ‰æ•ˆæ ·æœ¬è¿›è¡Œå½’ä¸€åŒ–
                    alignment_loss = alignment_loss * (batch_size / valid_count)
                elif valid_count == 0:
                    alignment_loss = torch.tensor(0.0, device=device, dtype=target_dtype)

            # è®¡ç®—ç›®æ ‡
            if self.prediction_type == 'epsilon':
                target = noise
            elif self.prediction_type == 'sample':
                target = action_gt
            else:
                raise ValueError(f"Unsupported prediction type {self.prediction_type}")
                
            # æ‰©æ•£æŸå¤±
            diffusion_loss = F.mse_loss(pred, target)
            
            # æ€»æŸå¤±
            total_loss = diffusion_loss
            if use_flare and alignment_loss is not None:
                total_loss = total_loss + self.alignment_loss_weight * alignment_loss
                
        # æ„å»ºæŸå¤±å­—å…¸
        loss_dict = {
            'diffusion_loss': diffusion_loss.item(),
            'alignment_loss': alignment_loss.item() if alignment_loss is not None else 0.0,
            'total_loss': total_loss.item(),
            'alignment_loss_weight': self.alignment_loss_weight,
            'used_flare': use_flare,
            'batch_size': batch_size,
            'valid_future_obs': has_future_obs.sum().item() if has_future_obs is not None else 0,
        }
        
        return total_loss, loss_dict

    def compute_loss(self, lang_tokens, lang_attn_mask, img_tokens, state_tokens, action_gt, action_mask,
                     ctrl_freqs, future_vision_tokens=None, text_instructions=None, has_future_obs=None):
        """
        å…¼å®¹æ€§æ¥å£ï¼šè®¡ç®—æŸå¤±
        """
        total_loss, loss_dict = self.compute_loss_with_flare(
            lang_tokens, lang_attn_mask, img_tokens, state_tokens, action_gt, action_mask,
            ctrl_freqs, future_vision_tokens, text_instructions, has_future_obs
        )
        return total_loss

    def predict_action(self, lang_tokens, lang_attn_mask, img_tokens, state_tokens, action_mask, ctrl_freqs):
        """é¢„æµ‹åŠ¨ä½œï¼ˆæ¨ç†æ—¶ä¸ä½¿ç”¨æœªæ¥è§‚æµ‹ï¼‰"""
        # å‡†å¤‡çŠ¶æ€å’Œæ¡ä»¶
        state_tokens = torch.cat([state_tokens, action_mask], dim=2)
        lang_cond, img_cond, state_traj, _ = self.adapt_conditions(lang_tokens, img_tokens, state_tokens)

        # è¿è¡Œé‡‡æ ·
        action_pred = self.conditional_sample(
            lang_cond,
            lang_attn_mask,
            img_cond,
            state_traj,
            action_mask,
            ctrl_freqs,
        )

        return action_pred

    def get_alignment_metrics(self):
        """è·å–å¯¹é½ç›¸å…³çš„æŒ‡æ ‡"""
        if hasattr(self.model, 'activation_aligner') and self.model.activation_aligner is not None:
            return self.model.activation_aligner.get_alignment_metrics()
        return {}

    def set_alignment_loss_weight(self, weight):
        """åŠ¨æ€è®¾ç½®å¯¹é½æŸå¤±æƒé‡"""
        self.alignment_loss_weight = weight

    def enable_flare_mode(self, enable=True):
        """å¯ç”¨/ç¦ç”¨FLAREæ¨¡å¼"""
        self.enable_flare = enable

    def forward(self, *args, **kwargs) -> torch.Tensor:
        """å‰å‘ä¼ æ’­æ¥å£"""
        return self.compute_loss(*args, **kwargs)