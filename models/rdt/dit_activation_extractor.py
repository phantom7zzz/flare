# models/rdt/dit_activation_extractor.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple
from collections import defaultdict


class ActivationHook:
    """æ¿€æ´»æå–é’©å­ç±»"""
    
    def __init__(self, name: str):
        self.name = name
        self.activations = {}
        self.gradients = {}
        
    def forward_hook(self, module, input, output):
        """å‰å‘ä¼ æ’­é’©å­"""
        if isinstance(output, tuple):
            # å¦‚æœè¾“å‡ºæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
            self.activations[self.name] = output[0].detach()
        else:
            self.activations[self.name] = output.detach()
    
    def backward_hook(self, module, grad_input, grad_output):
        """åå‘ä¼ æ’­é’©å­"""
        if isinstance(grad_output, tuple) and grad_output[0] is not None:
            self.gradients[self.name] = grad_output[0].detach()
        elif grad_output is not None:
            self.gradients[self.name] = grad_output.detach()
    
    def get_activation(self):
        """è·å–æœ€æ–°çš„æ¿€æ´»"""
        return self.activations.get(self.name, None)
    
    def get_gradient(self):
        """è·å–æœ€æ–°çš„æ¢¯åº¦"""
        return self.gradients.get(self.name, None)
    
    def clear(self):
        """æ¸…ç©ºå­˜å‚¨çš„æ¿€æ´»å’Œæ¢¯åº¦"""
        self.activations.clear()
        self.gradients.clear()


class DiTActivationExtractor:
    """
    DiTå±‚æ¿€æ´»æå–å™¨
    
    åŠŸèƒ½ï¼š
    1. åœ¨æŒ‡å®šDiTå±‚æ³¨å†Œé’©å­
    2. ç²¾ç¡®æå–æœªæ¥é¢„æµ‹tokençš„æ¿€æ´»
    3. æ”¯æŒå¤šå±‚æ¿€æ´»æå–å’Œæ¯”è¾ƒ
    4. æä¾›æ¿€æ´»å¯è§†åŒ–å’Œåˆ†æå·¥å…·
    """
    
    def __init__(self, 
                 model: nn.Module,
                 target_layers: List[int] = [21],
                 num_future_tokens: int = 32,
                 token_start_offset: int = 3,  # timestep + freq + state tokens
                 enable_gradient_hooks: bool = False):
        """
        åˆå§‹åŒ–æ¿€æ´»æå–å™¨
        
        Args:
            model: RDTWithFLAREæ¨¡å‹
            target_layers: ç›®æ ‡æå–å±‚åˆ—è¡¨ï¼Œé»˜è®¤[6]
            num_future_tokens: æœªæ¥é¢„æµ‹tokenæ•°é‡
            token_start_offset: çŠ¶æ€ã€åŠ¨ä½œtokenåçš„åç§»é‡
            enable_gradient_hooks: æ˜¯å¦å¯ç”¨æ¢¯åº¦é’©å­
        """
        self.model = model
        self.target_layers = target_layers
        self.num_future_tokens = num_future_tokens
        self.token_start_offset = token_start_offset
        self.enable_gradient_hooks_flag = enable_gradient_hooks
        
        if hasattr(model, 'indices'):
            self.sequence_indices = model.indices
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œä½¿ç”¨é»˜è®¤è®¡ç®—
            self.sequence_indices = self._compute_default_indices(model)
        
        # å­˜å‚¨é’©å­å’Œæ¿€æ´»
        self.hooks = {}
        self.hook_handles = []
        self.extracted_activations = {}
        self.layer_outputs = {}
        self._gradient_hooks_enabled = False
        
        # æ³¨å†Œé’©å­
        self._register_hooks()
        
    def _compute_default_indices(self, model):
        """ä¸ºæ—§ç‰ˆæœ¬æ¨¡å‹è®¡ç®—é»˜è®¤ç´¢å¼•"""
        horizon = getattr(model, 'horizon', 32)
        num_future_tokens = getattr(model, 'num_future_tokens', 32)
        
        indices = {
            'timestep': (0, 1),
            'freq': (1, 2),
            'state': (2, 3),
            'action': (3, 3 + horizon),
            'future_obs': (3 + horizon, 3 + horizon + num_future_tokens)
        }
        return indices
    
    def _register_hooks(self):
        """æ³¨å†Œé’©å­åˆ°æŒ‡å®šçš„DiTå±‚"""
        
        # è·å–DiT blocks
        if hasattr(self.model, 'blocks'):
            dit_blocks = self.model.blocks
        elif hasattr(self.model, 'model') and hasattr(self.model.model, 'blocks'):
            dit_blocks = self.model.model.blocks
        else:
            raise AttributeError("Cannot find DiT blocks in the model")
        
        for layer_idx in self.target_layers:
            if layer_idx >= len(dit_blocks):
                print(f"Warning: Layer {layer_idx} does not exist. Model has {len(dit_blocks)} layers.")
                continue
                
            layer_name = f"dit_layer_{layer_idx}"
            hook = ActivationHook(layer_name)
            self.hooks[layer_name] = hook
            
            # æ³¨å†Œå‰å‘é’©å­
            handle = dit_blocks[layer_idx].register_forward_hook(hook.forward_hook)
            self.hook_handles.append(handle)
            
            # å¦‚æœåˆå§‹åŒ–æ—¶è¦æ±‚å¯ç”¨æ¢¯åº¦é’©å­
            if self.enable_gradient_hooks_flag:
                handle = dit_blocks[layer_idx].register_full_backward_hook(hook.backward_hook)
                self.hook_handles.append(handle)
                
        print(f"Registered hooks for layers: {self.target_layers}")
        if self.enable_gradient_hooks_flag:
            self._gradient_hooks_enabled = True
    
    def enable_gradient_hooks(self):
        """å¯ç”¨æ¢¯åº¦é’©å­"""
        if self._gradient_hooks_enabled:
            return
            
        try:
            # è·å–DiT blocks
            if hasattr(self.model, 'blocks'):
                dit_blocks = self.model.blocks
            elif hasattr(self.model, 'model') and hasattr(self.model.model, 'blocks'):
                dit_blocks = self.model.model.blocks
            else:
                print("Warning: Cannot find DiT blocks for gradient hooks")
                return
            
            for layer_idx in self.target_layers:
                if layer_idx >= len(dit_blocks):
                    continue
                    
                layer_name = f"dit_layer_{layer_idx}"
                if layer_name in self.hooks:
                    # ä¸ºå·²å­˜åœ¨çš„é’©å­æ·»åŠ åå‘é’©å­
                    hook = self.hooks[layer_name]
                    handle = dit_blocks[layer_idx].register_full_backward_hook(hook.backward_hook)
                    self.hook_handles.append(handle)
            
            self._gradient_hooks_enabled = True
            print(f"âœ… å·²ä¸ºå±‚ {self.target_layers} å¯ç”¨æ¢¯åº¦é’©å­")
            
        except Exception as e:
            print(f"âŒ å¯ç”¨æ¢¯åº¦é’©å­å¤±è´¥: {e}")

    def disable_gradient_hooks(self):
        """ç¦ç”¨æ¢¯åº¦é’©å­"""
        # ç§»é™¤åå‘é’©å­ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ›´ç²¾ç¡®åœ°åªç§»é™¤åå‘é’©å­ï¼‰
        if hasattr(self, '_gradient_hooks_enabled'):
            self._gradient_hooks_enabled = False
        print("æ¢¯åº¦é’©å­å·²ç¦ç”¨")
    
    def extract_future_token_activations(self, layer_idx=6, **kwargs):
        """
        ä½¿ç”¨æ­£ç¡®çš„åºåˆ—ç´¢å¼•æå–æœªæ¥tokenæ¿€æ´»
        """
        layer_name = f"dit_layer_{layer_idx}"
        
        if layer_name not in self.hooks:
            print(f"Warning: Layer {layer_idx} hook not found")
            return None
            
        activation = self.hooks[layer_name].get_activation()
        
        if activation is None:
            print(f"Warning: No activation found for layer {layer_idx}")
            return None
        
        # ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•èŒƒå›´
        if 'future_obs' in self.sequence_indices:
            future_start, future_end = self.sequence_indices['future_obs']
        else:
            # å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨ä¼ ç»Ÿè®¡ç®—
            future_start = self.token_start_offset + kwargs.get('horizon', 32)
            future_end = future_start + self.num_future_tokens
        
        # è¾¹ç•Œæ£€æŸ¥
        if activation.shape[1] < future_end:
            print(f"Warning: Activation length {activation.shape[1]} < required {future_end}")
            print(f"Sequence indices: {self.sequence_indices}")
            return None
        
        # æå–æœªæ¥tokenæ¿€æ´»
        future_activations = activation[:, future_start:future_end, :]
        
        # éªŒè¯å½¢çŠ¶
        expected_shape = (activation.shape[0], self.num_future_tokens, activation.shape[2])
        if future_activations.shape != expected_shape:
            print(f"Warning: Future activation shape {future_activations.shape} != expected {expected_shape}")
            return None
        
        # å­˜å‚¨æå–çš„æ¿€æ´»
        self.extracted_activations[f"layer_{layer_idx}_future"] = future_activations.detach()
        
        return future_activations
    
    def extract_activations_at_step(self, 
                                  step_idx: int,
                                  layer_indices: List[int] = None) -> Dict[str, torch.Tensor]:
        """
        åœ¨æŒ‡å®šæ­¥éª¤æå–å¤šå±‚æ¿€æ´»
        
        Args:
            step_idx: æ—¶é—´æ­¥ç´¢å¼•
            layer_indices: å±‚ç´¢å¼•åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨target_layers
            
        Returns:
            activations: å„å±‚æ¿€æ´»å­—å…¸
        """
        if layer_indices is None:
            layer_indices = self.target_layers
            
        activations = {}
        
        for layer_idx in layer_indices:
            layer_name = f"dit_layer_{layer_idx}"
            if layer_name in self.hooks:
                activation = self.hooks[layer_name].get_activation()
                if activation is not None:
                    activations[f"layer_{layer_idx}"] = activation.detach()
                    
        return activations
    
    def compute_activation_statistics(self, 
                                    layer_idx: int = 6) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            layer_idx: å±‚ç´¢å¼•
            
        Returns:
            statistics: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        future_activations = self.extract_future_token_activations(layer_idx)
        
        if future_activations is None:
            return {}
            
        stats = {
            'mean': future_activations.mean(dim=[0, 1]),
            'std': future_activations.std(dim=[0, 1]),
            'max': future_activations.max(dim=1)[0].max(dim=0)[0],
            'min': future_activations.min(dim=1)[0].min(dim=0)[0],
            'norm': torch.norm(future_activations, dim=-1).mean(),
        }
        
        return stats
    
    def get_activation_similarity(self, 
                                layer_idx1: int, 
                                layer_idx2: int) -> torch.Tensor:
        """
        è®¡ç®—ä¸åŒå±‚ä¹‹é—´çš„æ¿€æ´»ç›¸ä¼¼æ€§
        
        Args:
            layer_idx1, layer_idx2: ä¸¤ä¸ªå±‚çš„ç´¢å¼•
            
        Returns:
            similarity: ä½™å¼¦ç›¸ä¼¼æ€§çŸ©é˜µ
        """
        act1 = self.extract_future_token_activations(layer_idx1)
        act2 = self.extract_future_token_activations(layer_idx2)
        
        if act1 is None or act2 is None:
            return torch.tensor(0.0)
            
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§
        act1_flat = act1.reshape(-1, act1.shape[-1])
        act2_flat = act2.reshape(-1, act2.shape[-1])
        
        similarity = F.cosine_similarity(act1_flat, act2_flat, dim=-1).mean()
        
        return similarity
    
    def get_extracted_activations(self):
        """è·å–æå–çš„æ¿€æ´»"""
        return self.extracted_activations.copy()

    def get_layer_outputs(self):
        """è·å–å±‚è¾“å‡º"""
        return self.layer_outputs.copy()
    
    def clear_activations(self):
        """æ¸…ç©ºæ‰€æœ‰å­˜å‚¨çš„æ¿€æ´»"""
        for hook in self.hooks.values():
            hook.clear()
        self.extracted_activations.clear()
        self.layer_outputs.clear()
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰é’©å­"""
        for handle in self.hook_handles:
            try:
                handle.remove()
            except:
                pass
        self.hook_handles.clear()
        self.hooks.clear()
        self._gradient_hooks_enabled = False
        print("All hooks removed")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿é’©å­è¢«ç§»é™¤"""
        self.remove_hooks()


class FLAREActivationAligner:
    """
    FLAREæ¿€æ´»å¯¹é½å™¨
    
    åŠŸèƒ½ï¼š
    1. æ•´åˆæ¿€æ´»æå–å’Œç›®æ ‡tokenç”Ÿæˆ
    2. è®¡ç®—ç²¾ç¡®çš„å¯¹é½æŸå¤±
    3. æä¾›è®­ç»ƒæ—¶çš„æ¿€æ´»ç›‘æ§
    """
    
    def __init__(self, 
                 model: nn.Module,
                 target_layer: int = 21,
                 num_future_tokens: int = 32,
                 alignment_temperature: float = 0.07,
                 loss_type: str = "cosine_contrastive"):
        """
        åˆå§‹åŒ–å¯¹é½å™¨
        
        Args:
            model: FLAREæ¨¡å‹
            target_layer: ç›®æ ‡DiTå±‚
            num_future_tokens: æœªæ¥tokenæ•°é‡
            alignment_temperature: å¯¹æ¯”å­¦ä¹ æ¸©åº¦
            loss_type: æŸå¤±ç±»å‹ ("cosine_contrastive", "mse", "kl_div")
        """
        self.model = model
        self.target_layer = target_layer
        self.num_future_tokens = num_future_tokens
        self.alignment_temperature = alignment_temperature
        self.loss_type = loss_type
        
        # åˆ›å»ºæ¿€æ´»æå–å™¨
        self.activation_extractor = DiTActivationExtractor(
            model=model,
            target_layers=[target_layer],
            num_future_tokens=num_future_tokens
        )
        
        # æ¿€æ´»å†å²è®°å½•
        self.activation_history = []
        self.loss_history = []
        
    def compute_precise_alignment_loss(self, target_tokens, horizon=32, future_token_indices=None):
        # """
        # è®¡ç®—ç²¾ç¡®çš„å¯¹é½æŸå¤±ï¼Œæ”¯æŒè‡ªå®šä¹‰ç´¢å¼•
        # """
        # # ä½¿ç”¨ä¼ å…¥çš„ç´¢å¼•æˆ–é»˜è®¤è®¡ç®—
        # if future_token_indices is not None:
        #     # æ›´æ–°æ¿€æ´»æå–å™¨çš„ç´¢å¼•ä¿¡æ¯
        #     if hasattr(self.activation_extractor, 'sequence_indices'):
        #         self.activation_extractor.sequence_indices['future_obs'] = future_token_indices
        
        # # æå–DiTå±‚æ¿€æ´»
        # pred_tokens = self.activation_extractor.extract_future_token_activations(
        #     layer_idx=self.target_layer,
        #     horizon=horizon
        # )
        
        # if pred_tokens is None:
        #     return torch.tensor(0.0, device=target_tokens.device), {}
        
        # # å½¢çŠ¶æ£€æŸ¥å’Œä¿®æ­£
        # if pred_tokens.shape != target_tokens.shape:
        #     print(f"Shape mismatch: pred {pred_tokens.shape} vs target {target_tokens.shape}")
            
        #     # å°è¯•ä¿®æ­£å½¢çŠ¶ä¸åŒ¹é…
        #     if pred_tokens.shape[1] != target_tokens.shape[1]:
        #         # tokenæ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨æ’å€¼è°ƒæ•´
        #         pred_tokens = F.adaptive_avg_pool1d(
        #             pred_tokens.transpose(1, 2),
        #             target_tokens.shape[1]
        #         ).transpose(1, 2)
            
        #     if pred_tokens.shape[2] != target_tokens.shape[2]:
        #         # ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨çº¿æ€§æŠ•å½±
        #         if not hasattr(self, 'dim_adapter'):
        #             self.dim_adapter = nn.Linear(
        #                 pred_tokens.shape[2], 
        #                 target_tokens.shape[2]
        #             ).to(pred_tokens.device)
        #         pred_tokens = self.dim_adapter(pred_tokens)
        
        # # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        # if torch.isnan(pred_tokens).any() or torch.isnan(target_tokens).any():
        #     print("Warning: NaN detected in tokens")
        #     return torch.tensor(0.0, device=target_tokens.device), {}
        
        # # è®¡ç®—å¯¹é½æŸå¤±
        # if self.loss_type == "cosine_contrastive":
        #     loss = self._cosine_contrastive_loss(pred_tokens, target_tokens)
        # elif self.loss_type == "mse":
        #     loss = F.mse_loss(pred_tokens, target_tokens)
        # elif self.loss_type == "kl_div":
        #     loss = self._kl_divergence_loss(pred_tokens, target_tokens)
        # else:
        #     raise ValueError(f"Unknown loss type: {self.loss_type}")
        
        # # é¢å¤–ä¿¡æ¯
        # info = {
        #     'pred_norm': torch.norm(pred_tokens, dim=-1).mean().item(),
        #     'target_norm': torch.norm(target_tokens, dim=-1).mean().item(),
        #     'cosine_sim': F.cosine_similarity(
        #         pred_tokens.reshape(-1, pred_tokens.shape[-1]),
        #         target_tokens.reshape(-1, target_tokens.shape[-1]),
        #         dim=-1
        #     ).mean().item(),
        #     'pred_shape': list(pred_tokens.shape),
        #     'target_shape': list(target_tokens.shape)
        # }
        
        # return loss, info
        """
        è®¡ç®—å¯¹é½æŸå¤± - å¤„ç†tokenæ•°é‡ä¸åŒ¹é…
        
        Args:
            target_tokens: (B, 64, D) SigLIP2ç”Ÿæˆçš„ç›®æ ‡tokens
            horizon: DiTçš„horizon
            future_token_indices: æœªæ¥tokençš„ç´¢å¼•èŒƒå›´
            
        Returns:
            loss: å¯¹é½æŸå¤±
            info: é¢å¤–ä¿¡æ¯
        """
        # æå–DiTå±‚æ¿€æ´» (B, 32, D)
        pred_tokens = self.extract_future_token_activations(
            layer_idx=self.target_layer,
            horizon=horizon
        )
        
        if pred_tokens is None:
            return torch.tensor(0.0, device=target_tokens.device), {}
        
        print(f"ğŸ” å¯¹é½shapes: pred={pred_tokens.shape}, target={target_tokens.shape}")
        
        # å¤„ç†tokenæ•°é‡ä¸åŒ¹é…: 32 vs 64
        if pred_tokens.shape[1] != target_tokens.shape[1]:
            print(f"ğŸ”§ å¤„ç†tokenæ•°é‡ä¸åŒ¹é…: {pred_tokens.shape[1]} vs {target_tokens.shape[1]}")
            
            if pred_tokens.shape[1] < target_tokens.shape[1]:
                # DiT tokenså°‘ï¼Œéœ€è¦ä»SigLIP2 tokensä¸­é‡‡æ ·
                # æ–¹æ³•1: å¹³å‡æ± åŒ–é‡‡æ ·
                target_tokens = F.adaptive_avg_pool1d(
                    target_tokens.transpose(1, 2),  # (B, D, 64)
                    pred_tokens.shape[1]            # é‡‡æ ·åˆ°32
                ).transpose(1, 2)                   # (B, 32, D)
                print(f"   é‡‡æ ·ç›®æ ‡tokensåˆ°: {target_tokens.shape}")
                
            else:
                # SigLIP2 tokenså°‘ï¼Œæ‰©å±•DiT tokensï¼ˆä¸å¤ªå¯èƒ½ï¼‰
                pred_tokens = F.adaptive_avg_pool1d(
                    pred_tokens.transpose(1, 2),
                    target_tokens.shape[1]
                ).transpose(1, 2)
                print(f"   é‡‡æ ·é¢„æµ‹tokensåˆ°: {pred_tokens.shape}")
        
        # å¤„ç†ç‰¹å¾ç»´åº¦ä¸åŒ¹é…
        if pred_tokens.shape[2] != target_tokens.shape[2]:
            print(f"ğŸ”§ å¤„ç†ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: {pred_tokens.shape[2]} vs {target_tokens.shape[2]}")
            if not hasattr(self, 'dim_adapter'):
                self.dim_adapter = nn.Linear(
                    pred_tokens.shape[2], 
                    target_tokens.shape[2]
                ).to(pred_tokens.device)
            pred_tokens = self.dim_adapter(pred_tokens)
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(pred_tokens).any() or torch.isnan(target_tokens).any():
            print("âš ï¸ æ£€æµ‹åˆ°NaNå€¼")
            return torch.tensor(0.0, device=target_tokens.device), {}
        
        # è®¡ç®—FLAREåŸç‰ˆå¯¹é½æŸå¤±
        cosine_sim = F.cosine_similarity(pred_tokens, target_tokens, dim=-1)
        loss = -cosine_sim.mean()
        
        # é¢å¤–ä¿¡æ¯
        info = {
            'pred_norm': torch.norm(pred_tokens, dim=-1).mean().item(),
            'target_norm': torch.norm(target_tokens, dim=-1).mean().item(),
            'cosine_sim': cosine_sim.mean().item(),
            'pred_shape': list(pred_tokens.shape),
            'target_shape': list(target_tokens.shape)
        }
        
        print(f"ğŸ“Š å¯¹é½ç»Ÿè®¡: cosine_sim={cosine_sim.mean():.4f}, loss={loss:.4f}")
        
        return loss, info
    
    def _cosine_contrastive_loss(self, pred_tokens, target_tokens):
        """ä½™å¼¦å¯¹æ¯”æŸå¤±"""
        # # L2 å½’ä¸€åŒ–
        # pred_norm = F.normalize(pred_tokens, p=2, dim=-1)
        # target_norm = F.normalize(target_tokens, p=2, dim=-1)
        
        # batch_size, num_tokens, hidden_dim = pred_norm.shape
        
        # # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # similarity = torch.bmm(pred_norm, target_norm.transpose(1, 2)) / self.alignment_temperature
        
        # # å¯¹è§’çº¿å…ƒç´ æ˜¯æ­£æ ·æœ¬å¯¹
        # labels = torch.arange(num_tokens, device=similarity.device).unsqueeze(0).expand(batch_size, -1)
        
        # # è®¡ç®—å¯¹æ¯”æŸå¤±
        # loss = F.cross_entropy(similarity.reshape(-1, num_tokens), labels.reshape(-1))
        cosine_sim = F.cosine_similarity(pred_tokens, target_tokens, dim=-1)
    
        # å–è´Ÿæ•°ï¼ˆæœ€å¤§åŒ–ç›¸ä¼¼åº¦ = æœ€å°åŒ–è´Ÿç›¸ä¼¼åº¦ï¼‰
        loss = 1 - cosine_sim.mean()

        return loss
    
    def _kl_divergence_loss(self, pred_tokens, target_tokens):
        """KLæ•£åº¦æŸå¤±"""
        # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        pred_prob = F.softmax(pred_tokens, dim=-1)
        target_prob = F.softmax(target_tokens, dim=-1)
        
        # è®¡ç®—KLæ•£åº¦
        kl_loss = F.kl_div(pred_prob.log(), target_prob, reduction='batchmean')
        
        return kl_loss
    
    def get_alignment_metrics(self) -> Dict[str, float]:
        """è·å–å¯¹é½æŒ‡æ ‡"""
        if not self.loss_history:
            return {}
            
        recent_losses = self.loss_history[-100:]  # æœ€è¿‘100æ­¥
        
        if not recent_losses:
            return {}
            
        metrics = {
            'avg_loss': sum(recent_losses) / len(recent_losses),
            'loss_trend': recent_losses[-1] - recent_losses[0] if len(recent_losses) > 1 else 0,
        }
        
        if self.activation_history:
            recent_activations = self.activation_history[-100:]
            if recent_activations:
                metrics.update({
                    'avg_pred_mean': sum(act.get('pred_mean', 0) for act in recent_activations) / len(recent_activations),
                    'avg_target_mean': sum(act.get('target_mean', 0) for act in recent_activations) / len(recent_activations),
                    'activation_stability': sum(act.get('pred_std', 0) for act in recent_activations) / len(recent_activations)
                })
        
        return metrics
    
    def set_target_layer(self, layer_idx: int):
        """åŠ¨æ€è®¾ç½®ç›®æ ‡å±‚"""
        self.target_layer = layer_idx
        # é‡æ–°åˆ›å»ºæ¿€æ´»æå–å™¨
        self.activation_extractor.remove_hooks()
        self.activation_extractor = DiTActivationExtractor(
            model=self.model,
            target_layers=[layer_idx],
            num_future_tokens=self.num_future_tokens
        )
    
    def clear_history(self):
        """æ¸…ç©ºå†å²è®°å½•"""
        self.activation_history.clear()
        self.loss_history.clear()
        self.activation_extractor.clear_activations()


# ä½¿ç”¨ç¤ºä¾‹
def test_dit_activation_extractor():
    """æµ‹è¯•DiTæ¿€æ´»æå–å™¨"""
    # è¿™é‡Œéœ€è¦ä¸€ä¸ªå®é™…çš„FLAREæ¨¡å‹è¿›è¡Œæµ‹è¯•
    print("DiT Activation Extractor test completed")
    
    # ç¤ºä¾‹ç”¨æ³•ï¼š
    # model = RDTWithFLARE(...)
    # aligner = FLAREActivationAligner(model, target_layer=6)
    # 
    # # åœ¨è®­ç»ƒå¾ªç¯ä¸­
    # target_tokens = target_generator(vl_tokens, vl_mask)
    # alignment_loss, info = aligner.compute_precise_alignment_loss(target_tokens, horizon=32)


if __name__ == "__main__":
    test_dit_activation_extractor()