import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict
from pathlib import Path
import sys
import os

# å¯¼å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å—
from models.rdt.blocks import (FinalLayer, RDTBlock, TimestepEmbedder, get_1d_sincos_pos_embed_from_grid,
                              get_multimodal_cond_pos_embed)
from models.multimodal_encoder.vl_token_generator import VLTokenGenerator
from models.multimodal_encoder.qformer_target_generator import QFormerTargetGenerator
from models.rdt.dit_activation_extractor import FLAREActivationAligner


class RDTWithFLARE(nn.Module):
    """
    å®Œæ•´é›†æˆçš„FLAREå¢å¼ºRDTæ¨¡å‹
    
    åŠŸèƒ½ï¼š
    1. æ ‡å‡†çš„RDTåŠ¨ä½œé¢„æµ‹
    2. æœªæ¥è§‚æµ‹çš„VL tokenç”Ÿæˆ
    3. Q-Formerç›®æ ‡tokenç”Ÿæˆ
    4. DiTå±‚æ¿€æ´»æå–å’Œå¯¹é½
    5. è”åˆæŸå¤±ä¼˜åŒ–
    """

    def __init__(self,
                 output_dim=128,
                 horizon=32,
                 hidden_size=1152,
                 depth=28,
                 num_heads=16,
                 max_lang_cond_len=1024,
                 img_cond_len=4096,
                 lang_pos_embed_config=None,
                 img_pos_embed_config=None,
                 dtype=torch.bfloat16,
                 # FLAREç›¸å…³å‚æ•°
                 num_future_tokens=32,
                 activation_layer=6,
                 num_vl_fusion_layers=4,
                 num_qformer_layers=6,
                 alignment_temperature=0.07,
                 vision_model_name="google/siglip-so400m-patch14-384",
                 text_model_name="google/siglip-so400m-patch14-384"):
        super().__init__()
        
        # åŸºç¡€å‚æ•°
        self.horizon = horizon
        self.hidden_size = hidden_size
        self.max_lang_cond_len = max_lang_cond_len
        self.img_cond_len = img_cond_len
        self.dtype = dtype
        self.lang_pos_embed_config = lang_pos_embed_config
        self.img_pos_embed_config = img_pos_embed_config
        
        # FLAREç›¸å…³å‚æ•°
        self.num_future_tokens = num_future_tokens
        self.activation_layer = activation_layer
        self.alignment_temperature = alignment_temperature

        # åŸºç¡€RDTç»„ä»¶
        self.t_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        self.freq_embedder = TimestepEmbedder(hidden_size, dtype=dtype)

        # ä½ç½®ç¼–ç ï¼š[timestep; freq; state; action; future_obs]
        self.x_pos_embed = nn.Parameter(torch.zeros(1, horizon + 3 + num_future_tokens, hidden_size))
        
        # æ¡ä»¶ä½ç½®ç¼–ç 
        self.lang_cond_pos_embed = nn.Parameter(torch.zeros(1, max_lang_cond_len, hidden_size))
        self.img_cond_pos_embed = nn.Parameter(torch.zeros(1, img_cond_len, hidden_size))

        # Transformer blocks
        self.blocks = nn.ModuleList([RDTBlock(hidden_size, num_heads) for _ in range(depth)])
        self.final_layer = FinalLayer(hidden_size, output_dim)
        
        # FLAREç»„ä»¶
        # 1. VL Tokenç”Ÿæˆå™¨
        self.vl_token_generator = VLTokenGenerator(
            vision_model_name=vision_model_name,
            text_model_name=text_model_name,
            hidden_size=hidden_size,
            num_fusion_layers=num_vl_fusion_layers,
            num_heads=num_heads
        )
        
        # 2. Q-Formerç›®æ ‡ç”Ÿæˆå™¨
        self.target_generator = QFormerTargetGenerator(
            hidden_size=hidden_size,
            num_query_tokens=num_future_tokens,
            num_layers=num_qformer_layers,
            num_heads=num_heads
        )
        
        # 3. æœªæ¥è§‚æµ‹tokenåˆå§‹åŒ–
        self.future_obs_tokens = nn.Parameter(torch.randn(1, num_future_tokens, hidden_size))
        
        # 4. æœªæ¥è§‚æµ‹tokençš„MLPå¤„ç†å™¨
        self.future_obs_mlp = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.GELU(),
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_size)
        )
        
        # 5. æ¿€æ´»å¯¹é½å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¾ªç¯å¼•ç”¨ï¼‰
        self.activation_aligner = None
        
        self.initialize_weights()
        self._ensure_bf16_consistency()
    def _ensure_bf16_consistency(self):
        """ç¡®ä¿æ¨¡å‹æ‰€æœ‰ç»„ä»¶éƒ½ä½¿ç”¨BF16"""
        target_dtype = self.dtype
        
        # è½¬æ¢æ‰€æœ‰å‚æ•°
        for name, param in self.named_parameters():
            if param.dtype != target_dtype:
                param.data = param.data.to(target_dtype)
                
        # è½¬æ¢æ‰€æœ‰ç¼“å†²åŒº  
        for name, buffer in self.named_buffers():
            if buffer.dtype != target_dtype and buffer.dtype.is_floating_point:
                buffer.data = buffer.data.to(target_dtype)
                
        print(f"âœ… æ¨¡å‹ç»Ÿä¸€ä½¿ç”¨æ•°æ®ç±»å‹: {target_dtype}")
    def initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        # åˆå§‹åŒ–transformerå±‚
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

        self.apply(_basic_init)

        # åˆå§‹åŒ–ä½ç½®ç¼–ç 
        x_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                    mm_cond_lens=OrderedDict([
                                                        ('timestep', 1),
                                                        ('ctrl_freq', 1),
                                                        ('state', 1),
                                                        ('action', self.horizon),
                                                        ('future_obs', self.num_future_tokens),
                                                    ]))
        self.x_pos_embed.data.copy_(torch.from_numpy(x_pos_embed).float().unsqueeze(0))

        # è¯­è¨€ä½ç½®ç¼–ç 
        if self.lang_pos_embed_config is None:
            lang_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(self.hidden_size,
                                                                    torch.arange(self.max_lang_cond_len))
        else:
            lang_cond_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                                mm_cond_lens=OrderedDict(self.lang_pos_embed_config),
                                                                embed_modality=False)
        self.lang_cond_pos_embed.data.copy_(torch.from_numpy(lang_cond_pos_embed).float().unsqueeze(0))

        # å›¾åƒä½ç½®ç¼–ç 
        if self.img_pos_embed_config is None:
            img_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(self.hidden_size, torch.arange(self.img_cond_len))
        else:
            img_cond_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                               mm_cond_lens=OrderedDict(self.img_pos_embed_config),
                                                               embed_modality=False)
        self.img_cond_pos_embed.data.copy_(torch.from_numpy(img_cond_pos_embed).float().unsqueeze(0))

        # åˆå§‹åŒ–timestepå’Œfreq embedding
        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[2].weight, std=0.02)

        # åˆå§‹åŒ–æœ€ç»ˆå±‚
        nn.init.constant_(self.final_layer.ffn_final.fc2.weight, 0)
        nn.init.constant_(self.final_layer.ffn_final.fc2.bias, 0)
        
        # åˆå§‹åŒ–FLAREç»„ä»¶
        nn.init.normal_(self.future_obs_tokens, std=0.02)

        # ç§»åŠ¨åˆ°æŒ‡å®šæ•°æ®ç±»å‹
        self.to(self.dtype)
        
    def _initialize_activation_aligner(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ¿€æ´»å¯¹é½å™¨"""
        if self.activation_aligner is None:
            self.activation_aligner = FLAREActivationAligner(
                model=self,
                target_layer=self.activation_layer,
                num_future_tokens=self.num_future_tokens,
                alignment_temperature=self.alignment_temperature,
                loss_type="cosine_contrastive"
            )

    def compute_alignment_loss(self, pred_future_tokens, target_future_tokens, temperature=0.07):
        """
        è®¡ç®—å¯¹é½æŸå¤±ï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ 
        
        Args:
            pred_future_tokens: (B, M, D) é¢„æµ‹çš„æœªæ¥è§‚æµ‹token
            target_future_tokens: (B, M, D) ç›®æ ‡æœªæ¥è§‚æµ‹token
            temperature: æ¸©åº¦å‚æ•°
        """
        # L2å½’ä¸€åŒ–
        pred_norm = F.normalize(pred_future_tokens, p=2, dim=-1)  # (B, M, D)
        target_norm = F.normalize(target_future_tokens, p=2, dim=-1)  # (B, M, D)
        
        batch_size, num_tokens, hidden_dim = pred_norm.shape
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (B, M, M)
        similarity = torch.bmm(pred_norm, target_norm.transpose(1, 2)) / temperature
        
        # å¯¹è§’çº¿å…ƒç´ æ˜¯æ­£æ ·æœ¬å¯¹
        labels = torch.arange(num_tokens, device=similarity.device).unsqueeze(0).expand(batch_size, -1)
        
        # è®¡ç®—å¯¹æ¯”æŸå¤±
        loss = F.cross_entropy(similarity.view(-1, num_tokens), labels.view(-1))
        
        return loss

    def forward(self, x, freq, t, lang_c, img_c, lang_mask=None, img_mask=None, 
                future_vision_tokens=None, text_instructions=None, return_alignment_loss=False):
        """
        FLAREæ¨¡å‹å‰å‘ä¼ æ’­
        
        Args:
            x: (B, T, D) çŠ¶æ€å’ŒåŠ¨ä½œåºåˆ—
            freq: (B,) æ§åˆ¶é¢‘ç‡
            t: (B,) æ—¶é—´æ­¥
            lang_c: (B, L, D) è¯­è¨€æ¡ä»¶
            img_c: (B, I, D) å›¾åƒæ¡ä»¶
            lang_mask: (B, L) è¯­è¨€æ©ç 
            img_mask: (B, I) å›¾åƒæ©ç 
            future_vision_tokens: (B, V, D) æœªæ¥è§‚æµ‹çš„è§†è§‰token
            text_instructions: æ–‡æœ¬æŒ‡ä»¤ï¼ˆç”¨äºVLç”Ÿæˆï¼‰
            return_alignment_loss: æ˜¯å¦è¿”å›å¯¹é½æŸå¤±
        """
        # ğŸ¯ ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹ä¸€è‡´
        # ç»Ÿä¸€æ•°æ®ç±»å‹å¤„ç†
        target_dtype = self.dtype
        device = x.device

        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
        x = x.to(dtype=target_dtype, device=device)
        if isinstance(freq, torch.Tensor):
            freq = freq.to(dtype=target_dtype, device=device)
        if isinstance(t, torch.Tensor):
            t = t.to(dtype=target_dtype, device=device)
        lang_c = lang_c.to(dtype=target_dtype, device=device)
        img_c = img_c.to(dtype=target_dtype, device=device)
        if future_vision_tokens is not None:
            future_vision_tokens = future_vision_tokens.to(dtype=target_dtype, device=device)
        
        batch_size = x.shape[0]
        
        # ç¼–ç æ—¶é—´æ­¥å’Œé¢‘ç‡
        t = self.t_embedder(t).unsqueeze(1)  # (B, 1, D)
        freq = self.freq_embedder(freq).unsqueeze(1)  # (B, 1, D)
        
        # åˆå§‹åŒ–æœªæ¥è§‚æµ‹token
        future_obs_tokens = self.future_obs_tokens.expand(batch_size, -1, -1)  # (B, M, D)
        future_obs_tokens = self.future_obs_mlp(future_obs_tokens)
        
        # æ‰©å±•æ—¶é—´æ­¥åˆ°batch
        if t.shape[0] == 1:
            t = t.expand(batch_size, -1, -1)
        
        # æ‹¼æ¥æ‰€æœ‰token: [timestep, freq, state+action, future_obs]
        x = torch.cat([t, freq, x, future_obs_tokens], dim=1)  # (B, T+3+M, D)

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.x_pos_embed
        lang_c = lang_c + self.lang_cond_pos_embed[:, :lang_c.shape[1]]

        # FLARE: è®¡ç®—ç›®æ ‡tokensï¼ˆå¦‚æœéœ€è¦ï¼‰
        target_future_tokens = None
        if future_vision_tokens is not None and return_alignment_loss:
            try:
                # 1. ç”ŸæˆVL tokens
                vl_tokens, vl_mask = self.vl_token_generator(
                    future_vision_tokens, text_instructions
                )
                
                # 2. ç”Ÿæˆç›®æ ‡tokens
                target_future_tokens = self.target_generator(vl_tokens, vl_mask)
                
            except Exception as e:
                print(f"Warning: FLARE target generation failed: {e}")
                target_future_tokens = None

        # é€šè¿‡transformer blocks
        conds = [lang_c, img_c]
        masks = [lang_mask, img_mask]
        alignment_loss = None
        
        for i, block in enumerate(self.blocks):
            c, mask = conds[i % 2], masks[i % 2]
            x = block(x, c, mask)  # (B, T+3+M, D)

        # æœ€ç»ˆå±‚å¤„ç†
        x = self.final_layer(x)  # (B, T+3+M, out_channels)

        # åªè¿”å›åŠ¨ä½œtokenï¼Œå»é™¤æ—¶é—´æ­¥ã€é¢‘ç‡å’Œæœªæ¥è§‚æµ‹token
        action_tokens = x[:, 2:2+self.horizon]  # (B, horizon, out_channels)
        
        # è®¡ç®—å¯¹é½æŸå¤±ï¼ˆä½¿ç”¨æ¿€æ´»å¯¹é½å™¨ï¼‰
        if return_alignment_loss and target_future_tokens is not None:
            try:
                # åˆå§‹åŒ–æ¿€æ´»å¯¹é½å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
                self._initialize_activation_aligner()
                
                # è®¡ç®—ç²¾ç¡®çš„å¯¹é½æŸå¤±
                alignment_loss, alignment_info = self.activation_aligner.compute_precise_alignment_loss(
                    target_future_tokens, horizon=self.horizon
                )
            except Exception as e:
                print(f"Warning: Alignment loss computation failed: {e}")
                alignment_loss = torch.tensor(0.0, device=action_tokens.device)
        
        if return_alignment_loss:
            return action_tokens, alignment_loss
        else:
            return action_tokens