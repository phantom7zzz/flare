import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict
from pathlib import Path
import sys
import os
import traceback
# å¯¼å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å—
from models.rdt.blocks import (FinalLayer, RDTBlock, TimestepEmbedder, get_1d_sincos_pos_embed_from_grid,
                              get_multimodal_cond_pos_embed)
from models.multimodal_encoder.vl_token_generator import VLTokenGenerator
from models.multimodal_encoder.qformer_target_generator import QFormerTargetGenerator
from models.rdt.dit_activation_extractor import FLAREActivationAligner


# class RDTWithFLARE(nn.Module):
#     """
#     å®Œæ•´é›†æˆçš„FLAREå¢å¼ºRDTæ¨¡å‹
    
#     åŠŸèƒ½ï¼š
#     1. æ ‡å‡†çš„RDTåŠ¨ä½œé¢„æµ‹
#     2. æœªæ¥è§‚æµ‹çš„VL tokenç”Ÿæˆ
#     3. Q-Formerç›®æ ‡tokenç”Ÿæˆ
#     4. DiTå±‚æ¿€æ´»æå–å’Œå¯¹é½
#     5. è”åˆæŸå¤±ä¼˜åŒ–
#     """

#     def __init__(self,
#                  output_dim=128,
#                  horizon=32,
#                  hidden_size=1152,
#                  depth=28,
#                  num_heads=16,
#                  max_lang_cond_len=1024,
#                  img_cond_len=4096,
#                  lang_pos_embed_config=None,
#                  img_pos_embed_config=None,
#                  dtype=torch.bfloat16,
#                  # FLAREç›¸å…³å‚æ•°
#                  num_future_tokens=32,
#                  activation_layer=6,
#                  num_vl_fusion_layers=4,
#                  num_qformer_layers=2,
#                  alignment_temperature=0.07,
#                  vision_model_name="google/siglip-so400m-patch14-384",
#                  text_model_name="google/siglip-so400m-patch14-384"):
#         super().__init__()
        
#         # åŸºç¡€å‚æ•°
#         self.horizon = horizon
#         self.hidden_size = hidden_size
#         self.max_lang_cond_len = max_lang_cond_len
#         self.img_cond_len = img_cond_len
#         self.dtype = dtype
#         self.lang_pos_embed_config = lang_pos_embed_config
#         self.img_pos_embed_config = img_pos_embed_config
        
#         # FLAREç›¸å…³å‚æ•°
#         self.num_future_tokens = num_future_tokens
#         self.activation_layer = activation_layer
#         self.alignment_temperature = alignment_temperature

#         # åŸºç¡€RDTç»„ä»¶
#         self.t_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
#         self.freq_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        
#         self.vision_feature_adapter = nn.Linear(1152, 2048, bias=False)
#         with torch.no_grad():
#             nn.init.xavier_uniform_(self.vision_feature_adapter.weight)
        
#         # ç¡®ä¿future_obs_tokensç»´åº¦æ­£ç¡®
#         if hasattr(self, 'future_obs_tokens'):
#             if self.future_obs_tokens.shape[-1] != 2048:
#                 self.future_obs_tokens = nn.Parameter(
#                     torch.randn(1, self.num_future_tokens, 2048) * 0.02
#                 )
        
#         print("âœ… ç»´åº¦é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
#         # ä½ç½®ç¼–ç ï¼š[timestep; freq; state; action; future_obs]
#         #self.x_pos_embed = nn.Parameter(torch.zeros(1, horizon + 3 + num_future_tokens, hidden_size))
#         self.state_token_len = 1  # çŠ¶æ€å‹ç¼©ä¸º1ä¸ªtoken
#         self.seq_structure = {
#             'timestep': 1,
#             'freq': 1, 
#             'state': self.state_token_len,
#             'action': self.horizon,
#             'future_obs': self.num_future_tokens
#         }
        
#         # è®¡ç®—æ€»åºåˆ—é•¿åº¦
#         total_seq_len = sum(self.seq_structure.values())
#         self.x_pos_embed = nn.Parameter(torch.zeros(1, total_seq_len, hidden_size))
        
#         # é¢„è®¡ç®—ç´¢å¼•ä½ç½®
#         self._compute_sequence_indices()
        
        
        
        
        
#         # æ¡ä»¶ä½ç½®ç¼–ç 
#         self.lang_cond_pos_embed = nn.Parameter(torch.zeros(1, max_lang_cond_len, hidden_size))
#         self.img_cond_pos_embed = nn.Parameter(torch.zeros(1, img_cond_len, hidden_size))

#         # Transformer blocks
#         self.blocks = nn.ModuleList([RDTBlock(hidden_size, num_heads) for _ in range(depth)])
#         self.final_layer = FinalLayer(hidden_size, output_dim)
        
#         # FLAREç»„ä»¶
#         # 1. VL Tokenç”Ÿæˆå™¨
#         self.vl_token_generator = VLTokenGenerator(
#             vision_model_name=vision_model_name,
#             text_model_name=text_model_name,
#             hidden_size=hidden_size,
#             num_fusion_layers=num_vl_fusion_layers,
#             num_heads=num_heads
#         )
        
#         # 2. Q-Formerç›®æ ‡ç”Ÿæˆå™¨
#         self.target_generator = QFormerTargetGenerator(
#             hidden_size=hidden_size,
#             num_query_tokens=num_future_tokens,
#             num_layers=num_qformer_layers,
#             num_heads=num_heads
#         )
        
#         # 3. æœªæ¥è§‚æµ‹tokenåˆå§‹åŒ–
#         self.future_obs_tokens = nn.Parameter(torch.randn(1, num_future_tokens, hidden_size))
        
#         # 4. æœªæ¥è§‚æµ‹tokençš„MLPå¤„ç†å™¨
#         self.future_obs_mlp = nn.Sequential(
#             nn.Linear(hidden_size, hidden_size * 2),
#             nn.GELU(),
#             nn.Linear(hidden_size * 2, hidden_size),
#             nn.Dropout(0.1),
#             nn.LayerNorm(hidden_size)
#         )
        
#         # 5. æ¿€æ´»å¯¹é½å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¾ªç¯å¼•ç”¨ï¼‰
#         self.activation_aligner = None
        
#         self.initialize_weights()
#         self._ensure_bf16_consistency()

class RDTWithFLARE(nn.Module):
    """
    å®Œæ•´é›†æˆçš„FLAREå¢å¼ºRDTæ¨¡å‹
    
    åŠŸèƒ½ï¼š
    1. æ ‡å‡†çš„RDTåŠ¨ä½œé¢„æµ‹
    2. æœªæ¥è§‚æµ‹çš„VL tokenç”Ÿæˆ
    3. Q-Formerç›®æ ‡tokenç”Ÿæˆ
    4. DiTå±‚æ¿€æ´»æå–å’Œå¯¹é½
    5. è”åˆæŸå¤±ä¼˜åŒ–
    """

    def __init__(self,
                 output_dim=128,
                 horizon=32,
                 hidden_size=1152,
                 depth=28,                    # ğŸ¯ é»˜è®¤28å±‚DiT
                 num_heads=16,
                 max_lang_cond_len=32,
                 img_cond_len=4096,
                 lang_pos_embed_config=None,
                 img_pos_embed_config=None,
                 dtype=torch.bfloat16,
                 # FLAREç›¸å…³å‚æ•°
                 num_future_tokens=32,
                 activation_layer=21,
                 num_vl_fusion_layers=4,
                 num_qformer_layers=2,        # ğŸ¯ é»˜è®¤2å±‚Q-Former
                 alignment_temperature=0.07,
                 # ğŸ”§ åªæ¥æ”¶æœªæ¥è§‚æµ‹ç¼–ç å™¨å‚æ•°
                 future_vision_model_name=None,
                 future_text_model_name=None,
                 future_vision_image_size=256,
                 # SigLIP2ç›¸å…³å‚æ•°
                 siglip2_model_name="google/siglip-large-patch16-256",
                 use_pooling=True,
                 target_tokens=64,  # 2x2æ± åŒ–åçš„tokenæ•°é‡):
        super().__init__()
        
        
        # åŸºç¡€å‚æ•°
        self.horizon = horizon
        self.hidden_size = hidden_size
        self.max_lang_cond_len = max_lang_cond_len
        self.img_cond_len = img_cond_len
        self.dtype = dtype
        self.lang_pos_embed_config = lang_pos_embed_config
        self.img_pos_embed_config = img_pos_embed_config
        
        # FLAREç›¸å…³å‚æ•°
        self.num_future_tokens = num_future_tokens
        self.activation_layer = activation_layer
        self.alignment_temperature = alignment_temperature
        # ğŸ”§ æœªæ¥è§‚æµ‹ç¼–ç å™¨è·¯å¾„
        self.future_vision_path = future_vision_model_name or "./models/siglip2-large-patch16-256"
        self.future_text_path = future_text_model_name or self.future_vision_path
        self.future_vision_image_size = future_vision_image_size
        
        self.use_pooling = use_pooling
        self.target_tokens = target_tokens
        
        print(f"ğŸ”§ åˆå§‹åŒ–FLARE-SigLIP2æ¨¡å‹:")
        print(f"   DiTæœªæ¥tokens: {num_future_tokens}")
        print(f"   SigLIP2ç›®æ ‡tokens: {target_tokens}")
        print(f"   ä½¿ç”¨æ± åŒ–: {use_pooling}")
        # åŸºç¡€RDTç»„ä»¶
        self.t_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        self.freq_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        
        self.vision_feature_adapter = nn.Linear(1152, 2048, bias=False)
        with torch.no_grad():
            nn.init.xavier_uniform_(self.vision_feature_adapter.weight)
        
        # ç¡®ä¿future_obs_tokensç»´åº¦æ­£ç¡®
        if hasattr(self, 'future_obs_tokens'):
            if self.future_obs_tokens.shape[-1] != 2048:
                self.future_obs_tokens = nn.Parameter(
                    torch.randn(1, self.num_future_tokens, 2048) * 0.02
                )
        
        print("âœ… ç»´åº¦é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åºåˆ—ç»“æ„å®šä¹‰
        self.state_token_len = 1  # çŠ¶æ€å‹ç¼©ä¸º1ä¸ªtoken
        self.seq_structure = {
            'timestep': 1,
            'freq': 1, 
            'state': self.state_token_len,
            'action': self.horizon,
            'future_obs': self.num_future_tokens
        }
        
        # è®¡ç®—æ€»åºåˆ—é•¿åº¦
        total_seq_len = sum(self.seq_structure.values())
        self.x_pos_embed = nn.Parameter(torch.zeros(1, total_seq_len, hidden_size))
        
        # é¢„è®¡ç®—ç´¢å¼•ä½ç½®
        self._compute_sequence_indices()
        
        # æ¡ä»¶ä½ç½®ç¼–ç 
        self.lang_cond_pos_embed = nn.Parameter(torch.zeros(1, max_lang_cond_len, hidden_size))
        self.img_cond_pos_embed = nn.Parameter(torch.zeros(1, img_cond_len, hidden_size))

        # ğŸ¯ 28å±‚Transformer blocks
        print(f"ğŸ—ï¸  æ„å»º{depth}å±‚DiT blocks...")
        self.blocks = nn.ModuleList([RDTBlock(hidden_size, num_heads) for _ in range(depth)])
        print(f"âœ… {depth}å±‚DiT blocksæ„å»ºå®Œæˆ")
        
        self.final_layer = FinalLayer(hidden_size, output_dim)
        
        # FLAREç»„ä»¶
        print("ğŸ—ï¸  æ„å»ºFLAREç»„ä»¶...")
        
        # # 1. VL Tokenç”Ÿæˆå™¨
        # self.vl_token_generator = VLTokenGenerator(
        #     vision_model_name=self.future_vision_path,     # SigLIP2-256
        #     text_model_name=self.future_text_path,         # SigLIP2-256
        #     hidden_size=hidden_size,
        #     num_fusion_layers=num_vl_fusion_layers,
        #     num_heads=num_heads,
        #     max_text_length=max_lang_cond_len,             # 32
        #     image_size=self.future_vision_image_size,      # 256
        # )
        
        # # 2. ğŸ¯ 2å±‚Q-Formerç›®æ ‡ç”Ÿæˆå™¨
        # print(f"ğŸ—ï¸  æ„å»º{num_qformer_layers}å±‚Q-Former...")
        # self.target_generator = QFormerTargetGenerator(
        #     hidden_size=hidden_size,
        #     num_query_tokens=num_future_tokens,
        #     num_layers=num_qformer_layers,  # ä½¿ç”¨2å±‚
        #     num_heads=num_heads
        # )
        # print(f"âœ… {num_qformer_layers}å±‚Q-Formeræ„å»ºå®Œæˆ")
        # ===========================================
        # ğŸ¯ FLAREæ ¸å¿ƒï¼šSigLIP2ç›®æ ‡ç”Ÿæˆå™¨
        # ===========================================
        print("ğŸ—ï¸  åˆå§‹åŒ–SigLIP2ç›®æ ‡ç”Ÿæˆå™¨...")
        self.siglip2_model = AutoModel.from_pretrained(
            siglip2_model_name, 
            local_files_only=True
        )
        self.siglip2_processor = AutoImageProcessor.from_pretrained(
            siglip2_model_name,
            local_files_only=True
        )
        
        # å†»ç»“SigLIP2æ¨¡å‹
        self.siglip2_model.requires_grad_(False)
        print("âœ… SigLIP2æ¨¡å‹å·²å†»ç»“")
        
        # SigLIP2ç‰¹å¾ç»´åº¦é€‚é…
        siglip2_dim = self.siglip2_model.config.hidden_size  # é€šå¸¸æ˜¯1024
        self.siglip2_adapter = nn.Linear(siglip2_dim, hidden_size, bias=False)
        print(f"   SigLIP2ç»´åº¦: {siglip2_dim} â†’ {hidden_size}")
        
        
        # 3. æœªæ¥è§‚æµ‹tokenåˆå§‹åŒ–
        self.future_obs_tokens = nn.Parameter(torch.randn(1, num_future_tokens, hidden_size))
        
        # 4. æœªæ¥è§‚æµ‹tokençš„MLPå¤„ç†å™¨
        self.future_obs_mlp = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.GELU(),
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_size)
        )
        
        # 5. æ¿€æ´»å¯¹é½å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¾ªç¯å¼•ç”¨ï¼‰
        self.activation_aligner = None
        
        print("âœ… FLAREç»„ä»¶æ„å»ºå®Œæˆ")
        
        self.initialize_weights()
        self._ensure_bf16_consistency()
        
        # æ¨¡å‹è§„æ¨¡ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.parameters())
        dit_params = sum(p.numel() for p in self.blocks.parameters())
        siglip2_params = sum(p.numel() for p in self.siglip2_model.parameters())
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   DiTå‚æ•°: {dit_params:,} ({dit_params/total_params:.1%})")
        print(f"   SigLIP2å‚æ•°: {siglip2_params:,} (å†»ç»“)")
    def _compute_sequence_indices(self):
        """é¢„è®¡ç®—åºåˆ—ä¸­å„éƒ¨åˆ†çš„ç´¢å¼•ä½ç½®"""
        self.indices = {}
        start_idx = 0
        for key, length in self.seq_structure.items():
            self.indices[key] = (start_idx, start_idx + length)
            start_idx += length
    def _generate_siglip2_targets(self, future_obs_images):
        """
        ä½¿ç”¨SigLIP2ç”Ÿæˆç›®æ ‡tokens
        
        Args:
            future_obs_images: (B, 3, H, W) æœªæ¥è§‚æµ‹å›¾åƒ
            
        Returns:
            target_tokens: (B, target_tokens, hidden_size) ç›®æ ‡tokens
        """
        batch_size = future_obs_images.shape[0]
        device = future_obs_images.device
        
        with torch.no_grad():
            # 1. é€šè¿‡SigLIP2æå–ç‰¹å¾
            try:
                # ç¡®ä¿å›¾åƒå°ºå¯¸æ­£ç¡® (SigLIP2æœŸæœ›256x256)
                if future_obs_images.shape[-1] != 256:
                    future_obs_images = F.interpolate(
                        future_obs_images, size=(256, 256), 
                        mode='bilinear', align_corners=False
                    )
                
                # SigLIP2å‰å‘ä¼ æ’­
                siglip2_outputs = self.siglip2_model(future_obs_images)
                siglip2_features = siglip2_outputs.last_hidden_state  # (B, 256, 1024)
                
                print(f"ğŸ” SigLIP2åŸå§‹ç‰¹å¾: {siglip2_features.shape}")
                
                # 2. å¯é€‰çš„2x2æ± åŒ–
                if self.use_pooling:
                    # é‡å¡‘ä¸º2D: (B, 16, 16, 1024) -> (B, 1024, 16, 16)
                    B, L, D = siglip2_features.shape
                    H = W = int(L ** 0.5)  # 256 tokens -> 16x16
                    assert H * W == L, f"Expected square tokens, got {L}"
                    
                    siglip2_features = siglip2_features.transpose(1, 2).reshape(B, D, H, W)
                    
                    # 2x2å¹³å‡æ± åŒ–: 16x16 -> 8x8 = 64 tokens
                    pooled_features = F.avg_pool2d(siglip2_features, kernel_size=2)  # (B, 1024, 8, 8)
                    
                    # é‡å¡‘å›tokenåºåˆ—: (B, 1024, 8, 8) -> (B, 64, 1024)
                    _, D, H_new, W_new = pooled_features.shape
                    siglip2_features = pooled_features.reshape(B, D, H_new * W_new).transpose(1, 2)
                    
                    print(f"ğŸ” æ± åŒ–åç‰¹å¾: {siglip2_features.shape}")
                
            except Exception as e:
                print(f"âŒ SigLIP2ç‰¹å¾æå–å¤±è´¥: {e}")
                # å›é€€åˆ°é›¶ç‰¹å¾
                target_len = self.target_tokens if self.use_pooling else 256
                siglip2_features = torch.zeros(
                    batch_size, target_len, self.siglip2_model.config.hidden_size,
                    device=device, dtype=self.dtype
                )
        
        # 3. ç»´åº¦é€‚é…ï¼šSigLIP2ç»´åº¦ -> DiTéšè—ç»´åº¦
        target_tokens = self.siglip2_adapter(siglip2_features.to(self.dtype))
        
        print(f"ğŸ¯ æœ€ç»ˆç›®æ ‡tokens: {target_tokens.shape}")
        
        return target_tokens
    def _ensure_bf16_consistency(self):
        """ç¡®ä¿æ¨¡å‹æ‰€æœ‰ç»„ä»¶éƒ½ä½¿ç”¨BF16"""
        target_dtype = self.dtype
        
        # è½¬æ¢æ‰€æœ‰å‚æ•°
        for name, param in self.named_parameters():
            if param.dtype != target_dtype:
                param.data = param.data.to(target_dtype)
                
        # è½¬æ¢æ‰€æœ‰ç¼“å†²åŒº  
        for name, buffer in self.named_buffers():
            if buffer.dtype != target_dtype and buffer.dtype.is_floating_point:
                buffer.data = buffer.data.to(target_dtype)
                
        print(f"âœ… æ¨¡å‹ç»Ÿä¸€ä½¿ç”¨æ•°æ®ç±»å‹: {target_dtype}")
    def initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        # åˆå§‹åŒ–transformerå±‚
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

        self.apply(_basic_init)

        # åˆå§‹åŒ–ä½ç½®ç¼–ç 
        x_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                    mm_cond_lens=OrderedDict([
                                                        ('timestep', 1),
                                                        ('ctrl_freq', 1),
                                                        ('state', 1),
                                                        ('action', self.horizon),
                                                        ('future_obs', self.num_future_tokens),
                                                    ]))
        self.x_pos_embed.data.copy_(torch.from_numpy(x_pos_embed).float().unsqueeze(0))

        # è¯­è¨€ä½ç½®ç¼–ç 
        if self.lang_pos_embed_config is None:
            lang_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(self.hidden_size,
                                                                    torch.arange(self.max_lang_cond_len))
        else:
            lang_cond_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                                mm_cond_lens=OrderedDict(self.lang_pos_embed_config),
                                                                embed_modality=False)
        self.lang_cond_pos_embed.data.copy_(torch.from_numpy(lang_cond_pos_embed).float().unsqueeze(0))

        # å›¾åƒä½ç½®ç¼–ç 
        if self.img_pos_embed_config is None:
            img_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(self.hidden_size, torch.arange(self.img_cond_len))
        else:
            img_cond_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                               mm_cond_lens=OrderedDict(self.img_pos_embed_config),
                                                               embed_modality=False)
        self.img_cond_pos_embed.data.copy_(torch.from_numpy(img_cond_pos_embed).float().unsqueeze(0))

        # åˆå§‹åŒ–timestepå’Œfreq embedding
        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[2].weight, std=0.02)

        # åˆå§‹åŒ–æœ€ç»ˆå±‚
        nn.init.constant_(self.final_layer.ffn_final.fc2.weight, 0)
        nn.init.constant_(self.final_layer.ffn_final.fc2.bias, 0)
        
        # åˆå§‹åŒ–FLAREç»„ä»¶
        nn.init.normal_(self.future_obs_tokens, std=0.02)

        # ç§»åŠ¨åˆ°æŒ‡å®šæ•°æ®ç±»å‹
        self.to(self.dtype)
        
    def _initialize_activation_aligner(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ¿€æ´»å¯¹é½å™¨"""
        if self.activation_aligner is None:
            self.activation_aligner = FLAREActivationAligner(
                model=self,
                target_layer=self.activation_layer,
                num_future_tokens=self.num_future_tokens,
                alignment_temperature=self.alignment_temperature,
                loss_type="cosine_contrastive"
            )

    def compute_alignment_loss(self, pred_future_tokens, target_future_tokens, temperature=0.07):
        """
        è®¡ç®—å¯¹é½æŸå¤±ï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ 
        
        Args:
            pred_future_tokens: (B, M, D) é¢„æµ‹çš„æœªæ¥è§‚æµ‹token
            target_future_tokens: (B, M, D) ç›®æ ‡æœªæ¥è§‚æµ‹token
            temperature: æ¸©åº¦å‚æ•°
        """
        # # L2å½’ä¸€åŒ–
        # pred_norm = F.normalize(pred_future_tokens, p=2, dim=-1)  # (B, M, D)
        # target_norm = F.normalize(target_future_tokens, p=2, dim=-1)  # (B, M, D)
        
        # batch_size, num_tokens, hidden_dim = pred_norm.shape
        
        # # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (B, M, M)
        # similarity = torch.bmm(pred_norm, target_norm.transpose(1, 2)) / temperature
        
        # # å¯¹è§’çº¿å…ƒç´ æ˜¯æ­£æ ·æœ¬å¯¹
        # labels = torch.arange(num_tokens, device=similarity.device).unsqueeze(0).expand(batch_size, -1)
        
        # # è®¡ç®—å¯¹æ¯”æŸå¤±
        # loss = F.cross_entropy(similarity.reshape(-1, num_tokens), labels.reshape(-1))
        
        # return loss
         # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_sim = F.cosine_similarity(pred_future_tokens, target_future_tokens, dim=-1)
        
        # è¿”å›è´Ÿä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæœ€å¤§åŒ–ç›¸ä¼¼åº¦ï¼‰
        loss = 1 - cosine_sim.mean()

        return loss

#     def forward(self, x, freq, t, lang_c, img_c, lang_mask=None, img_mask=None, 
#                 future_vision_tokens=None, text_instructions=None, future_obs_image=None, return_alignment_loss=False):
#         """
#         FLAREæ¨¡å‹å‰å‘ä¼ æ’­
        
#         Args:
#             x: (B, T, D) çŠ¶æ€å’ŒåŠ¨ä½œåºåˆ—
#             freq: (B,) æ§åˆ¶é¢‘ç‡
#             t: (B,) æ—¶é—´æ­¥
#             lang_c: (B, L, D) è¯­è¨€æ¡ä»¶
#             img_c: (B, I, D) å›¾åƒæ¡ä»¶
#             lang_mask: (B, L) è¯­è¨€æ©ç 
#             img_mask: (B, I) å›¾åƒæ©ç 
#             future_vision_tokens: (B, V, D) æœªæ¥è§‚æµ‹çš„è§†è§‰token
#             text_instructions: æ–‡æœ¬æŒ‡ä»¤ï¼ˆç”¨äºVLç”Ÿæˆï¼‰
#             return_alignment_loss: æ˜¯å¦è¿”å›å¯¹é½æŸå¤±
#         """
#         # ğŸ¯ ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹ä¸€è‡´
#         # ç»Ÿä¸€æ•°æ®ç±»å‹å¤„ç†
#         target_dtype = self.dtype
#         device = x.device

#         # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
#         x = x.to(dtype=target_dtype, device=device)
#         if isinstance(freq, torch.Tensor):
#             freq = freq.to(dtype=target_dtype, device=device)
#         if isinstance(t, torch.Tensor):
#             t = t.to(dtype=target_dtype, device=device)
#         lang_c = lang_c.to(dtype=target_dtype, device=device)
#         img_c = img_c.to(dtype=target_dtype, device=device)
#         if future_vision_tokens is not None:
#             future_vision_tokens = future_vision_tokens.to(dtype=target_dtype, device=device)
        
#         # batch_size = x.shape[0]
        
#         # # ç¼–ç æ—¶é—´æ­¥å’Œé¢‘ç‡
#         # t = self.t_embedder(t).unsqueeze(1)  # (B, 1, D)
#         # freq = self.freq_embedder(freq).unsqueeze(1)  # (B, 1, D)
        
#         # # åˆå§‹åŒ–æœªæ¥è§‚æµ‹token
#         # future_obs_tokens = self.future_obs_tokens.expand(batch_size, -1, -1)  # (B, M, D)
#         # future_obs_tokens = self.future_obs_mlp(future_obs_tokens)
        
#         # # æ‰©å±•æ—¶é—´æ­¥åˆ°batch
#         # if t.shape[0] == 1:
#         #     t = t.expand(batch_size, -1, -1)
        
#         # # æ‹¼æ¥æ‰€æœ‰token: [timestep, freq, state+action, future_obs]
#         # #x = torch.cat([t, freq, x, future_obs_tokens], dim=1)  # (B, T+3+M, D)
#         # # æ·»åŠ ä½ç½®ç¼–ç 
#         # x = x + self.x_pos_embed
#         # lang_c = lang_c + self.lang_cond_pos_embed[:, :lang_c.shape[1]]
#         batch_size = x.shape[0]
#         if t.shape[0] == 1 and batch_size != 1:
#             # æ¨ç†/é‡‡æ ·æ—¶å¦‚æœtæ˜¯å•å…ƒç´ ï¼Œæ‰©å±•æˆå’Œbatch_sizeä¸€è‡´
#             t = t.expand(batch_size)
#         t_embed = self.t_embedder(t).unsqueeze(1)  # (B, 1, D)
#         device = x.device
#         target_dtype = self.dtype
        
#         # 1. ç¼–ç æ—¶é—´æ­¥å’Œé¢‘ç‡
#         t_embed = self.t_embedder(t).unsqueeze(1)  # (B, 1, D)
#         freq_embed = self.freq_embedder(freq).unsqueeze(1)  # (B, 1, D)
        
#         # 2. åˆ†ç¦»çŠ¶æ€å’ŒåŠ¨ä½œ (å…³é”®ä¿®å¤)
#         # x è¾“å…¥åº”è¯¥æ˜¯ [state_tokens, action_tokens]
#         state_start, state_end = 0, self.state_token_len
#         action_start, action_end = self.state_token_len, x.shape[1]
        
#         state_tokens = x[:, state_start:state_end]  # (B, state_len, D)
#         action_tokens = x[:, action_start:action_end]  # (B, action_len, D)
        
#         # 3. å¤„ç†æœªæ¥è§‚æµ‹token (FLAREæ ¸å¿ƒä¿®å¤)
#         future_obs_tokens = self._process_future_obs_tokens(
#             batch_size, future_obs_image, device, target_dtype
#         )
        
#         # 4. æ­£ç¡®çš„åºåˆ—æ‹¼æ¥ (æŒ‰ç…§å®šä¹‰çš„ç»“æ„)
#         sequence_parts = [
#             t_embed,           # timestep
#             freq_embed,        # freq
#             state_tokens,      # state
#             action_tokens,     # action  
#             future_obs_tokens  # future_obs
#         ]
#         sequence_parts = [part for part in sequence_parts if part is not None]
#         if not sequence_parts:
#             # å¦‚æœæ‰€æœ‰éƒ¨åˆ†éƒ½æ˜¯Noneï¼Œåˆ›å»ºdummy tensor
#             sequence_parts = [torch.zeros(batch_size, 1, 2048, device=device, dtype=dtype)] 
#         for i, p in enumerate(sequence_parts):
#             assert p.shape[0] == batch_size, f"part {i} batch size {p.shape[0]} != {batch_size}"  
#         sequence = torch.cat(sequence_parts, dim=1)  # (B, total_seq_len, D)
        
#         # 5. éªŒè¯åºåˆ—é•¿åº¦
#         expected_len = sum(self.seq_structure.values())
#         assert sequence.shape[1] == expected_len, \
#             f"åºåˆ—é•¿åº¦ä¸åŒ¹é…: {sequence.shape[1]} vs {expected_len}"
        
#         # 6. æ·»åŠ ä½ç½®ç¼–ç 
#         sequence = sequence + self.x_pos_embed
        
#         # 7. å‡†å¤‡æ¡ä»¶
#         conds = [lang_c, img_c]
#         masks = [lang_mask, img_mask]
#         # 8. é€šè¿‡transformer blocks (æ”¹è¿›æ¡ä»¶ä½¿ç”¨ç­–ç•¥)
#         target_future_tokens = None
#         alignment_loss = None
        

#         # FLARE: è®¡ç®—ç›®æ ‡tokensï¼ˆå¦‚æœéœ€è¦ï¼‰
#         # target_future_tokens = None
#         # if future_vision_tokens is not None and return_alignment_loss:
#         #     try:
#         #         # 1. ç”ŸæˆVL tokens
#         #         vl_tokens, vl_mask = self.vl_token_generator(
#         #             future_obs_image, text_instructions
#         #         )

                
#         #         # 2. ç”Ÿæˆç›®æ ‡tokens
#         #         target_future_tokens = self.target_generator(vl_tokens, vl_mask)
#         #     except Exception as e:
#         #         raise  # ç›´æ¥è®©ç¨‹åºå´©æºƒï¼Œæ‰“å°å®Œæ•´Traceback
#         # FLARE: è®¡ç®—ç›®æ ‡tokensï¼ˆåœ¨transformerå¤„ç†å‰ï¼‰
#         if return_alignment_loss and future_obs_image is not None:
#             import torch.nn.functional as F
#             # æŠŠå›¾åƒä» (B,3,384,384) resize åˆ° (B,3,256,256)
#             future_obs_image = F.interpolate(
#                 future_obs_image,
#                 size=(256, 256),
#                 mode='bilinear',
#                 align_corners=False
# )
#             try:
#                 vl_tokens, vl_mask = self.vl_token_generator(
#                     future_obs_image, text_instructions
#                 )
#                 vl_mask = vl_mask.bool()
#                 target_future_tokens = self.target_generator(vl_tokens, vl_mask)
#             except Exception as e:
#                 print(f"ğŸ”´ Target token generation failed: {repr(e)}")
#                 traceback.print_exc()
#                 target_future_tokens = None

#         # é€šè¿‡transformer blocks
#         # conds = [lang_c, img_c]
#         # masks = [lang_mask, img_mask]
#         # alignment_loss = None
        
#         # Transformerå¤„ç†

#         for i, block in enumerate(self.blocks):
#             # äº¤æ›¿æ³¨å…¥è¯­è¨€/è§†è§‰æ¡ä»¶
#             condition_idx = i % len(conds)     # i ä¸ºå¶æ•°æ—¶ 0 â†’ è¯­è¨€ï¼›i ä¸ºå¥‡æ•°æ—¶ 1 â†’ è§†è§‰
#             c, mask = conds[condition_idx], masks[condition_idx]
#             sequence = block(sequence, c, mask)

#         # æœ€ç»ˆå±‚å¤„ç†
#         sequence = self.final_layer(sequence)

#         # åªè¿”å›åŠ¨ä½œtokenï¼Œå»é™¤æ—¶é—´æ­¥ã€é¢‘ç‡å’Œæœªæ¥è§‚æµ‹token
#         #action_tokens = x[:, 2:2+self.horizon]  # (B, horizon, out_channels)
#         action_start_idx, action_end_idx = self.indices['action']
#         action_tokens = sequence[:, action_start_idx:action_end_idx]
        
#         # 11. è®¡ç®—å¯¹é½æŸå¤± (ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•)
#         if return_alignment_loss and target_future_tokens is not None:
#             try:
#                 self._initialize_activation_aligner()
                
#                 # ä½¿ç”¨æ­£ç¡®çš„æœªæ¥tokenä½ç½®
#                 future_start_idx, future_end_idx = self.indices['future_obs']
#                 alignment_loss, _ = self.activation_aligner.compute_precise_alignment_loss(
#                     target_future_tokens, 
#                     horizon=self.horizon,
#                     future_token_indices=(future_start_idx, future_end_idx)
#                 )
#             except Exception as e:
#                 print(f"Warning: Alignment loss computation failed: {e}")
#                 alignment_loss = torch.tensor(0.0, device=action_tokens.device)
        
#         if return_alignment_loss:
#             return action_tokens, alignment_loss
#         else:
#             return action_tokens
    def forward(self, x, freq, t, lang_c, img_c, lang_mask=None, img_mask=None, 
                future_obs_image=None, return_alignment_loss=False, **kwargs):
        """
        FLAREæ¨¡å‹å‰å‘ä¼ æ’­
        """
        # ç»Ÿä¸€æ•°æ®ç±»å‹å¤„ç†
        target_dtype = self.dtype
        device = x.device
        batch_size = x.shape[0]

        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
        x = x.to(dtype=target_dtype, device=device)
        if isinstance(freq, torch.Tensor):
            freq = freq.to(dtype=target_dtype, device=device)
        if isinstance(t, torch.Tensor):
            t = t.to(dtype=target_dtype, device=device)
        lang_c = lang_c.to(dtype=target_dtype, device=device)
        img_c = img_c.to(dtype=target_dtype, device=device)
        if future_obs_image is not None:
            future_obs_image = future_obs_image.to(dtype=target_dtype, device=device)
        
        # å¤„ç†æ—¶é—´æ­¥æ‰©å±•
        if t.shape[0] == 1 and batch_size != 1:
            t = t.expand(batch_size)
            
        # 1. ç¼–ç æ—¶é—´æ­¥å’Œé¢‘ç‡
        t_embed = self.t_embedder(t).unsqueeze(1)  # (B, 1, D)
        freq_embed = self.freq_embedder(freq).unsqueeze(1)  # (B, 1, D)
        
        # 2. åˆ†ç¦»çŠ¶æ€å’ŒåŠ¨ä½œ
        state_start, state_end = 0, self.state_token_len
        action_start, action_end = self.state_token_len, x.shape[1]
        
        state_tokens = x[:, state_start:state_end]  # (B, state_len, D)
        action_tokens = x[:, action_start:action_end]  # (B, action_len, D)
        
        # 3. å¤„ç†æœªæ¥è§‚æµ‹token
        future_obs_tokens = self._process_future_obs_tokens(batch_size, device, target_dtype)
        
        # 4. åºåˆ—æ‹¼æ¥
        sequence_parts = [t_embed, freq_embed, state_tokens, action_tokens, future_obs_tokens]
        sequence = torch.cat(sequence_parts, dim=1)  # (B, total_seq_len, D)
        
        # 5. æ·»åŠ ä½ç½®ç¼–ç 
        sequence = sequence + self.x_pos_embed
        
        # 6. å‡†å¤‡æ¡ä»¶
        conds = [lang_c, img_c]
        masks = [lang_mask, img_mask]
        
        # ===========================================
        # ğŸ¯ FLARE: ç”ŸæˆSigLIP2ç›®æ ‡tokens
        # ===========================================
        target_future_tokens = None
        if return_alignment_loss and future_obs_image is not None:
            try:
                target_future_tokens = self._generate_siglip2_targets(future_obs_image)
                print(f"âœ… æˆåŠŸç”Ÿæˆç›®æ ‡tokens: {target_future_tokens.shape}")
            except Exception as e:
                print(f"ğŸ”´ ç›®æ ‡tokenç”Ÿæˆå¤±è´¥: {repr(e)}")
                target_future_tokens = None

        # 7. é€šè¿‡transformer blocks
        for i, block in enumerate(self.blocks):
            condition_idx = i % len(conds)
            c, mask = conds[condition_idx], masks[condition_idx]
            sequence = block(sequence, c, mask)

        # 8. æœ€ç»ˆå±‚å¤„ç†
        sequence = self.final_layer(sequence)

        # 9. åªè¿”å›åŠ¨ä½œtoken
        action_start_idx, action_end_idx = self.indices['action']
        action_tokens = sequence[:, action_start_idx:action_end_idx]
        
        # 10. è®¡ç®—å¯¹é½æŸå¤±
        alignment_loss = None
        if return_alignment_loss and target_future_tokens is not None:
            try:
                self._initialize_activation_aligner()
                
                # ä½¿ç”¨æ­£ç¡®çš„æœªæ¥tokenä½ç½®
                future_start_idx, future_end_idx = self.indices['future_obs']
                alignment_loss, _ = self.activation_aligner.compute_precise_alignment_loss(
                    target_future_tokens, 
                    horizon=self.horizon,
                    future_token_indices=(future_start_idx, future_end_idx)
                )
                print(f"âœ… å¯¹é½æŸå¤±è®¡ç®—æˆåŠŸ: {alignment_loss.item():.4f}")
            except Exception as e:
                print(f"âš ï¸ å¯¹é½æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                alignment_loss = torch.tensor(0.0, device=action_tokens.device)
        
        if return_alignment_loss:
            return action_tokens, alignment_loss
        else:
            return action_tokens
        
    def _process_future_obs_tokens(self, batch_size, device, target_dtype):
        """å¤„ç†æœªæ¥è§‚æµ‹tokens - DiTåºåˆ—ä¸­çš„å ä½ç¬¦"""
        # ç¡®ä¿future_obs_tokenså‚æ•°å­˜åœ¨ä¸”ç»´åº¦æ­£ç¡®
        if not hasattr(self, 'future_obs_tokens') or self.future_obs_tokens.shape[-1] != self.hidden_size:
            print(f"ğŸ”§ åˆ›å»ºå¯å­¦ä¹ future_obs_tokens: {self.num_future_tokens} x {self.hidden_size}")
            self.future_obs_tokens = nn.Parameter(
                torch.randn(1, self.num_future_tokens, self.hidden_size) * 0.02
            ).to(device=device, dtype=target_dtype)
        
        # æ‰©å±•åˆ°batch size
        future_obs_tokens = self.future_obs_tokens.expand(batch_size, -1, -1)
        
        # é€šè¿‡MLPå¤„ç†
        future_obs_tokens = self.future_obs_mlp(future_obs_tokens)
        
        return future_obs_tokens.to(device=device, dtype=target_dtype)