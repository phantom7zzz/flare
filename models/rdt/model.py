import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict
from pathlib import Path
import sys
import os
import traceback
# å¯¼å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å—
from models.rdt.blocks import (FinalLayer, RDTBlock, TimestepEmbedder, get_1d_sincos_pos_embed_from_grid,
                              get_multimodal_cond_pos_embed)
from models.rdt.dit_activation_extractor import FLAREActivationAligner
from transformers import AutoModel, AutoImageProcessor

class RDTWithFLARE(nn.Module):
    """
    FLAREå¢å¼ºRDTæ¨¡å‹
    
    åŠŸèƒ½ï¼š
    1. æ ‡å‡†çš„RDTåŠ¨ä½œé¢„æµ‹
    2. SigLIP2è§†è§‰ç‰¹å¾ç”Ÿæˆ
    3. DiTå±‚æ¿€æ´»æå–å’Œå¯¹é½
    4. è”åˆæŸå¤±ä¼˜åŒ–
    """

    def __init__(self,
                 output_dim=128,
                 horizon=32,
                 hidden_size=1152,
                 depth=28,
                 num_heads=16,
                 max_lang_cond_len=32,
                 img_cond_len=4096,
                 lang_pos_embed_config=None,
                 img_pos_embed_config=None,
                 dtype=torch.bfloat16,
                 # FLAREç›¸å…³å‚æ•°
                 num_future_tokens=32,
                 activation_layer=21,
                 alignment_temperature=0.07,
                 future_vision_model_name=None,
                 future_text_model_name=None,
                 future_vision_image_size=256,
                 use_pooling=True,
                 target_tokens=64,
                 # å…¼å®¹æ€§å‚æ•°ï¼ˆä¸ä½¿ç”¨ä½†éœ€è¦æ¥æ”¶ï¼‰
                 num_vl_fusion_layers=4,
                 num_qformer_layers=2,
                 **kwargs):
        super().__init__()
        
        # åŸºç¡€å‚æ•°
        self.horizon = horizon
        self.hidden_size = hidden_size
        self.max_lang_cond_len = max_lang_cond_len
        self.img_cond_len = img_cond_len
        self.dtype = dtype
        self.lang_pos_embed_config = lang_pos_embed_config
        self.img_pos_embed_config = img_pos_embed_config
        
        # FLAREç›¸å…³å‚æ•°
        self.num_future_tokens = num_future_tokens
        self.activation_layer = activation_layer
        self.alignment_temperature = alignment_temperature
        self.use_pooling = use_pooling
        self.target_tokens = target_tokens
        
        print(f"ğŸ”§ åˆå§‹åŒ–FLAREæ¨¡å‹: DiT={num_future_tokens}tokens, SigLIP2={target_tokens}tokens")
        
        # åŸºç¡€RDTç»„ä»¶
        self.t_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        self.freq_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        
        self.vision_feature_adapter = nn.Linear(1152, 2048, bias=False)
        with torch.no_grad():
            nn.init.xavier_uniform_(self.vision_feature_adapter.weight)
        
        # åºåˆ—ç»“æ„å®šä¹‰
        self.state_token_len = 1
        self.seq_structure = {
            'timestep': 1,
            'freq': 1, 
            'state': self.state_token_len,
            'action': self.horizon,
            'future_obs': self.num_future_tokens
        }
        
        # è®¡ç®—æ€»åºåˆ—é•¿åº¦
        total_seq_len = sum(self.seq_structure.values())
        self.x_pos_embed = nn.Parameter(torch.zeros(1, total_seq_len, hidden_size))
        
        # é¢„è®¡ç®—ç´¢å¼•ä½ç½®
        self._compute_sequence_indices()
        
        # æ¡ä»¶ä½ç½®ç¼–ç 
        self.lang_cond_pos_embed = nn.Parameter(torch.zeros(1, max_lang_cond_len, hidden_size))
        self.img_cond_pos_embed = nn.Parameter(torch.zeros(1, img_cond_len, hidden_size))

        # Transformer blocks
        self.blocks = nn.ModuleList([RDTBlock(hidden_size, num_heads) for _ in range(depth)])
        self.final_layer = FinalLayer(hidden_size, output_dim)
        
        # FLAREç»„ä»¶ï¼šSigLIP2ç›®æ ‡ç”Ÿæˆå™¨
        self._initialize_siglip2_model(future_vision_model_name)
        
        # æœªæ¥è§‚æµ‹tokenåˆå§‹åŒ–
        self.future_obs_tokens = nn.Parameter(torch.randn(1, num_future_tokens, hidden_size))
        
        # æœªæ¥è§‚æµ‹tokençš„MLPå¤„ç†å™¨
        self.future_obs_mlp = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.GELU(),
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_size)
        )
        
        # æ¿€æ´»å¯¹é½å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.activation_aligner = None
        
        self.initialize_weights()
        self._ensure_bf16_consistency()
        
        # æ¨¡å‹å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.parameters())
        dit_params = sum(p.numel() for p in self.blocks.parameters())
        siglip2_params = sum(p.numel() for p in self.siglip2_model.parameters()) if self.siglip2_model else 0
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡{total_params:,}, DiT{dit_params:,}, SigLIP2{siglip2_params:,}(å†»ç»“)")

    def _initialize_siglip2_model(self, siglip2_path):
        """åˆå§‹åŒ–SigLIP2è§†è§‰ç¼–ç å™¨"""
        if siglip2_path is None:
            raise ValueError("future_vision_model_name ä¸èƒ½ä¸ºç©ºï¼")
        
        if not os.path.exists(siglip2_path):
            raise FileNotFoundError(f"SigLIP2æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {siglip2_path}")
        
        print(f"ğŸ”§ åŠ è½½SigLIP2æ¨¡å‹: {siglip2_path}")
        
        # åŠ è½½å®Œæ•´æ¨¡å‹å¹¶æå–è§†è§‰ç¼–ç å™¨
        full_model = AutoModel.from_pretrained(siglip2_path, local_files_only=True)
        self.siglip2_model = full_model.vision_model  # åªè¦è§†è§‰éƒ¨åˆ†
        self.siglip2_model.eval()
        self.siglip2_model.requires_grad_(False)
        
        # è·å–è§†è§‰ç¼–ç å™¨çš„hidden_size
        vision_config = full_model.config.vision_config
        hidden_size = getattr(vision_config, 'hidden_size', 1024)
        
        self.siglip2_adapter = nn.Linear(hidden_size, self.hidden_size, bias=False)
        print(f"âœ… SigLIP2è§†è§‰ç¼–ç å™¨å·²åŠ è½½ï¼Œç»´åº¦: {hidden_size} â†’ {self.hidden_size}")

    def _compute_sequence_indices(self):
        """é¢„è®¡ç®—åºåˆ—ä¸­å„éƒ¨åˆ†çš„ç´¢å¼•ä½ç½®"""
        self.indices = {}
        start_idx = 0
        for key, length in self.seq_structure.items():
            self.indices[key] = (start_idx, start_idx + length)
            start_idx += length
            
    def _generate_siglip2_targets(self, future_obs_images):
        """ä½¿ç”¨SigLIP2è§†è§‰ç¼–ç å™¨ç”Ÿæˆç›®æ ‡tokens"""
        if future_obs_images is None or self.siglip2_model is None:
            batch_size = future_obs_images.shape[0] if future_obs_images is not None else 1
            device = future_obs_images.device if future_obs_images is not None else next(self.parameters()).device
            return torch.zeros(batch_size, self.target_tokens, self.hidden_size, device=device, dtype=self.dtype)
        
        batch_size = future_obs_images.shape[0]
        device = future_obs_images.device
        
        # ç¡®ä¿å›¾åƒå°ºå¯¸æ˜¯256x256
        if future_obs_images.shape[-1] != 256:
            future_obs_images = F.interpolate(future_obs_images, size=(256, 256), mode='bilinear', align_corners=False)
        
        # ç¡®ä¿è®¾å¤‡å’Œæ•°æ®ç±»å‹åŒ¹é…
        model_device = next(self.siglip2_model.parameters()).device
        model_dtype = next(self.siglip2_model.parameters()).dtype
        future_obs_images = future_obs_images.to(device=model_device, dtype=model_dtype)
        
        with torch.no_grad():
            # è°ƒç”¨SigLIP2è§†è§‰ç¼–ç å™¨
            vision_outputs = self.siglip2_model(future_obs_images)
            
            # è·å–è§†è§‰ç‰¹å¾
            if hasattr(vision_outputs, 'last_hidden_state'):
                features = vision_outputs.last_hidden_state
            elif hasattr(vision_outputs, 'pooler_output'):
                features = vision_outputs.pooler_output.unsqueeze(1)
            else:
                features = vision_outputs
            
            # è°ƒæ•´åˆ°ç›®æ ‡tokenæ•°é‡
            seq_len = features.shape[1]
            if seq_len >= self.target_tokens:
                features = features[:, :self.target_tokens, :]
            else:
                repeat_times = (self.target_tokens + seq_len - 1) // seq_len
                features = features.repeat(1, repeat_times, 1)[:, :self.target_tokens, :]
            
            # ç»´åº¦é€‚é…
            target_tokens = self.siglip2_adapter(features.to(self.dtype))
            
            return target_tokens
    
    def _ensure_bf16_consistency(self):
        """ç¡®ä¿æ¨¡å‹æ‰€æœ‰ç»„ä»¶éƒ½ä½¿ç”¨BF16"""
        target_dtype = self.dtype
        
        # è½¬æ¢æ‰€æœ‰å‚æ•°å’Œç¼“å†²åŒº
        for name, param in self.named_parameters():
            if param.dtype != target_dtype:
                param.data = param.data.to(target_dtype)
                
        for name, buffer in self.named_buffers():
            if buffer.dtype != target_dtype and buffer.dtype.is_floating_point:
                buffer.data = buffer.data.to(target_dtype)
        
        # å¤„ç†SigLIP2æ¨¡å‹
        if hasattr(self, 'siglip2_model') and self.siglip2_model is not None:
            self.siglip2_model = self.siglip2_model.to(target_dtype)
            
    def initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        # åˆå§‹åŒ–transformerå±‚
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

        self.apply(_basic_init)

        # åˆå§‹åŒ–ä½ç½®ç¼–ç 
        x_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                    mm_cond_lens=OrderedDict([
                                                        ('timestep', 1),
                                                        ('ctrl_freq', 1),
                                                        ('state', 1),
                                                        ('action', self.horizon),
                                                        ('future_obs', self.num_future_tokens),
                                                    ]))
        self.x_pos_embed.data.copy_(torch.from_numpy(x_pos_embed).float().unsqueeze(0))

        # è¯­è¨€ä½ç½®ç¼–ç 
        if self.lang_pos_embed_config is None:
            lang_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(self.hidden_size,
                                                                    torch.arange(self.max_lang_cond_len))
        else:
            lang_cond_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                                mm_cond_lens=OrderedDict(self.lang_pos_embed_config),
                                                                embed_modality=False)
        self.lang_cond_pos_embed.data.copy_(torch.from_numpy(lang_cond_pos_embed).float().unsqueeze(0))

        # å›¾åƒä½ç½®ç¼–ç 
        if self.img_pos_embed_config is None:
            img_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(self.hidden_size, torch.arange(self.img_cond_len))
        else:
            img_cond_pos_embed = get_multimodal_cond_pos_embed(embed_dim=self.hidden_size,
                                                               mm_cond_lens=OrderedDict(self.img_pos_embed_config),
                                                               embed_modality=False)
        self.img_cond_pos_embed.data.copy_(torch.from_numpy(img_cond_pos_embed).float().unsqueeze(0))

        # åˆå§‹åŒ–timestepå’Œfreq embedding
        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[2].weight, std=0.02)

        # åˆå§‹åŒ–æœ€ç»ˆå±‚
        nn.init.constant_(self.final_layer.ffn_final.fc2.weight, 0)
        nn.init.constant_(self.final_layer.ffn_final.fc2.bias, 0)
        
        # åˆå§‹åŒ–FLAREç»„ä»¶
        nn.init.normal_(self.future_obs_tokens, std=0.02)

        # ç§»åŠ¨åˆ°æŒ‡å®šæ•°æ®ç±»å‹
        self.to(self.dtype)
        
    def _initialize_activation_aligner(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ¿€æ´»å¯¹é½å™¨"""
        if self.activation_aligner is None:
            self.activation_aligner = FLAREActivationAligner(
                model=self,
                target_layer=self.activation_layer,
                num_future_tokens=self.num_future_tokens,
                alignment_temperature=self.alignment_temperature,
                loss_type="cosine_contrastive"
            )

    def forward(self, x, freq, t, lang_c, img_c, lang_mask=None, img_mask=None, 
                future_obs_image=None, return_alignment_loss=False, **kwargs):
        """FLAREæ¨¡å‹å‰å‘ä¼ æ’­"""
        # ç»Ÿä¸€æ•°æ®ç±»å‹å¤„ç†
        target_dtype = self.dtype
        device = x.device
        batch_size = x.shape[0]

        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç±»å‹
        x = x.to(dtype=target_dtype, device=device)
        if isinstance(freq, torch.Tensor):
            freq = freq.to(dtype=target_dtype, device=device)
        if isinstance(t, torch.Tensor):
            t = t.to(dtype=target_dtype, device=device)
        lang_c = lang_c.to(dtype=target_dtype, device=device)
        img_c = img_c.to(dtype=target_dtype, device=device)
        if future_obs_image is not None:
            future_obs_image = future_obs_image.to(dtype=target_dtype, device=device)
        
        # å¤„ç†æ—¶é—´æ­¥æ‰©å±•
        if t.shape[0] == 1 and batch_size != 1:
            t = t.expand(batch_size)
            
        # 1. ç¼–ç æ—¶é—´æ­¥å’Œé¢‘ç‡
        t_embed = self.t_embedder(t).unsqueeze(1)  # (B, 1, D)
        freq_embed = self.freq_embedder(freq).unsqueeze(1)  # (B, 1, D)
        
        # 2. åˆ†ç¦»çŠ¶æ€å’ŒåŠ¨ä½œ
        state_start, state_end = 0, self.state_token_len
        action_start, action_end = self.state_token_len, x.shape[1]
        
        state_tokens = x[:, state_start:state_end]  # (B, state_len, D)
        action_tokens = x[:, action_start:action_end]  # (B, action_len, D)
        
        # 3. å¤„ç†æœªæ¥è§‚æµ‹token
        future_obs_tokens = self._process_future_obs_tokens(batch_size, device, target_dtype)
        
        # 4. åºåˆ—æ‹¼æ¥
        sequence_parts = [t_embed, freq_embed, state_tokens, action_tokens, future_obs_tokens]
        sequence = torch.cat(sequence_parts, dim=1)  # (B, total_seq_len, D)
        
        # 5. æ·»åŠ ä½ç½®ç¼–ç 
        sequence = sequence + self.x_pos_embed
        
        # 6. å‡†å¤‡æ¡ä»¶
        conds = [lang_c, img_c]
        masks = [lang_mask, img_mask]
        
        # 7. FLARE: ç”ŸæˆSigLIP2ç›®æ ‡tokens
        target_future_tokens = None
        if return_alignment_loss and future_obs_image is not None:
            target_future_tokens = self._generate_siglip2_targets(future_obs_image)
            print(f"ğŸ¯ SigLIP2ç›®æ ‡tokens: {target_future_tokens.shape}")

        # 8. é€šè¿‡transformer blocks
        for i, block in enumerate(self.blocks):
            condition_idx = i % len(conds)
            c, mask = conds[condition_idx], masks[condition_idx]
            sequence = block(sequence, c, mask)

        # 9. æœ€ç»ˆå±‚å¤„ç†
        sequence = self.final_layer(sequence)

        # 10. åªè¿”å›åŠ¨ä½œtoken
        action_start_idx, action_end_idx = self.indices['action']
        action_tokens = sequence[:, action_start_idx:action_end_idx]
        
        # 11. è®¡ç®—å¯¹é½æŸå¤±
        alignment_loss = None
        if return_alignment_loss and target_future_tokens is not None:
            self._initialize_activation_aligner()
            
            # ä½¿ç”¨æ­£ç¡®çš„æœªæ¥tokenä½ç½®
            future_start_idx, future_end_idx = self.indices['future_obs']
            alignment_loss, _ = self.activation_aligner.compute_precise_alignment_loss(
                target_future_tokens, 
                horizon=self.horizon,
                future_token_indices=(future_start_idx, future_end_idx)
            )
            print(f"âœ… å¯¹é½æŸå¤±: {alignment_loss.item():.4f}")
        
        if return_alignment_loss:
            return action_tokens, alignment_loss
        else:
            return action_tokens
        
    def _process_future_obs_tokens(self, batch_size, device, target_dtype):
        """å¤„ç†æœªæ¥è§‚æµ‹tokens - DiTåºåˆ—ä¸­çš„å ä½ç¬¦"""
        # ç¡®ä¿future_obs_tokenså‚æ•°å­˜åœ¨ä¸”ç»´åº¦æ­£ç¡®
        if not hasattr(self, 'future_obs_tokens') or self.future_obs_tokens.shape[-1] != self.hidden_size:
            self.future_obs_tokens = nn.Parameter(
                torch.randn(1, self.num_future_tokens, self.hidden_size) * 0.02
            ).to(device=device, dtype=target_dtype)
        
        # æ‰©å±•åˆ°batch size
        future_obs_tokens = self.future_obs_tokens.expand(batch_size, -1, -1)
        
        # é€šè¿‡MLPå¤„ç†
        future_obs_tokens = self.future_obs_mlp(future_obs_tokens)
        
        return future_obs_tokens.to(device=device, dtype=target_dtype)