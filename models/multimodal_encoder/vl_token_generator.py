import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoConfig, AutoModel, AutoTokenizer
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
import os

# å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"

class SigLIP2TextEncoder(nn.Module):
    """SigLIP2æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå¤„ç†ä»»åŠ¡æŒ‡ä»¤ - ç¦»çº¿ç‰ˆæœ¬"""
    
    def __init__(self, text_model_name="google/siglip-so400m-patch14-384", max_length=77, device="cuda", torch_dtype=torch.float16):
        super().__init__()
        self.max_length = max_length
        self.device = device
        self.torch_dtype = torch_dtype
        
        print(f"ğŸ”§ åˆå§‹åŒ–SigLIP2TextEncoder (ç¦»çº¿æ¨¡å¼)")
        print(f"   æ¨¡å‹è·¯å¾„: {text_model_name}")
        
        # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œé¿å…ç½‘ç»œä¸‹è½½
        try:
            print(f"   å°è¯•ä»æœ¬åœ°åŠ è½½SigLIPæ¨¡å‹...")
            from transformers import SiglipTextModel, SiglipTokenizer
            
            # å¼ºåˆ¶æœ¬åœ°åŠ è½½
            self.tokenizer = SiglipTokenizer.from_pretrained(
                text_model_name, 
                local_files_only=True,
                trust_remote_code=False
            )
            self.text_model = SiglipTextModel.from_pretrained(
                text_model_name,
                local_files_only=True,
                trust_remote_code=False,
                torch_dtype=torch_dtype
            )
            print(f"   âœ… æˆåŠŸåŠ è½½æœ¬åœ°SigLIPæ¨¡å‹")
            
        except Exception as e:
            print(f"   âŒ æœ¬åœ°SigLIPåŠ è½½å¤±è´¥: {e}")
            print(f"   ğŸ”„ å°è¯•ä½¿ç”¨CLIPä½œä¸ºfallback...")
            
            try:
                from transformers import CLIPTextModel, CLIPTokenizer
                self.tokenizer = CLIPTokenizer.from_pretrained(
                    "openai/clip-vit-base-patch32",
                    local_files_only=True
                )
                self.text_model = CLIPTextModel.from_pretrained(
                    "openai/clip-vit-base-patch32",
                    local_files_only=True,
                    torch_dtype=torch_dtype
                )
                print(f"   âœ… æˆåŠŸåŠ è½½æœ¬åœ°CLIPæ¨¡å‹ä½œä¸ºfallback")
                
            except Exception as e2:
                print(f"   âŒ CLIP fallbackä¹Ÿå¤±è´¥: {e2}")
                print(f"   ğŸ†˜ åˆ›å»ºdummyæ–‡æœ¬ç¼–ç å™¨")
                
                # åˆ›å»ºdummy tokenizerå’Œæ¨¡å‹
                self.tokenizer = None
                self.text_model = None
                self.hidden_size = 768  # é»˜è®¤éšè—å±‚å¤§å°
                return
        
        if self.text_model is not None:
            self.text_model.eval()
            self.hidden_size = self.text_model.config.hidden_size
        
    def forward(self, text_inputs):
        """
        ç¼–ç æ–‡æœ¬æŒ‡ä»¤
        Args:
            text_inputs: æ–‡æœ¬æŒ‡ä»¤åˆ—è¡¨æˆ–å·²ç¼–ç çš„token ids
        Returns:
            text_embeddings: (B, L, D) æ–‡æœ¬åµŒå…¥
            text_mask: (B, L) æ–‡æœ¬æ©ç 
        """
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¿”å›dummyç»“æœ
        if self.text_model is None or self.tokenizer is None:
            if isinstance(text_inputs, list):
                batch_size = len(text_inputs)
                seq_length = self.max_length
            else:
                batch_size, seq_length = text_inputs.shape[:2]
            
            # è¿”å›é›¶åµŒå…¥å’Œå…¨Trueæ©ç 
            device = next(self.parameters()).device if list(self.parameters()) else torch.device('cuda')
            text_embeddings = torch.zeros(batch_size, seq_length, self.hidden_size, 
                                        device=device, dtype=self.torch_dtype)
            text_mask = torch.ones(batch_size, seq_length, device=device, dtype=torch.bool)
            
            print(f"âš ï¸ è¿”å›dummyæ–‡æœ¬åµŒå…¥: {text_embeddings.shape}")
            return text_embeddings, text_mask

        device = next(self.text_model.parameters()).device

        if isinstance(text_inputs, list):
            tokens = self.tokenizer(
                text_inputs,
                max_length=self.max_length,
                padding="longest",
                truncation=True,
                return_tensors="pt",
                return_attention_mask=True
            )
            input_ids = tokens.input_ids.to(device)
            attention_mask = tokens.attention_mask.to(device)
        else:
            input_ids = text_inputs.to(device)
            attention_mask = (input_ids != self.tokenizer.pad_token_id).long().to(device)

        # è·å–æ–‡æœ¬åµŒå…¥
        with torch.no_grad():
            text_outputs = self.text_model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            text_embeddings = text_outputs.last_hidden_state

        return text_embeddings, attention_mask.bool()


class VLFusionBlock(nn.Module):
    """VLèåˆå—ï¼šSelf-Attention + FeedForward"""
    
    def __init__(self, hidden_size, num_heads=8, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        
        # Self-Attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Norm
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)
        
        # FeedForward
        self.feedforward = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 4, hidden_size),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, mask=None):
        """
        Args:
            x: (B, N, D) è¾“å…¥ç‰¹å¾
            mask: (B, N) æ³¨æ„åŠ›æ©ç 
        Returns:
            x: (B, N, D) è¾“å‡ºç‰¹å¾
        """
        # Self-Attention with residual connection
        residual = x
        x = self.norm1(x)
        attn_out, _ = self.self_attention(
            x, x, x, 
            key_padding_mask=mask if mask is not None else None
        )
        x = residual + attn_out
        
        # FeedForward with residual connection
        residual = x
        x = self.norm2(x)
        ff_out = self.feedforward(x)
        x = residual + ff_out
        
        return x


class VLTokenGenerator(nn.Module):
    """
    å®Œæ•´çš„VL Tokenç”Ÿæˆæ¨¡å— - ç¦»çº¿ç‰ˆæœ¬
    
    æµç¨‹ï¼š
    1. æœªæ¥è§‚æµ‹å›¾åƒ â†’ SigLIPè§†è§‰ç¼–ç  â†’ patch tokens
    2. ä»»åŠ¡æŒ‡ä»¤ â†’ SigLIP2æ–‡æœ¬ç¼–ç  â†’ è¯­è¨€tokens
    3. patch tokens + è¯­è¨€tokens â†’ å¤šå±‚self-attentionèåˆ â†’ VL tokens
    """
    
    def __init__(self, 
                 vision_model_name="google/siglip-so400m-patch14-384",
                 text_model_name="google/siglip-so400m-patch14-384",
                 hidden_size=1152,
                 num_fusion_layers=4,
                 num_heads=8,
                 dropout=0.1,
                 max_text_length=77,
                 device="cuda",
                 torch_dtype=torch.float16,
                 use_precomp_lang_embed=False):
        super().__init__()
        
        print(f"ğŸ”§ åˆå§‹åŒ–VLTokenGenerator (ç¦»çº¿æ¨¡å¼)")
        print(f"   vision_model: {vision_model_name}")
        print(f"   text_model: {text_model_name}")
        print(f"   use_precomp_lang_embed: {use_precomp_lang_embed}")
        
        self.hidden_size = hidden_size
        self.num_fusion_layers = num_fusion_layers
        self.use_precomp_lang_embed = use_precomp_lang_embed
        
        # 1. è§†è§‰ç¼–ç å™¨ (SigLIP) - å¼ºåˆ¶æœ¬åœ°åŠ è½½
        print(f"   ğŸ–¼ï¸ åˆå§‹åŒ–è§†è§‰ç¼–ç å™¨...")
        self.vision_encoder = SiglipVisionTower(
            vision_tower=vision_model_name, 
            args=None
        )
        print(f"   âœ… è§†è§‰ç¼–ç å™¨åˆå§‹åŒ–å®Œæˆï¼Œhidden_size: {self.vision_encoder.hidden_size}")
        
        # 2. æ–‡æœ¬ç¼–ç å™¨ (SigLIP2) - æ ¹æ®æ˜¯å¦ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥å†³å®š
        if use_precomp_lang_embed:
            print(f"   âš ï¸ è·³è¿‡æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–ï¼ˆä½¿ç”¨é¢„è®¡ç®—è¯­è¨€åµŒå…¥ï¼‰")
            self.text_encoder = None
            # ä¸ºé¢„è®¡ç®—åµŒå…¥è®¾ç½®é»˜è®¤æ–‡æœ¬ç»´åº¦
            text_dim = hidden_size  # å‡è®¾é¢„è®¡ç®—åµŒå…¥å·²ç»æ˜¯ç›®æ ‡ç»´åº¦
        else:
            print(f"   ğŸ“ åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨...")
            self.text_encoder = SigLIP2TextEncoder(
                text_model_name=text_model_name,
                max_length=max_text_length,
                device=device,
                torch_dtype=torch_dtype
            )
            text_dim = self.text_encoder.hidden_size if self.text_encoder.text_model is not None else hidden_size
            print(f"   âœ… æ–‡æœ¬ç¼–ç å™¨åˆå§‹åŒ–å®Œæˆï¼Œhidden_size: {text_dim}")
        
        # 3. ç‰¹å¾ç»´åº¦å¯¹é½
        vision_dim = self.vision_encoder.hidden_size
        
        print(f"   ğŸ”§ é…ç½®ç»´åº¦å¯¹é½:")
        print(f"      vision_dim: {vision_dim}")
        print(f"      text_dim: {text_dim}")
        print(f"      target_hidden_size: {hidden_size}")
        
        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œæ·»åŠ æŠ•å½±å±‚
        if vision_dim != hidden_size:
            self.vision_proj = nn.Linear(vision_dim, hidden_size)
            print(f"      åˆ›å»ºè§†è§‰æŠ•å½±å±‚: {vision_dim} -> {hidden_size}")
        else:
            self.vision_proj = nn.Identity()
            print(f"      è§†è§‰ç»´åº¦åŒ¹é…ï¼Œæ— éœ€æŠ•å½±")
            
        if text_dim != hidden_size:
            self.text_proj = nn.Linear(text_dim, hidden_size)
            print(f"      åˆ›å»ºæ–‡æœ¬æŠ•å½±å±‚: {text_dim} -> {hidden_size}")
        else:
            self.text_proj = nn.Identity()
            print(f"      æ–‡æœ¬ç»´åº¦åŒ¹é…ï¼Œæ— éœ€æŠ•å½±")
        
        # 4. ä½ç½®ç¼–ç 
        self.vision_pos_embed = nn.Parameter(
            torch.zeros(1, self.vision_encoder.num_patches, hidden_size)
        )
        self.text_pos_embed = nn.Parameter(
            torch.zeros(1, max_text_length, hidden_size)
        )
        
        # 5. æ¨¡æ€ç±»å‹åµŒå…¥
        self.vision_type_embed = nn.Parameter(torch.zeros(1, 1, hidden_size))
        self.text_type_embed = nn.Parameter(torch.zeros(1, 1, hidden_size))
        
        # 6. å¤šå±‚VLèåˆå—
        self.fusion_layers = nn.ModuleList([
            VLFusionBlock(hidden_size, num_heads, dropout)
            for _ in range(num_fusion_layers)
        ])
        
        # 7. è¾“å‡ºå±‚å½’ä¸€åŒ–
        self.output_norm = nn.LayerNorm(hidden_size)
        
        # åˆå§‹åŒ–å‚æ•°
        self._initialize_weights()
        
        print(f"   âœ… VLTokenGeneratoråˆå§‹åŒ–å®Œæˆ")
        
    def _initialize_weights(self):
        """åˆå§‹åŒ–å‚æ•°"""
        # ä½ç½®ç¼–ç åˆå§‹åŒ–
        nn.init.trunc_normal_(self.vision_pos_embed, std=0.02)
        nn.init.trunc_normal_(self.text_pos_embed, std=0.02)
        
        # æ¨¡æ€ç±»å‹åµŒå…¥åˆå§‹åŒ–
        nn.init.trunc_normal_(self.vision_type_embed, std=0.02)
        nn.init.trunc_normal_(self.text_type_embed, std=0.02)
        
        # æŠ•å½±å±‚åˆå§‹åŒ–
        for module in [self.vision_proj, self.text_proj]:
            if isinstance(module, nn.Linear):
                nn.init.trunc_normal_(module.weight, std=0.02)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, future_images, text_instructions, image_mask=None, text_mask=None):
        """
        ç”ŸæˆVL tokens
        
        Args:
            future_images: (B, C, H, W) æœªæ¥è§‚æµ‹å›¾åƒ
            text_instructions: (B, L) æ–‡æœ¬æŒ‡ä»¤token ids æˆ–æ–‡æœ¬åˆ—è¡¨ æˆ–é¢„è®¡ç®—åµŒå…¥
            image_mask: (B, N_patches) å›¾åƒæ©ç ï¼ˆå¯é€‰ï¼‰
            text_mask: (B, L) æ–‡æœ¬æ©ç ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            vl_tokens: (B, N_total, D) èåˆåçš„VL tokens
            vl_mask: (B, N_total) VL tokensçš„æ©ç 
        """
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥æ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = next(self.parameters()).device
        
        if future_images is not None:
            future_images = future_images.to(device)
            batch_size = future_images.shape[0]
            
            # å¤„ç†3Då›¾åƒè¾“å…¥
            if len(future_images.shape) == 3:
                future_images = future_images.unsqueeze(0)
        else:
            # å¦‚æœæ²¡æœ‰å›¾åƒè¾“å…¥ï¼Œä»æ–‡æœ¬è·å–batch_size
            if isinstance(text_instructions, torch.Tensor):
                batch_size = text_instructions.shape[0]
            elif isinstance(text_instructions, list):
                batch_size = len(text_instructions)
            else:
                batch_size = 1
        
        # 1. è§†è§‰ç¼–ç 
        if future_images is not None:
            vision_features = self.vision_encoder(future_images)  # (B, N_patches, D_vision)
            vision_features = self.vision_proj(vision_features)   # (B, N_patches, hidden_size)
            
            # æ·»åŠ è§†è§‰ä½ç½®ç¼–ç å’Œæ¨¡æ€ç±»å‹åµŒå…¥
            vision_features = vision_features + self.vision_pos_embed[:, :vision_features.shape[1], :]
            vision_features = vision_features + self.vision_type_embed
        else:
            # å¦‚æœæ²¡æœ‰å›¾åƒï¼Œåˆ›å»ºç©ºçš„è§†è§‰ç‰¹å¾
            vision_features = torch.empty(batch_size, 0, self.hidden_size, device=device)
        
        # 2. æ–‡æœ¬ç¼–ç 
        if self.use_precomp_lang_embed:
            # ä½¿ç”¨é¢„è®¡ç®—çš„è¯­è¨€åµŒå…¥
            if isinstance(text_instructions, torch.Tensor):
                text_features = text_instructions.to(device)
                if text_mask is None:
                    text_mask = torch.ones(text_features.shape[:2], dtype=torch.bool, device=device)
            else:
                # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—åµŒå…¥ï¼Œåˆ›å»ºé›¶åµŒå…¥
                seq_len = 77  # é»˜è®¤é•¿åº¦
                text_features = torch.zeros(batch_size, seq_len, self.hidden_size, device=device)
                text_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool, device=device)
                
            computed_text_mask = text_mask
        else:
            # ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨
            if self.text_encoder is not None:
                text_features, computed_text_mask = self.text_encoder(text_instructions)  # (B, L, D_text)
                text_features = text_features.to(device)
                computed_text_mask = computed_text_mask.to(device)
            else:
                # å¦‚æœæ–‡æœ¬ç¼–ç å™¨ä¸å¯ç”¨ï¼Œåˆ›å»ºdummyç‰¹å¾
                seq_len = 77
                text_features = torch.zeros(batch_size, seq_len, self.hidden_size, device=device)
                computed_text_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool, device=device)
        
        text_features = self.text_proj(text_features)  # (B, L, hidden_size)
        
        # ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ©ç æˆ–æä¾›çš„æ©ç 
        if text_mask is None:
            text_mask = computed_text_mask
        
        # æ·»åŠ æ–‡æœ¬ä½ç½®ç¼–ç å’Œæ¨¡æ€ç±»å‹åµŒå…¥
        if text_features.shape[1] > 0:
            text_features = text_features + self.text_pos_embed[:, :text_features.shape[1], :]
            text_features = text_features + self.text_type_embed
        
        # 3. æ‹¼æ¥è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾
        if vision_features.shape[1] > 0 and text_features.shape[1] > 0:
            vl_features = torch.cat([vision_features, text_features], dim=1)
        elif vision_features.shape[1] > 0:
            vl_features = vision_features
        elif text_features.shape[1] > 0:
            vl_features = text_features
        else:
            # å¦‚æœéƒ½ä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªdummyç‰¹å¾
            vl_features = torch.zeros(batch_size, 1, self.hidden_size, device=device)
        
        # 4. åˆ›å»ºè”åˆæ©ç 
        if image_mask is None and vision_features.shape[1] > 0:
            image_mask = torch.ones(batch_size, vision_features.shape[1], 
                                  dtype=torch.bool, device=device)
        elif vision_features.shape[1] == 0:
            image_mask = torch.empty(batch_size, 0, dtype=torch.bool, device=device)
        
        if vision_features.shape[1] > 0 and text_features.shape[1] > 0:
            vl_mask = torch.cat([image_mask, text_mask], dim=1)
        elif vision_features.shape[1] > 0:
            vl_mask = image_mask
        elif text_features.shape[1] > 0:
            vl_mask = text_mask
        else:
            vl_mask = torch.ones(batch_size, 1, dtype=torch.bool, device=device)
        
        # 5. é€šè¿‡å¤šå±‚èåˆå—
        for fusion_layer in self.fusion_layers:
            vl_features = fusion_layer(vl_features, mask=~vl_mask)  # æ³¨æ„ï¼šmaskéœ€è¦åè½¬
        
        # 6. è¾“å‡ºå½’ä¸€åŒ–
        vl_tokens = self.output_norm(vl_features)
        
        return vl_tokens, vl_mask
    
    def get_vision_features(self, images):
        """å•ç‹¬è·å–è§†è§‰ç‰¹å¾ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        return self.vision_encoder(images)
    
    def get_text_features(self, text_instructions):
        """å•ç‹¬è·å–æ–‡æœ¬ç‰¹å¾ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        if self.text_encoder is not None:
            return self.text_encoder(text_instructions)
        else:
            print("âš ï¸ æ–‡æœ¬ç¼–ç å™¨ä¸å¯ç”¨")
            return None, None


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_vl_token_generator():
    """æµ‹è¯•VL Tokenç”Ÿæˆå™¨"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºVL Tokenç”Ÿæˆå™¨
    vl_generator = VLTokenGenerator(
        vision_model_name="../weights/RDT/siglip-so400m-patch14-384",  # ä½¿ç”¨æœ¬åœ°è·¯å¾„
        text_model_name="../weights/RDT/siglip-so400m-patch14-384",    # ä½¿ç”¨æœ¬åœ°è·¯å¾„
        hidden_size=1152,
        num_fusion_layers=4,
        use_precomp_lang_embed=True  # ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥
    ).to(device)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    future_images = torch.randn(batch_size, 3, 384, 384).to(device)
    
    # ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥è€Œä¸æ˜¯æ–‡æœ¬
    text_embeddings = torch.randn(batch_size, 77, 1152).to(device)  # é¢„è®¡ç®—åµŒå…¥
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        vl_tokens, vl_mask = vl_generator(future_images, text_embeddings)
    
    print(f"VL tokens shape: {vl_tokens.shape}")
    print(f"VL mask shape: {vl_mask.shape}")
    print(f"Valid tokens per sample: {vl_mask.sum(dim=1)}")
    
    return vl_tokens, vl_mask


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_vl_token_generator()