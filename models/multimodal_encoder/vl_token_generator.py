

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoConfig, AutoModel, AutoTokenizer
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower


class SigLIP2TextEncoder(nn.Module):
    """SigLIP2æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå¤„ç†ä»»åŠ¡æŒ‡ä»¤"""
    
    def __init__(self, text_model_name="google/siglip-so400m-patch14-384", max_length=77):
        super().__init__()
        self.max_length = max_length
        
        # åŠ è½½SigLIP2çš„æ–‡æœ¬ç¼–ç å™¨
        try:
            from transformers import SiglipTextModel, SiglipTokenizer
            self.tokenizer = SiglipTokenizer.from_pretrained(text_model_name)
            self.text_model = SiglipTextModel.from_pretrained(text_model_name)
        except ImportError:
            # å¦‚æœSigLIP2ä¸å¯ç”¨ï¼Œä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨ä½œä¸ºæ›¿ä»£
            print("Warning: SigLIP2 not available, using CLIP text encoder as fallback")
            from transformers import CLIPTextModel, CLIPTokenizer
            self.tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
            self.text_model = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
        
        self.text_model.eval()
        
    def forward(self, text_inputs):
        """
        ç¼–ç æ–‡æœ¬æŒ‡ä»¤
        Args:
            text_inputs: æ–‡æœ¬æŒ‡ä»¤åˆ—è¡¨æˆ–å·²ç¼–ç çš„token ids
        Returns:
            text_embeddings: (B, L, D) æ–‡æœ¬åµŒå…¥
            text_mask: (B, L) æ–‡æœ¬æ©ç 
        """
        device = next(self.text_model.parameters()).device

        if isinstance(text_inputs, list):
            tokens = self.tokenizer(
                text_inputs,
                max_length=self.max_length,
                padding="longest",
                truncation=True,
                return_tensors="pt",
                return_attention_mask=True
            )
            input_ids = tokens.input_ids.to(device)
            attention_mask = tokens.attention_mask.to(device)
        else:
            input_ids = text_inputs.to(device)
            attention_mask = (input_ids != self.tokenizer.pad_token_id).long().to(device)

        # è·å–æ–‡æœ¬åµŒå…¥
        with torch.no_grad():
            text_outputs = self.text_model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            text_embeddings = text_outputs.last_hidden_state

        return text_embeddings, attention_mask.bool()


class VLFusionBlock(nn.Module):
    """VLèåˆå—ï¼šSelf-Attention + FeedForward"""
    
    def __init__(self, hidden_size, num_heads=8, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        
        # Self-Attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Norm
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)
        
        # FeedForward
        self.feedforward = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 4, hidden_size),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, mask=None):
        """
        Args:
            x: (B, N, D) è¾“å…¥ç‰¹å¾
            mask: (B, N) æ³¨æ„åŠ›æ©ç 
        Returns:
            x: (B, N, D) è¾“å‡ºç‰¹å¾
        """
        # Self-Attention with residual connection
        residual = x
        x = self.norm1(x)
        attn_out, _ = self.self_attention(
            x, x, x, 
            key_padding_mask=mask if mask is not None else None
        )
        x = residual + attn_out
        
        # FeedForward with residual connection
        residual = x
        x = self.norm2(x)
        ff_out = self.feedforward(x)
        x = residual + ff_out
        
        return x


class VLTokenGenerator(nn.Module):
    """
    å®Œæ•´çš„VL Tokenç”Ÿæˆæ¨¡å—
    
    æµç¨‹ï¼š
    1. æœªæ¥è§‚æµ‹å›¾åƒ â†’ SigLIPè§†è§‰ç¼–ç  â†’ patch tokens
    2. ä»»åŠ¡æŒ‡ä»¤ â†’ SigLIP2æ–‡æœ¬ç¼–ç  â†’ è¯­è¨€tokens
    3. patch tokens + è¯­è¨€tokens â†’ å¤šå±‚self-attentionèåˆ â†’ VL tokens
    """
    
    def __init__(self, 
                 vision_model_name="google/siglip-so400m-patch14-384",
                 text_model_name="google/siglip-so400m-patch14-384",
                 hidden_size=1152,
                 num_fusion_layers=4,
                 num_heads=8,
                 dropout=0.1,
                 max_text_length=77):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_fusion_layers = num_fusion_layers
        
        # 1. è§†è§‰ç¼–ç å™¨ (SigLIP)
        self.vision_encoder = SiglipVisionTower(
            vision_tower=vision_model_name, 
            args=None
        )
        
        # 2. æ–‡æœ¬ç¼–ç å™¨ (SigLIP2)
        self.text_encoder = SigLIP2TextEncoder(
            text_model_name=text_model_name,
            max_length=max_text_length
        )
        
        # 3. ç‰¹å¾ç»´åº¦å¯¹é½
        vision_dim = self.vision_encoder.hidden_size
        text_dim = self.text_encoder.text_model.config.hidden_size
        
        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œæ·»åŠ æŠ•å½±å±‚
        if vision_dim != hidden_size:
            self.vision_proj = nn.Linear(vision_dim, hidden_size)
        else:
            self.vision_proj = nn.Identity()
            
        if text_dim != hidden_size:
            self.text_proj = nn.Linear(text_dim, hidden_size)
        else:
            self.text_proj = nn.Identity()
        
        # 4. ä½ç½®ç¼–ç 
        self.vision_pos_embed = nn.Parameter(
            torch.zeros(1, self.vision_encoder.num_patches, hidden_size)
        )
        self.text_pos_embed = nn.Parameter(
            torch.zeros(1, max_text_length, hidden_size)
        )
        
        # 5. æ¨¡æ€ç±»å‹åµŒå…¥
        self.vision_type_embed = nn.Parameter(torch.zeros(1, 1, hidden_size))
        self.text_type_embed = nn.Parameter(torch.zeros(1, 1, hidden_size))
        
        # 6. å¤šå±‚VLèåˆå—
        self.fusion_layers = nn.ModuleList([
            VLFusionBlock(hidden_size, num_heads, dropout)
            for _ in range(num_fusion_layers)
        ])
        
        # 7. è¾“å‡ºå±‚å½’ä¸€åŒ–
        self.output_norm = nn.LayerNorm(hidden_size)
        
        # åˆå§‹åŒ–å‚æ•°
        self._initialize_weights()
        
    def _initialize_weights(self):
        """åˆå§‹åŒ–å‚æ•°"""
        # ä½ç½®ç¼–ç åˆå§‹åŒ–
        nn.init.trunc_normal_(self.vision_pos_embed, std=0.02)
        nn.init.trunc_normal_(self.text_pos_embed, std=0.02)
        
        # æ¨¡æ€ç±»å‹åµŒå…¥åˆå§‹åŒ–
        nn.init.trunc_normal_(self.vision_type_embed, std=0.02)
        nn.init.trunc_normal_(self.text_type_embed, std=0.02)
        
        # æŠ•å½±å±‚åˆå§‹åŒ–
        for module in [self.vision_proj, self.text_proj]:
            if isinstance(module, nn.Linear):
                nn.init.trunc_normal_(module.weight, std=0.02)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, future_images, text_instructions, image_mask=None, text_mask=None):
        """
        ç”ŸæˆVL tokens
        
        Args:
            future_images: (B, C, H, W) æœªæ¥è§‚æµ‹å›¾åƒ
            text_instructions: (B, L) æ–‡æœ¬æŒ‡ä»¤token ids æˆ–æ–‡æœ¬åˆ—è¡¨
            image_mask: (B, N_patches) å›¾åƒæ©ç ï¼ˆå¯é€‰ï¼‰
            text_mask: (B, L) æ–‡æœ¬æ©ç ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            vl_tokens: (B, N_total, D) èåˆåçš„VL tokens
            vl_mask: (B, N_total) VL tokensçš„æ©ç 
        """
        # ğŸ”§ ç¡®ä¿æ‰€æœ‰è¾“å…¥æ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = next(self.parameters()).device  # è·å–æ¨¡å‹æ‰€åœ¨è®¾å¤‡
        
        if future_images is not None:
            future_images = future_images.to(device)
        
        if text_instructions is not None:
            if isinstance(text_instructions, torch.Tensor):
                text_instructions = text_instructions.to(device)
            elif isinstance(text_instructions, list):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¿æŒä¸å˜
                pass
        batch_size = future_images.shape[0]
        device = future_images.device
        if future_images is not None and len(future_images.shape) == 3:
            future_images = future_images.unsqueeze(0)
        # 1. è§†è§‰ç¼–ç 
        vision_features = self.vision_encoder(future_images)  # (B, N_patches, D_vision)
        vision_features = self.vision_proj(vision_features)   # (B, N_patches, hidden_size)
        
        # æ·»åŠ è§†è§‰ä½ç½®ç¼–ç å’Œæ¨¡æ€ç±»å‹åµŒå…¥
        vision_features = vision_features + self.vision_pos_embed[:, :vision_features.shape[1], :]
        vision_features = vision_features + self.vision_type_embed
        
        # 2. æ–‡æœ¬ç¼–ç 
        text_features, computed_text_mask = self.text_encoder(text_instructions)  # (B, L, D_text)
        text_features = self.text_proj(text_features)  # (B, L, hidden_size)
        
        # ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„æ©ç æˆ–æä¾›çš„æ©ç 
        if text_mask is None:
            text_mask = computed_text_mask
        
        # æ·»åŠ æ–‡æœ¬ä½ç½®ç¼–ç å’Œæ¨¡æ€ç±»å‹åµŒå…¥
        text_features = text_features + self.text_pos_embed[:, :text_features.shape[1], :]
        text_features = text_features + self.text_type_embed
        
        # 3. æ‹¼æ¥è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾
        vl_features = torch.cat([vision_features, text_features], dim=1)  # (B, N_patches+L, hidden_size)
        
        # 4. åˆ›å»ºè”åˆæ©ç 
        if image_mask is None:
            image_mask = torch.ones(batch_size, vision_features.shape[1], 
                                  dtype=torch.bool, device=device)
        
        vl_mask = torch.cat([image_mask, text_mask], dim=1)  # (B, N_patches+L)
        
        # 5. é€šè¿‡å¤šå±‚èåˆå—
        for fusion_layer in self.fusion_layers:
            vl_features = fusion_layer(vl_features, mask=~vl_mask)  # æ³¨æ„ï¼šmaskéœ€è¦åè½¬
        
        # 6. è¾“å‡ºå½’ä¸€åŒ–
        vl_tokens = self.output_norm(vl_features)
        
        return vl_tokens, vl_mask
    
    def get_vision_features(self, images):
        """å•ç‹¬è·å–è§†è§‰ç‰¹å¾ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        return self.vision_encoder(images)
    
    def get_text_features(self, text_instructions):
        """å•ç‹¬è·å–æ–‡æœ¬ç‰¹å¾ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        return self.text_encoder(text_instructions)


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_vl_token_generator():
    """æµ‹è¯•VL Tokenç”Ÿæˆå™¨"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºVL Tokenç”Ÿæˆå™¨
    vl_generator = VLTokenGenerator(
        vision_model_name="google/siglip-so400m-patch14-384",
        text_model_name="google/siglip-so400m-patch14-384",
        hidden_size=1152,
        num_fusion_layers=4
    ).to(device)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    future_images = torch.randn(batch_size, 3, 384, 384).to(device)
    text_instructions = [
        "Pick up the red cup and place it on the table",
        "Move the robot arm to the left side"
    ]
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        vl_tokens, vl_mask = vl_generator(future_images, text_instructions)
    
    print(f"VL tokens shape: {vl_tokens.shape}")
    print(f"VL mask shape: {vl_mask.shape}")
    print(f"Valid tokens per sample: {vl_mask.sum(dim=1)}")
    
    return vl_tokens, vl_mask


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_vl_token_generator()