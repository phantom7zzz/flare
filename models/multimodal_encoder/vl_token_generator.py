import torch
import torch.nn as nn
import torch.nn.functional as F
import os

# å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"


class VLFusionBlock(nn.Module):
    """VLèåˆå—ï¼šSelf-Attention + FeedForward"""
    
    def __init__(self, hidden_size, num_heads=8, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        
        # Self-Attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Norm
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)
        
        # FeedForward
        self.feedforward = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 4, hidden_size),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, mask=None):
        """
        Args:
            x: (B, N, D) è¾“å…¥ç‰¹å¾
            mask: (B, N) æ³¨æ„åŠ›æ©ç 
        Returns:
            x: (B, N, D) è¾“å‡ºç‰¹å¾
        """
        # ç»Ÿä¸€å°† mask è½¬æˆ boolï¼Œé¿å… key_padding_mask ç±»å‹æŠ¥é”™
        if mask is not None and mask.dtype != torch.bool:
            mask = mask.bool()

        # Self-Attention with residual connection
        residual = x
        x = self.norm1(x)
        attn_out, _ = self.self_attention(
            x, x, x,
            key_padding_mask=mask
        )
        x = residual + attn_out

        # FeedForward with residual connection
        residual = x
        x = self.norm2(x)
        ff_out = self.feedforward(x)
        x = residual + ff_out

        return x


class VLTokenGenerator(nn.Module):
    """
    ç»Ÿä¸€T5æ¶æ„çš„VL Tokenç”Ÿæˆå™¨
    
    å…³é”®ä¿®æ”¹ï¼š
    - å®Œå…¨ç§»é™¤SigLIP2æ–‡æœ¬ç¼–ç å™¨
    - åªä½¿ç”¨T5é¢„è®¡ç®—åµŒå…¥å¤„ç†æ‰€æœ‰æ–‡æœ¬
    - ç®€åŒ–æ¶æ„ï¼Œé¿å…åŒç¼–ç å™¨å¤æ‚æ€§
    """
    
    def __init__(self, 
                 vision_model_name=None,
                 text_model_name=None,
                 hidden_size=2048,
                 num_fusion_layers=4,
                 num_heads=8,
                 dropout=0.1,
                 max_text_length=1024,
                 image_size=256,
                 device="cuda",
                 torch_dtype=torch.float16,
                 t5_embed_dim=4096,
                 use_precomp_lang_embed=True):
        super().__init__()
        
        # ğŸ¯ æ ¸å¿ƒé…ç½®
        self.hidden_size = hidden_size
        self.max_text_length = max_text_length
        self.use_precomp_lang_embed = use_precomp_lang_embed
        self.t5_embed_dim = t5_embed_dim
        self.image_size = image_size
        self.device = device
        self.max_text_length = 1024
        print("ğŸ¯ åˆå§‹åŒ–ç»Ÿä¸€T5æ¶æ„çš„VLTokenGenerator")
        print(f"   éšè—å±‚å¤§å°: {hidden_size}")
        print(f"   T5åµŒå…¥ç»´åº¦: {t5_embed_dim}")
        print(f"   å›¾åƒå°ºå¯¸: {image_size}")
        print(f"   æœ€å¤§æ–‡æœ¬é•¿åº¦: {max_text_length}")
        
        # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šå®Œå…¨ç§»é™¤SigLIP2æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = None
        print("   âœ… è·³è¿‡SigLIP2æ–‡æœ¬ç¼–ç å™¨ï¼ˆç»Ÿä¸€ä½¿ç”¨T5ï¼‰")
        
        # 1. è§†è§‰ç¼–ç å™¨ (SigLIP2) - ä¿æŒä¸å˜
        print(f"   ğŸ–¼ï¸ åˆå§‹åŒ–è§†è§‰ç¼–ç å™¨...")
        if vision_model_name is None:
            vision_model_name = "./models/siglip2-large-patch16-256"
            
        # å¯¼å…¥æ‚¨çš„è§†è§‰ç¼–ç å™¨
        from models.multimodal_encoder.siglip2_encoder import SigLIP2VisionTower
        
        self.vision_encoder = SigLIP2VisionTower(
            vision_tower=vision_model_name, 
            args=None,
            image_size=image_size
        )
        vision_dim = self.vision_encoder.hidden_size
        print(f"   âœ… è§†è§‰ç¼–ç å™¨åˆå§‹åŒ–å®Œæˆï¼Œhidden_size: {vision_dim}")
        
        # 2. ğŸ¯ T5æ–‡æœ¬é€‚é…å™¨ï¼š4096 â†’ 2048
        print(f"   ğŸ“ åˆå§‹åŒ–T5æ–‡æœ¬é€‚é…å™¨...")
        self.t5_text_adapter = nn.Linear(
            self.t5_embed_dim, 
            self.hidden_size, 
            bias=False
        )
        print(f"   âœ… T5é€‚é…å™¨: {self.t5_embed_dim} â†’ {self.hidden_size}")
        
        # 3. ç‰¹å¾æŠ•å½±å±‚
        print(f"   ğŸ”§ é…ç½®ç»´åº¦å¯¹é½:")
        print(f"      vision_dim: {vision_dim}")
        print(f"      target_hidden_size: {hidden_size}")
        
        # è§†è§‰æŠ•å½±å±‚
        if vision_dim != hidden_size:
            self.vision_proj = nn.Linear(vision_dim, hidden_size)
            print(f"      åˆ›å»ºè§†è§‰æŠ•å½±å±‚: {vision_dim} â†’ {hidden_size}")
        else:
            self.vision_proj = nn.Identity()
            print(f"      è§†è§‰ç»´åº¦åŒ¹é…ï¼Œæ— éœ€æŠ•å½±")
            
        # T5æ–‡æœ¬æŠ•å½±å±‚ï¼ˆåœ¨é€‚é…å™¨ä¹‹åï¼‰
        self.text_proj = nn.Linear(hidden_size, hidden_size)
        print(f"      T5æ–‡æœ¬æŠ•å½±å±‚: {hidden_size} â†’ {hidden_size}")
        
        # 4. ä½ç½®ç¼–ç 
        max_vision_tokens = (image_size // 16) ** 2  # å‡è®¾patch_size=16
        max_text_tokens = max_text_length
        
        self.vision_pos_embed = nn.Parameter(
            torch.zeros(1, max_vision_tokens, hidden_size)
        )
        self.text_pos_embed = nn.Parameter(
            torch.zeros(1, max_text_tokens, hidden_size)
        )
        print(f"   ğŸ”¢ ä½ç½®ç¼–ç : vision({max_vision_tokens}), text({max_text_tokens})")
        
        # 5. æ¨¡æ€ç±»å‹åµŒå…¥
        self.vision_type_embed = nn.Parameter(torch.zeros(1, 1, hidden_size))
        self.text_type_embed = nn.Parameter(torch.zeros(1, 1, hidden_size))
        
        # 6. å¤šå±‚VLèåˆå—
        self.fusion_layers = nn.ModuleList([
            VLFusionBlock(hidden_size, num_heads, dropout)
            for _ in range(num_fusion_layers)
        ])
        print(f"   ğŸ”„ èåˆå±‚æ•°: {num_fusion_layers}")
        
        # 7. è¾“å‡ºå±‚å½’ä¸€åŒ–
        self.output_norm = nn.LayerNorm(hidden_size)
        
        # åˆå§‹åŒ–å‚æ•°
        self._initialize_weights()
        
        print(f"   âœ… ç»Ÿä¸€T5æ¶æ„VLTokenGeneratoråˆå§‹åŒ–å®Œæˆ")
        
    def _initialize_weights(self):
        """åˆå§‹åŒ–å‚æ•°"""
        # T5é€‚é…å™¨åˆå§‹åŒ–
        with torch.no_grad():
            nn.init.xavier_uniform_(self.t5_text_adapter.weight)
        
        # ä½ç½®ç¼–ç åˆå§‹åŒ–
        nn.init.trunc_normal_(self.vision_pos_embed, std=0.02)
        nn.init.trunc_normal_(self.text_pos_embed, std=0.02)
        
        # æ¨¡æ€ç±»å‹åµŒå…¥åˆå§‹åŒ–
        nn.init.trunc_normal_(self.vision_type_embed, std=0.02)
        nn.init.trunc_normal_(self.text_type_embed, std=0.02)
        
        # æŠ•å½±å±‚åˆå§‹åŒ–
        for module in [self.vision_proj, self.text_proj]:
            if isinstance(module, nn.Linear):
                nn.init.trunc_normal_(module.weight, std=0.02)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, future_images=None, text_instructions=None, image_mask=None, text_mask=None):
        """
        ç»Ÿä¸€T5æ¶æ„çš„å‰å‘ä¼ æ’­
        
        Args:
            future_images: (B, C, H, W) æœªæ¥è§‚æµ‹å›¾åƒ
            text_instructions: T5é¢„è®¡ç®—åµŒå…¥tensoræˆ–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            image_mask: (B, N_patches) å›¾åƒæ©ç ï¼ˆå¯é€‰ï¼‰
            text_mask: (B, L) æ–‡æœ¬æ©ç ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            vl_tokens: (B, N_total, D) èåˆåçš„VL tokens
            vl_mask: (B, N_total) VL tokensçš„æ©ç 
        """
        device = next(self.parameters()).device
        
        # ç¡®å®šbatch_size
        if future_images is not None:
            future_images = future_images.to(device)
            batch_size = future_images.shape[0]
            if len(future_images.shape) == 3:
                future_images = future_images.unsqueeze(0)
        else:
            if isinstance(text_instructions, torch.Tensor):
                batch_size = text_instructions.shape[0]
            elif isinstance(text_instructions, list):
                batch_size = len(text_instructions)
            else:
                batch_size = 1
        
        # 1. ğŸ–¼ï¸ è§†è§‰ç¼–ç  - ä¿æŒåŸæœ‰é€»è¾‘
        if future_images is not None:
            try:
                if hasattr(self.vision_encoder, 'vision_tower'):
                    
                    vision_outputs = self.vision_encoder.vision_tower(
                        future_images,
                        interpolate_pos_encoding=True
                    )
                    
                    if hasattr(vision_outputs, 'last_hidden_state'):
                        vision_features = vision_outputs.last_hidden_state
                    elif hasattr(vision_outputs, 'pooler_output'):
                        pooler_out = vision_outputs.pooler_output
                        num_patches = (self.image_size // 16) ** 2
                        vision_features = pooler_out.unsqueeze(1).expand(-1, num_patches, -1)
                    else:
                        vision_features = vision_outputs
                        
                    
                else:
                    print("âš ï¸ æ— æ³•ä½¿ç”¨ä½ç½®ç¼–ç æ’å€¼ï¼Œä½¿ç”¨æ ‡å‡†æ–¹æ³•")
                    vision_features = self.vision_encoder(future_images)
                    
            except Exception as e:
                print(f"âš ï¸ ä½ç½®ç¼–ç æ’å€¼å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•: {e}")
                vision_features = self.vision_encoder(future_images)
            
            # è§†è§‰æŠ•å½±
            vision_features = self.vision_proj(vision_features)
            
            # ä½ç½®ç¼–ç å¤„ç†
            seq_len = vision_features.shape[1]
            pos_embed_len = self.vision_pos_embed.shape[1]
            
            if seq_len <= pos_embed_len:
                vision_features = vision_features + self.vision_pos_embed[:, :seq_len, :]
            else:
                repeat_times = (seq_len // pos_embed_len) + 1
                extended_pos_embed = self.vision_pos_embed.repeat(1, repeat_times, 1)
                vision_features = vision_features + extended_pos_embed[:, :seq_len, :]
            
            vision_features = vision_features + self.vision_type_embed
            
        else:
            vision_features = torch.empty(batch_size, 0, self.hidden_size, device=device)
        
        # 2. ğŸ¯ T5æ–‡æœ¬å¤„ç† - æ ¸å¿ƒä¿®æ”¹
        text_features, computed_text_mask = self._process_t5_text(
            text_instructions, batch_size, device
        )
        
        # 3. ğŸ”— æ‹¼æ¥è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾
        if vision_features.shape[1] > 0 and text_features.shape[1] > 0:
            vl_features = torch.cat([vision_features, text_features], dim=1)
        elif vision_features.shape[1] > 0:
            vl_features = vision_features
            print(f"ğŸ–¼ï¸ ä»…è§†è§‰ç‰¹å¾: {vl_features.shape}")
        elif text_features.shape[1] > 0:
            vl_features = text_features
            print(f"ğŸ“ ä»…æ–‡æœ¬ç‰¹å¾: {vl_features.shape}")
        else:
            vl_features = torch.zeros(batch_size, 1, self.hidden_size, device=device)
            print(f"ğŸ”„ ä½¿ç”¨dummyç‰¹å¾: {vl_features.shape}")
        
        # 4. åˆ›å»ºè”åˆæ©ç 
        if image_mask is None and vision_features.shape[1] > 0:
            image_mask = torch.ones(batch_size, vision_features.shape[1], 
                                dtype=torch.bool, device=device)
        elif vision_features.shape[1] == 0:
            image_mask = torch.empty(batch_size, 0, dtype=torch.bool, device=device)
        
        # ä½¿ç”¨computed_text_maskè€Œä¸æ˜¯text_mask
        if text_mask is None:
            text_mask = computed_text_mask
        
        if vision_features.shape[1] > 0 and text_features.shape[1] > 0:
            vl_mask = torch.cat([image_mask, text_mask], dim=1)
        elif vision_features.shape[1] > 0:
            vl_mask = image_mask
        elif text_features.shape[1] > 0:
            vl_mask = text_mask
        else:
            vl_mask = torch.ones(batch_size, 1, dtype=torch.bool, device=device)
        
        # 5. ğŸ”„ é€šè¿‡å¤šå±‚èåˆå—
        for i, fusion_layer in enumerate(self.fusion_layers):
            vl_features = fusion_layer(vl_features, mask=~vl_mask)
        
        # 6. è¾“å‡ºå½’ä¸€åŒ–
        vl_tokens = self.output_norm(vl_features)
        
        return vl_tokens, vl_mask
    
    def _process_t5_text(self, text_instructions, batch_size, device):
        """
        ğŸ¯ æ ¸å¿ƒæ–¹æ³•ï¼šå¤„ç†T5æ–‡æœ¬è¾“å…¥
        
        ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ–‡æœ¬è¾“å…¥ï¼ŒåŒ…æ‹¬ï¼š
        1. T5é¢„è®¡ç®—åµŒå…¥tensor
        2. T5åµŒå…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        3. ç©ºè¾“å…¥
        """
        try:
            if text_instructions is None:
                # æ²¡æœ‰æ–‡æœ¬è¾“å…¥ï¼Œè¿”å›é›¶åµŒå…¥
                text_features = torch.zeros(batch_size, self.max_text_length, self.hidden_size, device=device)
                text_mask = torch.zeros(batch_size, self.max_text_length, dtype=torch.bool, device=device)
                return text_features, text_mask
            
            # ğŸ”§ æƒ…å†µ1ï¼šç›´æ¥ä¼ å…¥T5åµŒå…¥tensor
            if isinstance(text_instructions, torch.Tensor):
                print(f"ğŸ¯ ç›´æ¥ä½¿ç”¨T5åµŒå…¥tensor: {text_instructions.shape}")
                t5_embeds = text_instructions.to(device)
                
                # ç»´åº¦é€‚é…ï¼š4096 â†’ 2048
                if t5_embeds.shape[-1] == self.t5_embed_dim:
                    text_features = self.t5_text_adapter(t5_embeds)
                    print(f"   âœ… T5ç»´åº¦é€‚é…: {self.t5_embed_dim} â†’ {self.hidden_size}")
                else:
                    text_features = t5_embeds
                    print(f"   âš ï¸ T5åµŒå…¥ç»´åº¦å·²åŒ¹é…: {t5_embeds.shape[-1]}")
                
                # ç”Ÿæˆæ©ç 
                text_mask = torch.ones(text_features.shape[:2], dtype=torch.bool, device=device)
                
            # ğŸ”§ æƒ…å†µ2ï¼šæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆåŠ è½½T5åµŒå…¥ï¼‰
            elif isinstance(text_instructions, list) and len(text_instructions) > 0:
                if isinstance(text_instructions[0], str) and text_instructions[0].endswith('.pt'):
                    
                    t5_embeds_list = []
                    for embed_path in text_instructions:
                        try:
                            if os.path.exists(embed_path):
                                embed = torch.load(embed_path, map_location=device)
                                if isinstance(embed, torch.Tensor):
                                    t5_embeds_list.append(embed)
                                else:
                                    # å¤„ç†å­—å…¸æ ¼å¼
                                    if 'embedding' in embed:
                                        t5_embeds_list.append(embed['embedding'])
                                    elif 'last_hidden_state' in embed:
                                        t5_embeds_list.append(embed['last_hidden_state'])
                                    else:
                                        # ä½¿ç”¨é›¶åµŒå…¥
                                        t5_embeds_list.append(
                                            torch.zeros(self.max_text_length, self.t5_embed_dim, device=device)
                                        )
                            else:
                                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {embed_path}")
                                t5_embeds_list.append(
                                    torch.zeros(self.max_text_length, self.t5_embed_dim, device=device)
                                )
                        except Exception as e:
                            print(f"âŒ åŠ è½½åµŒå…¥å¤±è´¥ {embed_path}: {e}")
                            t5_embeds_list.append(
                                torch.zeros(self.max_text_length, self.t5_embed_dim, device=device)
                            )
                    
                    if t5_embeds_list:
                        # ç»Ÿä¸€åºåˆ—é•¿åº¦
                        max_len = min(max(embed.shape[0] if len(embed.shape) == 2 else embed.shape[1] 
                                         for embed in t5_embeds_list), self.max_text_length)
                        
                        padded_embeds = []
                        for embed in t5_embeds_list:
                            if len(embed.shape) == 3:
                                embed = embed[0]  # (1, seq_len, dim) â†’ (seq_len, dim)
                            
                            if embed.shape[0] > max_len:
                                embed = embed[:max_len]
                            elif embed.shape[0] < max_len:
                                pad_size = max_len - embed.shape[0]
                                embed = torch.cat([
                                    embed,
                                    torch.zeros(pad_size, embed.shape[1], device=device)
                                ], dim=0)
                            
                            padded_embeds.append(embed)
                        
                        t5_embeds = torch.stack(padded_embeds, dim=0)  # (B, seq_len, 4096)
                        
                        # ğŸ¯ å…³é”®ï¼šT5ç»´åº¦é€‚é… 4096 â†’ 2048
                        text_features = self.t5_text_adapter(t5_embeds)
                        text_mask = torch.ones(batch_size, max_len, dtype=torch.bool, device=device)
                        
                    else:
                        # åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é›¶åµŒå…¥
                        text_features = torch.zeros(batch_size, self.max_text_length, self.hidden_size, device=device)
                        text_mask = torch.zeros(batch_size, self.max_text_length, dtype=torch.bool, device=device)
                else:
                    # ğŸš¨ ä¸åº”è¯¥æ”¶åˆ°æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œå› ä¸ºæˆ‘ä»¬åªä½¿ç”¨T5
                    print("âŒ æ”¶åˆ°æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œä½†VLTokenGeneratoré…ç½®ä¸ºåªä½¿ç”¨T5åµŒå…¥")
                    print("   è¯·æ£€æŸ¥æ•°æ®ä¼ é€’æµç¨‹ï¼Œåº”è¯¥ä¼ é€’T5åµŒå…¥æ–‡ä»¶è·¯å¾„æˆ–tensor")
                    text_features = torch.zeros(batch_size, self.max_text_length, self.hidden_size, device=device)
                    text_mask = torch.zeros(batch_size, self.max_text_length, dtype=torch.bool, device=device)
            else:
                # å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨é›¶åµŒå…¥
                text_features = torch.zeros(batch_size, self.max_text_length, self.hidden_size, device=device)
                text_mask = torch.zeros(batch_size, self.max_text_length, dtype=torch.bool, device=device)
            
            # ğŸ”§ åº”ç”¨æ–‡æœ¬æŠ•å½±
            text_features = self.text_proj(text_features)
            
            # ğŸ”§ æ·»åŠ ä½ç½®ç¼–ç 
            seq_len = text_features.shape[1]
            if seq_len <= self.text_pos_embed.shape[1]:
                text_features = text_features + self.text_pos_embed[:, :seq_len, :]
            else:
                print(f"âš ï¸ æ–‡æœ¬åºåˆ—é•¿åº¦è¶…è¿‡ä½ç½®ç¼–ç : {seq_len} > {self.text_pos_embed.shape[1]}")
                repeat_times = (seq_len // self.text_pos_embed.shape[1]) + 1
                extended_pos_embed = self.text_pos_embed.repeat(1, repeat_times, 1)
                text_features = text_features + extended_pos_embed[:, :seq_len, :]
            
            text_features = text_features + self.text_type_embed
            
            # ğŸ”§ æ£€æŸ¥NaN
            if torch.isnan(text_features).any():
                print("âš ï¸ æ£€æµ‹åˆ°æ–‡æœ¬ç‰¹å¾ä¸­çš„NaNï¼Œä½¿ç”¨é›¶åµŒå…¥æ›¿æ¢")
                text_features = torch.zeros_like(text_features)
                text_mask = torch.zeros_like(text_mask)
            
            return text_features, text_mask
            
        except Exception as e:
            print(f"âŒ T5æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # å›é€€åˆ°é›¶åµŒå…¥
            text_features = torch.zeros(batch_size, self.max_text_length, self.hidden_size, device=device)
            text_mask = torch.zeros(batch_size, self.max_text_length, dtype=torch.bool, device=device)
            return text_features, text_mask


# ğŸ§ª æµ‹è¯•å‡½æ•°
def test_unified_t5_vl_generator():
    """æµ‹è¯•ç»Ÿä¸€T5æ¶æ„çš„VL Tokenç”Ÿæˆå™¨"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print("ğŸ§ª æµ‹è¯•ç»Ÿä¸€T5æ¶æ„VL Tokenç”Ÿæˆå™¨...")
    print("="*60)
    
    # åˆ›å»ºVL Tokenç”Ÿæˆå™¨
    vl_generator = VLTokenGenerator(
        vision_model_name="/home/deng_xiang/qian_daichao/RoboTwin/policy/RDT_flare/siglip2-large-patch16-256",
        hidden_size=2048,
        num_fusion_layers=4,
        max_text_length=32,
        image_size=256,
        t5_embed_dim=4096,
        use_precomp_lang_embed=True  # ğŸ¯ ä½¿ç”¨T5é¢„è®¡ç®—åµŒå…¥
    ).to(device)
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        
        # æµ‹è¯•å›¾åƒ
        future_images = torch.randn(batch_size, 3, 256, 256).to(device)
        
        # ğŸ¯ æµ‹è¯•T5åµŒå…¥tensor
        t5_embeds = torch.randn(batch_size, 16, 4096).to(device)  # æ¨¡æ‹ŸT5åµŒå…¥
        
        print(f"ğŸ“ æµ‹è¯•è¾“å…¥:")
        print(f"   å›¾åƒå½¢çŠ¶: {future_images.shape}")
        print(f"   T5åµŒå…¥å½¢çŠ¶: {t5_embeds.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            vl_tokens, vl_mask = vl_generator(future_images, t5_embeds)
        
        print(f"\nâœ… æµ‹è¯•æˆåŠŸ!")
        print(f"   VL tokenså½¢çŠ¶: {vl_tokens.shape}")
        print(f"   VL maskå½¢çŠ¶: {vl_mask.shape}")
        print(f"   æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆtokenæ•°: {vl_mask.sum(dim=1).tolist()}")
        print(f"   éšè—å±‚ç»´åº¦: {vl_tokens.shape[-1]}")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        print(f"\nğŸ“Š æ•°æ®è´¨é‡:")
        print(f"   VL tokensèŒƒå›´: [{vl_tokens.min():.3f}, {vl_tokens.max():.3f}]")
        print(f"   VL tokenså‡å€¼: {vl_tokens.mean():.3f}")
        print(f"   VL tokensæ ‡å‡†å·®: {vl_tokens.std():.3f}")
        
        return vl_generator, vl_tokens, vl_mask
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


def test_t5_file_loading():
    """æµ‹è¯•T5æ–‡ä»¶åŠ è½½åŠŸèƒ½"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print("\nğŸ§ª æµ‹è¯•T5æ–‡ä»¶åŠ è½½...")
    print("="*60)
    
    vl_generator = VLTokenGenerator(
        vision_model_name="/home/deng_xiang/qian_daichao/RoboTwin/policy/RDT_flare/siglip2-large-patch16-256",
        hidden_size=2048,
        t5_embed_dim=4096,
        use_precomp_lang_embed=True
    ).to(device)
    
    try:
        # æ¨¡æ‹ŸT5åµŒå…¥æ–‡ä»¶è·¯å¾„
        fake_t5_paths = [
            "training_data/grab_roller/demo_grab_roller/grab_roller-demo_clean-50/episode_30/instructions/lang_embed_33.pt",
            "training_data/grab_roller/demo_grab_roller/grab_roller-demo_clean-50/episode_31/instructions/lang_embed_34.pt"
        ]
        
        future_images = torch.randn(2, 3, 256, 256).to(device)
        
        print(f"ğŸ“ æµ‹è¯•T5æ–‡ä»¶è·¯å¾„:")
        for path in fake_t5_paths:
            print(f"   {path}")
        
        with torch.no_grad():
            vl_tokens, vl_mask = vl_generator(future_images, fake_t5_paths)
        
        print(f"âœ… T5æ–‡ä»¶è·¯å¾„å¤„ç†æˆåŠŸ!")
        print(f"   è¾“å‡ºå½¢çŠ¶: {vl_tokens.shape}")
        print(f"   æ©ç å½¢çŠ¶: {vl_mask.shape}")
        
    except Exception as e:
        print(f"âŒ T5æ–‡ä»¶è·¯å¾„å¤„ç†å¤±è´¥: {e}")


def test_integration_example():
    """é›†æˆæµ‹è¯•ç¤ºä¾‹"""
    print("\nğŸ¯ ç»Ÿä¸€T5æ¶æ„é›†æˆç¤ºä¾‹:")
    print("="*60)
    
    example_code = '''
# ğŸ¯ åœ¨è®­ç»ƒè„šæœ¬ä¸­çš„ä½¿ç”¨ç¤ºä¾‹

# 1. ä»æ•°æ®é›†è·å–æ•°æ®
batch = dataloader.next()
future_obs_images = batch["future_obs_images"]        # (B, 3, H, W)
t5_embed_paths = batch["flare_text_embed_paths"]     # List[str] - T5åµŒå…¥æ–‡ä»¶è·¯å¾„
lang_embeds = batch["lang_embeds"]                   # (B, seq_len, 4096) - T5åµŒå…¥

# 2. FLAREå¤„ç†
total_loss, loss_dict = model.compute_loss_with_flare(
    lang_tokens=lang_embeds,                         # T5åµŒå…¥ç”¨äºDiT
    img_tokens=img_embeds,                           # å½“å‰å›¾åƒç”¨äºDiT
    text_instructions=t5_embed_paths,                # T5è·¯å¾„ç”¨äºFLARE
    future_obs_images=future_obs_images,             # æœªæ¥å›¾åƒç”¨äºFLARE
    return_alignment_loss=True
)

# 3. VLTokenGeneratorå†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†T5åµŒå…¥ï¼š
#    - åŠ è½½.ptæ–‡ä»¶ â†’ 4096ç»´T5åµŒå…¥
#    - T5é€‚é…å™¨ â†’ 2048ç»´ç»Ÿä¸€è¡¨ç¤º
#    - ä¸è§†è§‰ç‰¹å¾èåˆ â†’ VL tokens
'''
    
    print(example_code)
    
    usage_notes = '''
ğŸ”§ å…³é”®ä¿®æ”¹ç‚¹:

1. VLTokenGeneratoræ”¹åŠ¨:
   âœ… ç§»é™¤SigLIP2æ–‡æœ¬ç¼–ç å™¨
   âœ… æ–°å¢T5é€‚é…å™¨ (4096â†’2048)
   âœ… ç»Ÿä¸€æ–‡æœ¬å¤„ç†æµç¨‹

2. æ•°æ®é›†æ”¹åŠ¨:
   âœ… è¿”å›flare_text_embed_paths (T5åµŒå…¥è·¯å¾„)
   âœ… ä¸å†ç”Ÿæˆç®€åŒ–æ–‡æœ¬æŒ‡ä»¤

3. è®­ç»ƒè„šæœ¬æ”¹åŠ¨:
   âœ… ä¼ é€’T5åµŒå…¥è·¯å¾„ç»™FLARE
   âœ… ä¸å†å¤„ç†åŒç¼–ç å™¨å¤æ‚æ€§

4. è§£å†³çš„é—®é¢˜:
   âœ… ç»´åº¦ä¸åŒ¹é… (4096 vs 2048)
   âœ… æ–‡ä»¶è·¯å¾„é”™è¯¯ä¼ é€’
   âœ… NaNé—®é¢˜
   âœ… æ¶æ„å¤æ‚æ€§

ğŸ¯ ç°åœ¨çš„æ•°æ®æµ:
T5åµŒå…¥(.pt) â†’ ç»Ÿä¸€é€‚é…å™¨ â†’ 2048ç»´ â†’ DiT/FLAREå…±äº«è¡¨ç¤º
'''
    
    print(usage_notes)


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ç»Ÿä¸€T5æ¶æ„æµ‹è¯•...")
    
    # æµ‹è¯•1: åŸºç¡€åŠŸèƒ½
    generator, tokens, mask = test_unified_t5_vl_generator()
    
    if generator is not None:
        # æµ‹è¯•2: T5æ–‡ä»¶åŠ è½½
        test_t5_file_loading()
        
        # æµ‹è¯•3: é›†æˆç¤ºä¾‹
        test_integration_example()
        
        print(f"\nğŸ‰ ç»Ÿä¸€T5æ¶æ„VLTokenGeneratoræµ‹è¯•å®Œæˆ!")
        print(f"ğŸ’¡ ç°åœ¨æ‰€æœ‰æ–‡æœ¬å¤„ç†éƒ½ä½¿ç”¨T5ï¼Œé¿å…äº†åŒç¼–ç å™¨çš„å¤æ‚æ€§ã€‚")
        print(f"ğŸ”§ è¯·æ›´æ–°æ‚¨çš„è®­ç»ƒè„šæœ¬ï¼Œç¡®ä¿ä¼ é€’T5åµŒå…¥è·¯å¾„ç»™FLAREç»„ä»¶ã€‚")
    else:
        print(f"\nâš ï¸ è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œä¾èµ–é¡¹ã€‚")


# ğŸ¯ é¢å¤–çš„è°ƒè¯•å·¥å…·
class VLTokenGeneratorDebugger:
    """VLTokenGeneratorè°ƒè¯•å·¥å…·"""
    
    def __init__(self, vl_generator):
        self.vl_generator = vl_generator
    
    def debug_t5_processing(self, text_instructions):
        """è°ƒè¯•T5å¤„ç†è¿‡ç¨‹"""
        print("ğŸ” è°ƒè¯•T5å¤„ç†è¿‡ç¨‹:")
        print("="*50)
        
        device = next(self.vl_generator.parameters()).device
        batch_size = len(text_instructions) if isinstance(text_instructions, list) else 1
        
        try:
            # ç›´æ¥è°ƒç”¨T5å¤„ç†æ–¹æ³•
            text_features, text_mask = self.vl_generator._process_t5_text(
                text_instructions, batch_size, device
            )
            
            print(f"âœ… T5å¤„ç†æˆåŠŸ:")
            print(f"   è¾“å…¥ç±»å‹: {type(text_instructions)}")
            print(f"   è¾“å‡ºç‰¹å¾: {text_features.shape}")
            print(f"   è¾“å‡ºæ©ç : {text_mask.shape}")
            print(f"   æœ‰æ•ˆtokenæ•°: {text_mask.sum(dim=1).tolist()}")
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            if torch.isnan(text_features).any():
                print("âŒ æ£€æµ‹åˆ°NaNå€¼!")
            else:
                print(f"   æ•°æ®èŒƒå›´: [{text_features.min():.3f}, {text_features.max():.3f}]")
                print(f"   æ•°æ®å‡å€¼: {text_features.mean():.3f}")
                print(f"   æ•°æ®æ ‡å‡†å·®: {text_features.std():.3f}")
                
        except Exception as e:
            print(f"âŒ T5å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def debug_vision_processing(self, images):
        """è°ƒè¯•è§†è§‰å¤„ç†è¿‡ç¨‹"""
        print("ğŸ” è°ƒè¯•è§†è§‰å¤„ç†è¿‡ç¨‹:")
        print("="*50)
        
        try:
            with torch.no_grad():
                vision_features = self.vl_generator.vision_encoder(images)
            
            print(f"âœ… è§†è§‰å¤„ç†æˆåŠŸ:")
            print(f"   è¾“å…¥å›¾åƒ: {images.shape}")
            print(f"   è¾“å‡ºç‰¹å¾: {vision_features.shape}")
            print(f"   æ•°æ®èŒƒå›´: [{vision_features.min():.3f}, {vision_features.max():.3f}]")
            
        except Exception as e:
            print(f"âŒ è§†è§‰å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def debug_full_pipeline(self, images, text_instructions):
        """è°ƒè¯•å®Œæ•´ç®¡é“"""
        print("ğŸ” è°ƒè¯•å®Œæ•´VLç®¡é“:")
        print("="*50)
        
        try:
            with torch.no_grad():
                vl_tokens, vl_mask = self.vl_generator(images, text_instructions)
            
            print(f"âœ… å®Œæ•´ç®¡é“æˆåŠŸ:")
            print(f"   VL tokens: {vl_tokens.shape}")
            print(f"   VL mask: {vl_mask.shape}")
            print(f"   æ•°æ®è´¨é‡æ­£å¸¸: {not torch.isnan(vl_tokens).any()}")
            
            return vl_tokens, vl_mask
            
        except Exception as e:
            print(f"âŒ å®Œæ•´ç®¡é“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None


# ğŸ”§ å¿«é€Ÿä¿®å¤æ£€æŸ¥å‡½æ•°
def quick_fix_check():
    """å¿«é€Ÿæ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤"""
    print("ğŸ”§ å¿«é€Ÿä¿®å¤æ£€æŸ¥æ¸…å•:")
    print("="*50)
    
    checklist = [
        ("VLTokenGeneratorç§»é™¤SigLIP2æ–‡æœ¬ç¼–ç å™¨", "self.text_encoder = None"),
        ("æ·»åŠ T5é€‚é…å™¨", "self.t5_text_adapter = nn.Linear(4096, 2048)"),
        ("æ›´æ–°_process_t5_textæ–¹æ³•", "ç»Ÿä¸€å¤„ç†T5åµŒå…¥"),
        ("æ•°æ®é›†è¿”å›T5è·¯å¾„", "flare_text_embed_paths"),
        ("è®­ç»ƒè„šæœ¬ä¼ é€’T5è·¯å¾„", "text_instructions=t5_paths"),
        ("æµ‹è¯•æ— ç»´åº¦ä¸åŒ¹é…", "4096â†’2048é€‚é…æ­£å¸¸"),
        ("æµ‹è¯•æ— NaNé—®é¢˜", "æ•°å€¼ç¨³å®šæ€§è‰¯å¥½"),
    ]
    
    for i, (item, detail) in enumerate(checklist, 1):
        print(f"{i}. {item}")
        print(f"   {detail}")
    
    print("\nğŸ¯ ä¿®å¤ä¼˜å…ˆçº§:")
    print("1. ç«‹å³: æ›´æ–°VLTokenGenerator (ç§»é™¤SigLIP2æ–‡æœ¬ç¼–ç å™¨)")
    print("2. ç«‹å³: æ·»åŠ T5é€‚é…å™¨å¤„ç†ç»´åº¦è½¬æ¢")
    print("3. ç«‹å³: æ›´æ–°è®­ç»ƒè„šæœ¬ä¼ é€’T5è·¯å¾„")
    print("4. éªŒè¯: æµ‹è¯•æ— ç»´åº¦ä¸åŒ¹é…å’ŒNaNé—®é¢˜")


# è¿è¡Œå¿«é€Ÿæ£€æŸ¥
if __name__ == "__main__":
    quick_fix_check()